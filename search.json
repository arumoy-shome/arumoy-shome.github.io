[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "All materials (excluding links to external websites and third party websites) on this website are open sourced under the Creative Commons Attribution 4.0 license. In brief, you are free to share and adapt all materials in any way you wish.\nThe above text was adapted from this website.\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Arumoy Shome",
    "section": "",
    "text": "Education\n\nPhD, Software Engineering for Artificial Intelligence, Delft University of Technology, Netherlands, 2021–Present\nM.Sc. Computer Science, Big Data Engineering, VU Amsterdam, Netherlands, 2018-2020\nB.Sc. Systems Design Engineering, University of Waterloo, Canada, 2012-2018\n\n\n\nTechnical Skills\n\nData Science & Machine Learning: Python, Scikit-Learn, PyTorch\nData Analysis & Engineering: Pandas, PySpark, Data Mining, Big Data Engineering, Data Pipeline Development, Data Collection\nVisualization & Analytics: Seaborn, Matplotlib, D3.js, Three.js, Information Visualization, Visual Analytics\nDevelopment Tools: Git, Docker, Linux, Unix, Nix\nProgramming Languages: Python, Ruby, Javascript, HTML, SCSS, Bash, LaTeX\nWeb Development: Ruby on Rails, UI/UX Development, Object-Relational Mapping\nResearch: Data Quality Analysis, Fairness Testing, Static Analysis, Quantitative Research\n\n\n\nProfessional Experience\nData Science (PhD Candidate), Delft University of Technology, Delft, Netherlands, 2021-2025\n\nDeveloped large-scale data mining pipeline to process 297,800 Jupyter notebooks (283 GB) from GitHub and Kaggle. Extracted 3 million code statements using Python, Pandas, and Bash. Released results as an open-source dataset to enable future research. Project website: https://github.com/arumoy-shome/shome2023notebook\nProposed data-centric fairness testing methodology for machine learning models. Evaluated 4 ML algorithms against 2 fairness metrics across 5 datasets with 1,600 end-to-end ML pipeline executions using Python and Scikit-Learn. Project website: https://github.com/arumoy-shome/shome2022qualitative\nPioneered “data smells” framework for ML dataset quality assessment. Analyzed top 25 ML datasets from Kaggle using Python and Pandas. Identified 14 data quality antipatterns and published the findings as an open-source catalog. Project website: https://arumoy.me/data-smells\nExecuted the data and ML pipelines on a Linux server and distributed the workloads across 20 CPU cores using Bash and Unix commands. Used Git and Docker to provide reproducible software artifacts.\nManaged 2 M.Sc. research projects, and an edX MOOC course on Unix tools with 1000+ active students.\nDelivered technical talks, poster presentations and guest lectures to academic and industry audiences. Translated technical concepts into actionable insights for diverse stakeholders.\n\nData Science (Research Intern), Netherlands eScience Centre, Amsterdam, Netherlands, 2019-2020\n\nBuilt ML pipeline for neutrino detection in the KM3NeT Neutrino Telescope, implementing Multi Layer Perceptrons and Graph Convolutional Neural Networks to process high-volume particle physics data. Project website: https://github.com/arumoy-shome/km3net\nDelivered solution that outperformed existing GPU-based systems in filtration quality while meeting strict performance requirements for real-time particle detection.\nCollaborated across interdisciplinary teams including particle physicists, GPU engineers, and computer scientists, translating complex AI requirements into practical implementations.\n\nWeb Developer Intern, Shopify, Ottawa, Canada, 2015-2016\n\nCollaborated with developers, designers and product managers to implement UI/UX features such as web components, animations and styling on a mature Ruby on Rails project using Ruby, JavaScript, HTML, & SCSS.\nApplied Object-Oriented Programming (OOP) principles and Test-Driven Development (TDD) to refactor code and improve test coverage.\nUsed Object-Relational Mapping (ORM) to optimize database queries and reduce server response time.\n\n\n\nTechnical Projects\n3D Kadaster, University of Amsterdam, 2018\n\nDeveloped 3D model of all buildings in The Netherlands using AHN2 point cloud dataset (1.6 TB) and BAG building polygons dataset (177 GB).\nProcessed massive geospatial datasets using PySpark distributed computing framework for scalable data processing.\nExecuted algorithms on SurfSara supercomputer infrastructure for high-performance geospatial analysis.\nCreated interactive 3D visualizations using Three.js for web-based exploration of national building infrastructure.\n\nACE: Art, Color and Emotions, ACM International Conference on MultiMedia, 2019\n\nBuilt ACE, a visual sentiment analysis platform by developing custom ML models trained on the large-scale OmniArt dataset (512 GB) to enable data-driven analysis of artistic emotions. Demo video: https://youtu.be/B1ZM6EQgEvU\nDesigned and implemented full-stack solution featuring intuitive D3.js interface with optimized interaction patterns and scalable web architecture capable of handling high-volume image processing and real-time sentiment analysis.\n\nElevate, University of Waterloo, 2017\n\nDeveloped an improved and cost-effective alternative to state-of-the-art cognitive assessment tools for Down Syndrome using web technologies, adaptive learning, and human-centric design. Project website: https://arumoy.me/elevate\nEstablished international research partnership with Waterloo Regional Down Syndrome Society (Canada) and Fundacion Paraiso Down (El Salvador) to explore specialized educational resource needs through comprehensive user surveys and interviews.\nImplemented iterative design methodology with usability testing, user testing, and engagement testing as primary validation protocols.\nDeveloped comprehensive business plan and presented product at multiple startup incubators and pitch competitions to obtain funding.\n\n\n\nProfessional Activities\n\nCollaborated as committee member for International Conference on AI Engineering (CAIN) ’25. Demonstrated teamwork, communication, and project management skills to coordinate logistics and enhance engagement for 125 participants.\nServed as program committee member for DeepTest ’25 and NL-based Software Engineering (NLBSE) ’24 international workshops. Critically evaluated and provided constructive feedback on 4 technical research papers.\nWrites and maintains open-source shell scripts using Python and Bash to automate frequent tasks and improve personal workflows. Publishes technical implementation details as blog posts.\n\n\n\nLanguage\n\nEnglish (Native), Dutch (Basic - A2), Hindi (Native), Bengali (Native)\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html",
    "href": "blogs/data-smells-public-datasets/index.html",
    "title": "Data Smells in Public Datasets",
    "section": "",
    "text": "In this talk I will present our recent paper titled Data Smells in Public Datasets which was published at the 1st International Conference on AI Engineering (CAIN) 2022. I will first present the problem we are trying to solve along with the contributions that we made. I will present the methodology which was followed along with the results obtained. I will present a select few smells which I personally find interesting & hope will generate some discussion. Finally, we will conclude the talk with some high level takeaways from our study along with the limitations & future directions of work."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#correlated-features",
    "href": "blogs/data-smells-public-datasets/index.html#correlated-features",
    "title": "Data Smells in Public Datasets",
    "section": "Correlated Features",
    "text": "Correlated Features\n\n\n\n\n\n\nFigure 4: Correlation of features in all datasets\n\n\n\nI will start with the presence of correlated features smell which many of us are perhaps already familiar with. I want to start with this smell as it was most frequently observed in our sample of datasets.\nWhen two features A & B are correlated, inducing a positive or negative change in A does the same in B. This presents an opportunity to perform feature engineering and construct a more efficient dataset. ML is highly experimental and any optimisation—no matter how smalls—counts. Small datasets ultimately are easier to understand, faster to train a model on & take up less storage."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#presence-of-sensitive-features",
    "href": "blogs/data-smells-public-datasets/index.html#presence-of-sensitive-features",
    "title": "Data Smells in Public Datasets",
    "section": "Presence of Sensitive Features",
    "text": "Presence of Sensitive Features\n\n\n\n\n\n\nFigure 5: Sensitive features\n\n\n\nI want to motivate this smell with this figure. This is a probability density plot from the adult census dataset. This dataset consists information regarding individuals including their race & sex. The supervised learning task is to predict the income class they belong to.\nLets focus primarily on the top left plot which shows that for this dataset, a male individual of fairer skin is likely to earn more. Such a bias also exist between male & female individuals of the same race. A model trained & tested on this dataset will perform well however putting such a model in production will result in devastating consequences since the model was trained using biased historical data which does not reflect the real world.\nAnd we see examples of this in the real world, when financial institutes use ML to predict if an individual is entitled to a loan, or when the criminal justice system want to predict the severity of sentencing and when police try to identify if an individual is a threat using video surveillance."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#hierarchy-from-label-encoding",
    "href": "blogs/data-smells-public-datasets/index.html#hierarchy-from-label-encoding",
    "title": "Data Smells in Public Datasets",
    "section": "Hierarchy from Label Encoding",
    "text": "Hierarchy from Label Encoding\n\n\n\n\n\n\nFigure 6: Hierarchy from label encoding\n\n\n\nFigure Figure 6 presents a probability density plot from the adult census dataset. Here we compare the income class of individuals to their level of education. We can see that for this dataset, an individual with a higher level of education is likely to earn more.\nSuch a hierarchy in categorical features is useful information which can be utilised by the model. A common practise is to encode categorical features using numbers. As an example we can encode the education levels using numbers between 0 and 4, where higher education gets a larger number. Such an encoding scheme can be beneficial to the model as it exposes the hierarchy amongst the values of a categorical feature. However applying the same encoding scheme for the race or sex feature can introduce an unwanted hierarchy amongst the values where non should exist."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#binary-missing-values",
    "href": "blogs/data-smells-public-datasets/index.html#binary-missing-values",
    "title": "Data Smells in Public Datasets",
    "section": "Binary Missing Values",
    "text": "Binary Missing Values\n\n\n\n\n\n\nFigure 7: Binary missing values\n\n\n\nTwo features from the permit dataset contain a lot of missing values (over 90% of the data in these features are missing). A common technique in such cases is to drop such features since they do not impart any knowledge to the model. However, taking a closer look at the value of the non-missing data, we find that the missing values in these features carry an implicit meaning of ‘no’ or a negative response.\nAttention must be paid to the distribution of the missing values. If the missing values are concentrated within a specific feature (along columns) as opposed to being evenly distributed across the dataset (along columns & rows), it may indicate that they carry an implicit meaning. A novice data scientist may hastily drop such features however in doing so they alter the original information portrait by dataset."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#strings-in-human-friendly-formats",
    "href": "blogs/data-smells-public-datasets/index.html#strings-in-human-friendly-formats",
    "title": "Data Smells in Public Datasets",
    "section": "Strings in Human-friendly Formats",
    "text": "Strings in Human-friendly Formats\n\n\n\n\n\n\nFigure 8: Strings in human-friendly formats\n\n\n\nThe netfix dataset contains information regarding content on the popular entertainment streaming service. The dataset contains information regarding movies & TV shows along with their duration.\nAlthough the duration for movies can be easily converted to a numerical representation, doing the same for TV shows poses several challenges and requires further effort and domain expertise."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#lack-of-proper-documentation",
    "href": "blogs/data-smells-public-datasets/index.html#lack-of-proper-documentation",
    "title": "Data Smells in Public Datasets",
    "section": "Lack of Proper Documentation",
    "text": "Lack of Proper Documentation\nWe saw several instances where a lack of proper documentation was felt. The heart dataset contains very cryptic column names and understanding the information contained within these column require domain expertise. The sex feature within the same dataset is label encoded however we do not know which number represents which gender. The cancer dataset contains several numerical features but we do not know the unit in which the measurements were recorded.\nEvery dataset is unique & contains its own idiosyncrasies and we require proper documentation to understand them. Documentation provides useful metadata & context to data scientists who are getting started with a dataset & also help re-familiarise them to the dataset quickly when they come back at a later time."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#lack-of-best-practices",
    "href": "blogs/data-smells-public-datasets/index.html#lack-of-best-practices",
    "title": "Data Smells in Public Datasets",
    "section": "Lack of Best Practices",
    "text": "Lack of Best Practices\nWe also found several instances of technical debt from lack of best practices in upstream processes. Going back to the netflix dataset where extracting numerical duration for TV shows was found to be challenging, or the heart dataset with its cryptic column names and the cancer dataset where the unit was not recorded. In all these instances, technical debt could have been avoided by simply using better column names or providing documentation. By following standardised procedures in the upstream data collection/creation stages, technical debt in the downstream stages can be avoided.\nWe feel that data smells can help identify such sources of technical debt in the early stages of a ML pipeline where the complexity is relatively lower, and fixes are cheaper and easier to implement. This becomes especially important when working with external stakeholders within financial constraints."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#limitations",
    "href": "blogs/data-smells-public-datasets/index.html#limitations",
    "title": "Data Smells in Public Datasets",
    "section": "Limitations",
    "text": "Limitations\nWe opted for a shallow as opposed to a deep analysis of this datasets. This means that we did not fit a model to each dataset and carry out a supervised learning task. While such a workflow may reveal more smells, we believe that the smells also become specific to the dataset, model or problem we are trying to solve. Our intention was to pick the smallest subset of analysis tasks that can be scaled across several datasets.\nThe smells are linked to the version of the dataset analysed. Unfortunately this is true for all data-centric work. However we do our best to make our results reproducible by providing the version of the data that was analysed in our paper.\nWe do not know the impact of the smells. For instance, if we consider the missing units smell, we do not know if and to what extent this smell affects the performance of a model. This was considered beyond the scope of this project however remains to be a viable extension to our current work.\nFinally, smells are subjective to the human. But this is true for code smells as well. Not all long methods are bad and god classes still exist in public software projects."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#future-work",
    "href": "blogs/data-smells-public-datasets/index.html#future-work",
    "title": "Data Smells in Public Datasets",
    "section": "Future Work",
    "text": "Future Work\nGrowing the catalogue using more datasets is a low-hanging fruit. It would also be interesting to explore the notion of smells for semi-structured and unstructured datasets. Finally, it would be interesting to understand the co-occurance and evolution of smells throughout the ML lifecycle."
  },
  {
    "objectID": "blogs/python-ast-extract-module-method-names/index.html",
    "href": "blogs/python-ast-extract-module-method-names/index.html",
    "title": "Extracting the Module and Function Names from Python ASTs",
    "section": "",
    "text": "Preliminaries: Python ast module\nPython has a built-in ast module which provides detailed documentation on the various nodes used to represent different elements of Python source code.\nWe can use the ast.parse method to create a AST from a given Python source code. Here is an example of how a function call is represented in a AST.\n\nimport ast\nast.parse(\"foo(x, y)\")\n\n&lt;ast.Module at 0x1063c3950&gt;\n\n\nThe ast.parse method returns a ast.Module object which by itself is not very helpful. To view the internal structure of the tree, we can use the ast.dump method.\n\nast.dump(ast.parse(\"foo(x, y)\"))\n\n\"Module(body=[Expr(value=Call(func=Name(id='foo', ctx=Load()), args=[Name(id='x', ctx=Load()), Name(id='y', ctx=Load())]))])\"\n\n\nWe can pass the indent argument to ast.dump along with a print statement to make the output more readable.\n\n\n\n\nListing 1: Base case\n\n\nprint(ast.dump(ast.parse(\"foo(x, y)\"), indent=4))\n\n\n\n\nModule(\n    body=[\n        Expr(\n            value=Call(\n                func=Name(id='foo', ctx=Load()),\n                args=[\n                    Name(id='x', ctx=Load()),\n                    Name(id='y', ctx=Load())]))])\n\n\nNote that the function call is represented by a ast.Call node which contains a func and args attribute. The function name (in our case, foo) is represented by a ast.Name node, with the actual name under the id attribute.\nAnd here is the AST when we want to use a function defined in a different module.\n\n\n\n\nListing 2: Single nested function call\n\n\nprint(ast.dump(ast.parse(\"bar.foo(x, y)\"), indent=4))\n\n\n\n\nModule(\n    body=[\n        Expr(\n            value=Call(\n                func=Attribute(\n                    value=Name(id='bar', ctx=Load()),\n                    attr='foo',\n                    ctx=Load()),\n                args=[\n                    Name(id='x', ctx=Load()),\n                    Name(id='y', ctx=Load())]))])\n\n\nThings are a bit different now. We see that the Call.func is no longer a ast.Name node, but instead an ast.Attribute node. Attribute.value is now a ast.Name node with the name of the module (in our case bar) on the id attribute and the name of the function on the attr attribute.\nLets examine something a bit more complicated: What if the function is in a submodule?\n\nprint(ast.dump(ast.parse(\"baz.bar.foo(x, y)\"), indent=4))\n\nModule(\n    body=[\n        Expr(\n            value=Call(\n                func=Attribute(\n                    value=Attribute(\n                        value=Name(id='baz', ctx=Load()),\n                        attr='bar',\n                        ctx=Load()),\n                    attr='foo',\n                    ctx=Load()),\n                args=[\n                    Name(id='x', ctx=Load()),\n                    Name(id='y', ctx=Load())]))])\n\n\nAnd even more nested?\n\nprint(ast.dump(ast.parse(\"quack.baz.bar.foo(x, y)\"), indent=4))\n\nModule(\n    body=[\n        Expr(\n            value=Call(\n                func=Attribute(\n                    value=Attribute(\n                        value=Attribute(\n                            value=Name(id='quack', ctx=Load()),\n                            attr='baz',\n                            ctx=Load()),\n                        attr='bar',\n                        ctx=Load()),\n                    attr='foo',\n                    ctx=Load()),\n                args=[\n                    Name(id='x', ctx=Load()),\n                    Name(id='y', ctx=Load())]))])\n\n\nIt seems that nested function calls are represented using nested ast.Attribute nodes. The top level module name is always a ast.Name node under the deepest ast.Attribute.value node. And the function name is always under the first ast.Attribute.attr node\n\n\nExtracting the Function Names\nLets start with the simplest case, where we are only interested in the function names (ie. we only want to extract foo from all scenarios presented above). There are two cases to consider here:\n\nIf its a direct function call, then the Call.func node will contain a Name node.\nIf its a nested function call, then Call.func will contain nested Attribute nodes. The name of the function will be under the first Attribute.attr.\n\nWe can do this using the ast.NodeVisitor class. Lets create a FunctionNameCollector class which inherits from ast.NodeVisitor. In the class, we define a visit_Name and visit_Attribute methods which are called every time we visit a Name or Attribute method respectively (more on this later).\n\nclass FunctionNameCollector(ast.NodeVisitor):\n    def __init__(self):\n        self.names = []\n\n    def visit_Name(self, node: ast.Name) -&gt; None:\n        self.names.append(node.id)\n\n    def visit_Attribute(self, node: ast.Attribute) -&gt; None:\n        self.names.append(node.attr)\n\n\ntests = [\"foo(x, y)\", \"bar.foo(x, y)\", \"baz.bar.foo(x, y)\", \"quack.baz.bar.foo(x, y)\"]\n\ntrees = [ast.parse(test) for test in tests]\ncall_nodes = [\n    node for tree in trees for node in ast.walk(tree) if isinstance(node, ast.Call)\n]\ncollector = FunctionNameCollector()\nfor node in call_nodes:\n    collector.visit(node.func)\n\ncollector.names\n\n['foo', 'foo', 'foo', 'foo']\n\n\nI collect all the ast.Call nodes in our test cases using the ast.walk function which returns a generator that yields every child node under the given AST. Then I call the visit method provided by ast.NodeVisitor which visits only the direct child nodes of all Call.func nodes in our test cases.\n\n\nCollecting Both Module and Function Names\nHere is where things get a bit more interesting. Here are the cases to consider:\n\nThe base case is that we have a direct function call, in which case we need to extract the function name from Name.id under Call.func (same as before).\nHowever, if it is a nested function call, then:\n\nThe function name will be under the first Attribute.attr and\nThe module name will be under the last Attribute.value.id.\n\n\nSo the visit_Name method remains the same however, we do need to modify the visit_Attribute method such that it traverses all child nodes under Attribute.value until we hit the base case. Here is the modified code.\n\nclass NameCollector(ast.NodeVisitor):\n    def __init__(self):\n        self.names = []\n        self.stack = []\n\n    def visit_Name(self, node: ast.Name) -&gt; None:\n        if self.stack:\n            self.names.append((node.id, self.stack[0].attr))\n        else:\n            self.names.append((None, node.id))\n\n    def visit_Attribute(self, node: ast.Attribute) -&gt; None:\n        self.stack.append(node)\n        self.visit(node.value)\n\ncollector = NameCollector()\nfor node in call_nodes:\n    collector.visit(node.func)\n\ncollector.names\n\n[(None, 'foo'), ('bar', 'foo'), ('baz', 'foo'), ('quack', 'foo')]\n\n\nThe code is similar to FunctionNameCollector defined above, with a few key changes. The collector.names now returns a list of tuples containing the module, function names.\nI use a stack to keep track of the Attribute nodes we visit. Whenever we visit an Attribute node, I append it to the stack and then call the NodeVisitor.visit method on the Node under its value attribute.\nWhen we visit a Name node, it can either be because we are at the base case (a direct function call) or because we have reached the last Attribute.value node. If the stack is not empty, then its the latter which means the Name node is the module name and the function name is under the first Attribute.attr in the stack. Otherwise, its a direct function call so the Name node is the function name. Since we don’t have a module in this case, we return None in the tuple.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/mytags/index.html",
    "href": "blogs/mytags/index.html",
    "title": "Generating tags for git repositories (mytags)",
    "section": "",
    "text": "Often while working on remote servers where I don’t have permissions to install language servers, I find ctags to be an effective tool to navigate code. mytags is a simple Bash script I wrote which wraps around ctags with some added functionality.\nHere is the script as of 2024-11-27, you can find the latest version in my dotfiles repo.\n\n\nmytags\n\n#!/usr/bin/env bash\n\n__is_git_repo() {\n  git rev-parse &&gt;/dev/null\n}\n\n__git_toplevel() {\n  git rev-parse --show-toplevel\n}\n\n__ctags() {\n  if [[ -x \"$(command -v fd)\" ]]\n  then\n5    command ctags \"$@\" $(fd --type f --hidden --exclude '.git')\n  else\n    command ctags \"$@\" $(git ls-files --exclude-standard)\n  fi\n}\n\n1if ! __is_git_repo\nthen\n  echo \"mytags: not inside git repo.\"\n  exit 1\nfi\n\n2(\n3  cd $(__git_toplevel) &&\n4  __ctags \"$@\"\n)\n\n\n1\n\nmytags check that we are inside a git repo, if not exit gracefully.\n\n2\n\nWhen inside a git repo, start a new subshell and…\n\n3\n\ncd to the top-level of the repo (i.e. where the .git folder is located), this allows us to run mytags from any subdirectory inside the git repo\n\n4\n\nexecute ctags with relevant files, either using fd if it exists or git ls-files\n\n5\n\nany additional arguments are passed along to ctags\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/doi2bib/index.html",
    "href": "blogs/doi2bib/index.html",
    "title": "Automatically retrieving Bibtex information from DOI",
    "section": "",
    "text": "doi2bib is a simple Python script I wrote to automatically retrieve bibtex information for a given DOI. The script queries Crossref to obtain the bibtex. The script can also handle pre-prints published on Arxiv. Here is the content of doi2bib along with some explaination of what the script does.\n\n\ndoi2bib\n\n#!/usr/bin/env python3\n\nimport sys\nimport argparse\nfrom urllib import request, error\n\n3def get_bibtex(doi,ispreprint):\n    if ispreprint:\n        url = f\"https://arxiv.org/bibtex/{doi}\"\n    else:\n        url = f\"https://api.crossref.org/works/{doi}/transform/application/x-bibtex\"\n    req = request.Request(url)\n\n    try:\n        with request.urlopen(req) as response:\n            return response.read().decode()\n    except error.HTTPError as e:\n        return f\"HTTP Error: {e.code}\"\n    except error.URLError as e:\n        return f\"URL Error: {e.reason}\"\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n            description=\"Convert DOI or Arxiv ID to Bibtex\"\n            )\n1    parser.add_argument(\n            \"doi\",\n            help=\"DOI or Arxiv ID of paper\"\n            )\n2    parser.add_argument(\n            \"-p\",\n            \"--preprint\",\n            help=\"Treat provided DOI as Arxiv ID\",\n            action=\"store_true\",\n            default=False,\n            )\n    args = parser.parse_args()\n\n    bibtex = get_bibtex(args.doi, args.preprint)\n    print(bibtex)\n\n\n1\n\nThe script must be provided with a DOI. This can also be an Arxiv ID.\n\n2\n\nThe -p or --preprint flag can be specified to indicate that the provided DOI is an Arxiv ID.\n\n3\n\nThe Crossref API is queried with the provided DOI to retrieve the bibtex information. With the --preprint flag, the Arxiv API is used.\n\n\nI pipe the results through the bib-tool CLI, to format the text and generate a unique key. I specify the following key format in my .bibtoolrsc file.\n\n\n.bibtoolrsc\n\nprint.use.tab=off\nfmt.et.al=\"\"\nkey.format=\"%-1n(author)%4d(year)%-T(title)\"\n\nBy default, bibtool uses tabs for indentation. I turn this off. Bibtool adds “.ea” to the author name to indicate “and others”. I prefer to just have the last name of the first author in the key, so I set it to an empty string. I set the format of the key to the last name of the first author, followed by the year of publication and the first meaningful word from the title.\nHere is the script in action, I use one of my own publications as an example.\n$ doi2bib 10.1145/3522664.3528621 |bibtool -k\n\n@InProceedings{   shome2022data,\n  series        = {CAIN ’22},\n  title         = {Data smells in public datasets},\n  url           = {http://dx.doi.org/10.1145/3522664.3528621},\n  doi           = {10.1145/3522664.3528621},\n  booktitle     = {Proceedings of the 1st International Conference on AI\n                  Engineering: Software Engineering for AI},\n  publisher     = {ACM},\n  author        = {Shome, Arumoy and Cruz, Luís and van Deursen, Arie},\n  year          = {2022},\n  month         = may,\n  collection    = {CAIN ’22}\n}\nAnd here is another example using an Arxiv ID (again, one of my own).\n$ doi2bib --preprint 2305.04988 |bibtool -k\n\n@Misc{            shome2023towards,\n  title         = {Towards Understanding Machine Learning Testing in\n                  Practise},\n  author        = {Arumoy Shome and Luis Cruz and Arie van Deursen},\n  year          = {2023},\n  eprint        = {2305.04988},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.SE}\n}\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/remla-data-validation/index.html",
    "href": "blogs/remla-data-validation/index.html",
    "title": "Data Validation with TFDV",
    "section": "",
    "text": "Note\n\n\n\nI gave this guest lecture once again in the 2023 iteration of the REMLA course.\n\n\nIn this lecture we will go over the basics of data validation. The first half of this lecture will be a talk on the fundamentals of data validation. We will answer what is data validation?, why should we validate our data? and how we can validate our data?. The second half of the lecture will be a hands-on tutorial on using Tensorflow Data Validation, instructions & code for which can be found on this github repo.\n\nWhat is data validation?\nI like to think of data validation in terms of expectations vs. reality. When working with data, we tend to have many implicit expectations from our data and data validation allows us to make such expectations explicit but defining validation rules.\nAnother (perhaps more technical) perspective would be that data validation is equivalent to data testing. This may include testing for presence of data, the data type of the columns (int, float or string) and statistical tests pertaining to the distribution of the feature.\n\n\nWhy should we validate our data?\nLets answer this question with an example. Lets assume we are working on a project which involves working with tabular data presented in Figure Figure 1. The dataset contains several numerical features such as the area & perimeter of the tumour and we want to train a model to predict whether the tumour is malignant or benign.\n\n\n\n\n\n\nFigure 1: Example dataset\n\n\n\nAnd lets say—being the ML experts that we are—we do some experimentation with various models and we manage to find one that fits the data well. We evaluate the model with a test set and achieve an acceptable value for the metric we are checking (accuracy, precision, recall or something else). Everybody is happy, you give yourself a pat on the back for a job well done, and call it a day.\nThis is a typical ML workflow which we tend to see in academia or in an educational setting. Turns out however, that the ML model related work is a single component of a much larger system as see in Figure Figure 2.\n\n\n\n\n\n\nFigure 2: ML Production Components\n\n\n\nContinuing along with the theme of data, lets dive deeper into the data collection stage. There may be several sources of data for the model. For instance, there may be a web service which is continually scrapping the internet for data, or we may have data stored in a database, a data warehouse or data lake.\nIn addition, we may have several systems with or without ML components which our system communicates with. For instance, in Figure Figure 3 our system may rely on data from another service. In return, other services may depend on the predictions from our system.\n\n\n\n\n\n\nFigure 3: ML Spaghetti\n\n\n\nMy point here is that ML pipelines are inherently complex and tangled. They consist of several stages and the ML model work tends to be a small part of a much larger system. A small change or bug in any of the stages ripples throughout the entire pipeline. Therefore, we cannot make implicit assumptions regarding the quality of the data.\nContinuing with the scenario of cancer detection, lets say that we have now managed to deploy our ML model in production. After a certain period of time (days, weeks or months) we may decide to re-train the model due to degrade in performance (perhaps the accuracy is lower than it used to be). The next batch of training data is typically generated by combining the unlabelled data in production with the predictions our live model is making as seen in Figure Figure 4.\n\n\n\n\n\n\nFigure 4: ML Production Training\n\n\n\nHowever, what happens if we no longer track the area_mean feature? Or what if we start tracking the numerical features in centimetres rather than millimetres? Or what if we use comma instead of periods to denote decimals?\nWith the exception of the last example, changes are that our pipeline continues to work albeit with a degraded performance. This is because we hold several implicit expectations from our data based on the training data which was used however the data in production may tell a completely different story. Thus it is important to make such expectations explicit by validating our data and catch data quality issues from feedback loops.\n\n\nHow should we validate our data?\nAlthough data validation has existing in the domain of database management systems for a very long time, its application in ML is new and still evolving. In this part of the talk I will present the theoretical principles on which tfdv operates.\nWe first generate a schema from the training data. A schema defines what we want our data to look like. For instance, for our cancer dataset, the schema may specify the columns we expect in the dataset, their data types and the distribution of each feature. Next, we gather statistics from the dataset we want to validate (this can be the local test set or the live data from production). Finally, we compare the statistics against the schema to make sure that they match. Figure Figure 5 puts a software engineering lens on how data validation works. The schema can be thought of as the test oracle and the live data is the current state. And we validate the two to ensure that there are no bugs in the live dataset.\nIt is important to realise that the schema generated by tfdv is best effort and the ML practitioner is still required to tweak the schema based on their understanding of the data & domain expertise.\n\n\n\n\n\n\nFigure 5: Data Validation How\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/organise-research-project-git/index.html",
    "href": "blogs/organise-research-project-git/index.html",
    "title": "Organising research projects with git",
    "section": "",
    "text": "In this post I list some of the standards and conventions I have developed over the course of my PhD to organise research project data. I use git to version control the relevant files and host them on Github."
  },
  {
    "objectID": "blogs/organise-research-project-git/index.html#additionally-for-the-subject-line",
    "href": "blogs/organise-research-project-git/index.html#additionally-for-the-subject-line",
    "title": "Organising research projects with git",
    "section": "Additionally for the subject line",
    "text": "Additionally for the subject line\nFor instance, say you want to commit a first draft of your paper. Use a subject line as follows:\nfeat(report): init icse24 paper draft\nWhere feat is an abbreviation for “feature”. The braces specify the folder within which the changes were made, and the title provides a quick description of the change.\nIn contrast, say you refactor the data processing pipeline script. The subject line could be as follows:\nrefac(bin): remove magic numbers in data-process.sh\nThis makes it very easy to track down prior commits that introduced changes in the paper versus in the code. For instance, you can target all refactoring commits within the report/ directory using the following git grep command:\ngit log --grep 'refac.*report.'"
  },
  {
    "objectID": "blogs/website-management-pandoc/index.html",
    "href": "blogs/website-management-pandoc/index.html",
    "title": "There and Back Again A Tale of Website Management",
    "section": "",
    "text": "After years of using orgmode along with the org-publish package to run my website, I came back to markdown, shell & vim.\nIn my humble shell dwelling days—before I began my journey into Emacs land—I was using Jekyll. Rather, I was fighting with it. Github requires a CNAME file in the directory from which the website should be served. Now, the github-pages gem can be used to instruct Github Pages (GHP) to automatically build and serve the website. But I faced several challenges getting the compatible versions of the github-pages, jekyll and ruby to match.\nI decided to forgo this madness and just use html & css to build my website. I used org-publish to accomplish this using the following setup in my init.el.\nSee the documentation for org-publish-project-alist on how to setup org-publish.\n\n\n~/.emacs.d/init.el\n\n(setq org-publish-project-alist\n '((\"org\" :components (\"org-posts\" \"org-static\"))\n   (\"website-posts\"\n    :base-directory \"~/code/arumoy\"\n    :base-extension \"org\"\n    :publishing-directory \"~/code/arumoy/docs/\"\n    :section-numbers nil\n    :auto-preamble t\n    :auto-sitemap t\n    :html-head \"&lt;link rel=\\\"stylesheet\\\" href=\\\"assets/css/main.css\\\" type=\\\"text/css\\\"/&gt;\"\n    :publishing-function org-html-publish-to-html)\n   (\"website-static\"\n    :base-directory \"~/code/arumoy/assets\"\n    :base-extension \"css\\\\|js\\\\|png\\\\|jpg\\\\|gif\\\\|pdf\\\\|mp3\\\\|ogg\\\\|swf\"\n    :publishing-directory \"~/code/arumoy/docs/assets/\"\n    :recursive t\n    :publishing-function org-publish-attachment)\n   (\"website-cname\"\n    :base-directory \"~/code/arumoy/\"\n    :base-extension \"\"\n    :publishing-directory \"~/code/arumoy/docs/\"\n    :include (\"CNAME\")\n    :publishing-function org-publish-attachment)\n   (\"website\" :components (\"website-posts\" \"website-static\" \"website-cname\"))))\n\nSince org-publish wipes the :publishing-directory clean prior to each build, I copy the CNAME file back in there.\nI was very pleased with its simplicity and its text-centric nature. The fact that it just worked out of the box was a pleasant surprise. However this intricate setup only worked in Emacs and this did not sit well with me. So I decided to find a more universal solution and landed on Pandoc.\nPandoc has the --standalone flag which produces a document which is valid on its own (think HTML documents with header and footer). One can write custom templates to produce documents styled to their liking. The default template can be viewed using pandoc -R FORMAT. A custom template can be specified using the --template flag. See section on templates in the pandoc manual for more info.\nFollowing the advice laid out by https://jgthms.com/web-design-in-4-minutes/, I designed a minimal pandoc custom template which you can find my in dotfiles repo.\nMy current workflow comprises of authoring content in markdown which I edit in vim. I use GNU make to automate the html generation using pandoc. The contents of my Makefile are as follows.\n\n\nMakefile\n\n# Taken from &lt;https://gist.github.com/kristopherjohnson/7466917&gt;\n\nSRCFILES:= $(wildcard *.md)\nPUBFILES=$(SRCFILES:.md=.html)\n\n%.html: %.md\n    pandoc --template=public -o docs/$@ $&lt;\n\n# Targets and dependencies\n\n.PHONY: all clean\n\nall : $(PUBFILES)\n\nclean:\n    rm $(PUBFILES)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aocp/index.html",
    "href": "blogs/aocp/index.html",
    "title": "Aru’s Org Capture Template (aocp.el)",
    "section": "",
    "text": "After observing my workflow of managing bibliographic information in Emacs, I extracted the repeated actions into an Emacs package.\nTo gain some perspective on my workflow, see my prior article on my research workflow.\nThe package is available on github with two alternative installation methods: 1. By manually downloading the aocp.el file and sticking it in your Emacs load-path, or 2. Using straight.el which is what I recommend.\nThe package works under the assumption that you manage your bibliographic information in org-mode (a major-mode for Emacs). The functions made available through this package are intended to be used in an org-capture template, they are not meant to be called interactively (ie. by using M-x).\nAssuming that you have a bibtex entry in your kill-ring (either by killing text within Emacs or by coping text from an external application into your clipboard), this package will do the following:\n\nExtract the bibkey\nExtract the first author\nExtract the last author\nExtract the source of publication\n\n* TODO %(aocp--get-bibkey nil)\n  :PROPERTIES:\n  :PDF: file:~/Documents/papers/%(aocp--get-bibkey t).pdf\n  :FIRST_AUTHOR: %(aocp--get-first-author)\n  :LAST_AUTHOR: %(aocp--get-last-author)\n  :SOURCE: %(aocp--get-source)\n  :END:\n%?\n+ problem statement ::\n+ solution ::\n+ results ::\n+ limitations ::\n+ remarks ::\n\n  #+begin_src bibtex :tangle yes\n  %c\n  #+end_src\nAssuming you have the above template in paper.txt, you can configure org as follows (replace your-org-inbox-file appropriately):\n(setq org-capture-templates\n    '((\"p\" \"Paper\" entry (file+headline your-org-inbox-file \"Inbox\")\n    \"%[~/.emacs.d/org-templates/paper.txt]\")))\nWith this in place, you can quickly collect all bibliographic information within an org file. Leveraging the powerful functionality provided by org-properties, one can quickly find relevant papers. For instance, I can look up all papers by author X or all papers by author X published at Y.\nA nice little tip is to download a local copy of the pdf and save them all in a folder. To make this easier, aocp.el also pushes the bibkey to the kill-ring. So all that is left to do is click the download button and paste the bibkey as the file name. This ensure 1. That you have all pdfs names consistently and 2. You have a link to the pdf from your org file (see the :PDF: property in the template above) which you can open by hitting C-c C-o over the link. You do not need to poke around in the directory containing the pdfs, all the context is available in the org file and should be the point of entry for all your bibliographic needs!\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/today/index.html",
    "href": "blogs/today/index.html",
    "title": "Timestamps in the Shell (today)",
    "section": "",
    "text": "I often work with text files containing pros (such as blog posts and git commit messages) and require adding a timestamp containing the current date, day & time.\nI wrote today, a shellscript which returns the current date in various formats. Here is the script as of 2022-03-09, the latest version can be found in my dotfiles.\n\n\ntoday\n\n#!/usr/bin/env bash\n\n# today: return today's date in various formats\n\n# Usage: today [OPTS]\n# Without any options, today will print today's date in %Y-%m-%d\n# format. Following are the supported options:\n#    --with-day: %Y-%m-%d %a\n#    --with-time: %Y-%m-%d %H:%M\n#    -l | --long: %Y-%m-%d %a %H:%M\n#    -h | --human: %a %b %d, %Y\n#    -s | --stamp: enclose the date in square braces\n\n# NOTE: when using both --with-day & --with-time, the order in which\n# the options are passed matters. For example:\n#\n# today --with-day --with-time will produce\n#    2022-03-03 Thu 02:20\n#\n# But today --with-time --with-day will produce\n# 2022-03-03 02:21 Thu\n\n# NOTE: when using --human option, all other options are ignored.\n\nmain() {\n  local FMT='%Y-%m-%d'\n  local STAMP_FLAG=1 # false\n\n  while [[ \"$1\" =~ ^- && ! \"$1\" == \"--\" ]]; do\n    case \"$1\" in\n      --with-day)\n        FMT=\"$FMT %a\"\n        ;;\n      --with-time)\n        FMT=\"$FMT %H:%M\"\n        ;;\n      -s | --stamp)\n        STAMP_FLAG=0 # true\n        ;;\n      -l | --long) # short for --with-day --with-time\n        FMT=\"$FMT %a %H:%M\"\n        ;;\n      -h | --human) # alternate format, ignore other flags\n        FMT=\"%a %b %d, %Y\"\n        ;;\n      *)\n        echo \"Error: unknown option $1.\"\n        return 1\n    esac; shift # only shift here since we only pass flags\n  done\n\n  local OUT=$(date +\"$FMT\")\n\n  [[ \"$STAMP_FLAG\" -eq 0 ]] && OUT=\"[$OUT]\"\n\n  echo \"$OUT\"\n}\n\nmain \"$@\"\n\nWithout any arguments, today prints the date in ‘%Y-%m-%d’ format. Using the following optional flags the output can be manipulated.\n\n\n\nflag\noutput\n\n\n\n\n–with-day\n‘%Y-%m-%d %a’\n\n\n–with-time\n‘%Y-%m-%d %H:%M’\n\n\n–long\n‘%Y-%m-%d %a %H:%M’\n\n\n–human\n‘%a %b %d, %Y’\n\n\n\nThe --stamp flag can be used to optionally wrap the output in square braces.\nI frequently use this to add a timestamp to my git commit messages.\ngit commit -m \"feat: timestamps from the shell $(today -l -s)\"\nOr insert a timestamp into the current buffer I am editing in vim.\n:r! today -l -s\n\n\n\n Back to top"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Following is a list of public talks I have given in the past.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\nabstract\n\n\n\n\n\n\n\n\nTowards Automatic Translation of Machine Learning Visual Insights to Analytical Assertions\n\n\nApr 20, 2024\n\n\nPaper presentation at International Workshop on Natural Language-Based Software Engineering (NLBSE) 2024. \n\n\n\n\n\n\nData vs. Model Fairness Testing: An Empirical Study\n\n\nApr 20, 2024\n\n\nPaper presentation at International Workshop on Deep Learning for Testing and Testing for Deep Learning (DeepTest) 2024 workshop. \n\n\n\n\n\n\nData vs. Model Fairness Testing: An Empirical Study\n\n\nApr 18, 2024\n\n\nPoster presentation at International Conference of Software Engineering (ICSE) 2024. \n\n\n\n\n\n\nBridging the Gap Between Visual and Analytical Machine Learning Testing\n\n\nJun 2, 2023\n\n\nLightening talk at SEN Symposium 2023. \n\n\n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nMay 15, 2023\n\n\nPoster presentation at International Conference on AI Engineering (CAIN) 2023. \n\n\n\n\n\n\nData Validation with TFDV\n\n\nMay 16, 2022\n\n\nGuest lecture for Release Engineering for Machine Learning Applications course at TU Delft. Materials for the hands-on tutorial on using Tensorflow Data Validation, instructions & code can be found in this github repo. \n\n\n\n\n\n\nData Smells in Public Datasets\n\n\nMay 4, 2022\n\n\nPaper presentation at the International Conference on AI Engineering (CAIN) 2022. \n\n\n\n\n\n\nPrivacy Preserving Deep Learning\n\n\nSep 7, 2021\n\n\nA talk on Privacy Preserving Deep Learning (PPDL) I gave to my research group. It was largly based on a literature review I did during my Msc. \n\n\n\n\n\n\nResearch Workflow in Plaintext\n\n\nJul 12, 2021\n\n\nTalk on using Emacs and org-mode to craft a research workflow. \n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Following is a list of my scientific publications.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Venue\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Authors\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nVenue\n\n\n\nDate\n\n\n\nAuthors\n\n\n\n\n\n\n\n\nUnderstanding Feedback Mechanisms in Machine Learning Jupyter Notebooks\n\n\nJournal of Empirical Software Engineering (EMSE)–Under Review\n\n\nAug 1, 2024\n\n\nArumoy Shome, Luis Cruz, Diomidis Spinellis, Arie van Deursen\n\n\n\n\n\n\nData vs. Model Machine Learning Fairness Testing: An Empirical Study (Extended Abstract)\n\n\nProceedings of 46th International Conference on Software Engineering (Companion Track)\n\n\nJan 22, 2024\n\n\nArumoy Shome, Luis Cruz, Arie van Deursen\n\n\n\n\n\n\nData vs. Model Machine Learning Fairness Testing: An Empirical Study\n\n\nProceedings of 5th International Workshop on Deep Learning for Testing and Testing for Deep Learning\n\n\nJan 12, 2024\n\n\nArumoy Shome, Luis Cruz, Arie van Deursen\n\n\n\n\n\n\nTowards Automatic Translation of Machine Learning Visual Insights to Analytical Assertions\n\n\nProceedings of 3nd International Workshop on NL-based Software Engineering\n\n\nJan 12, 2024\n\n\nArumoy Shome, Luis Cruz, Arie van Deursen\n\n\n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nProceedings of 2nd International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 15, 2023\n\n\nArumoy Shome, Luis Cruz, Arie van Deursen\n\n\n\n\n\n\nData Smells in Public Datasets\n\n\nProceedings of 1st International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 1, 2022\n\n\nArumoy Shome, Luis Cruz, Arie van Deursen\n\n\n\n\n\n\nPrivacy Preserving Deep Learning for Medical Imaging\n\n\nMsc. Systematic Literature Review, unpublished\n\n\nDec 1, 2020\n\n\nArumoy Shome, Saba Amiri, Adam Belloum\n\n\n\n\n\n\nKM3NeT Neutrino Detection using Deep Learning\n\n\nMsc. Thesis, unpublished\n\n\nOct 29, 2020\n\n\nArumoy Shome, Adam Belloum, Ben van Werkhoven, Ronald Bruijn\n\n\n\n\n\n\nACE: Art, Color and Emotions\n\n\nProceedings of the 27th ACM Conference on Multimedia\n\n\nOct 1, 2019\n\n\nGjorgji Strezoski, Arumoy Shome, Riccardo Bianchi, Shruti Rao, Marcel Worring\n\n\n\n\n\n\nImproving the Cognitive Assessment of Individuals with Down Syndrome\n\n\nBsc. Capstone Project, unpublished\n\n\nJan 1, 2018\n\n\nMaathusan Rajendram, Arumoy Shome, Mira Sleiman\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "This page contains links to some of my writings on topics that interest me. Usually, they are inspired by problems that I experience in my day-to-day life.\nI like pondering over the act or the process of doing something.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nRenaming files so they make sense (rename)\n\n\n\nshell\n\npython\n\n\n\nrename is a bash script I wrote to automatically rename long files (the way I like it).\n\n\n\n\n\nNov 28, 2024\n\n\n\n\n\n\n\nGenerating tags for git repositories (mytags)\n\n\n\nshell\n\n\n\nmytags is a wrapper around ctags which respects your gitignore files.\n\n\n\n\n\nNov 17, 2024\n\n\n\n\n\n\n\nExtracting the Module and Function Names from Python ASTs\n\n\n\npython\n\n\n\nHow to extract the module and function name from Python Abstract Syntax Trees.\n\n\n\n\n\nMar 23, 2024\n\n\n\n\n\n\n\nVisualisation Zoo\n\n\n\npython\n\ndesign\n\n\n\nCollection of data visualisations I have created using Python.\n\n\n\n\n\nMar 17, 2024\n\n\n\n\n\n\n\nOrganising research projects with git\n\n\n\nresearch\n\nproductivity\n\n\n\nSome standards and conventions I follow when organising research project data using git.\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\nToggling background color in kitty and vim (yob)\n\n\n\nshell\n\nvim\n\n\n\nyob is a tiny shell script which toggles between a light and dark colorscheme in Kitty, my terminal of choice.\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\nManaging Scientific Bibliography using Emacs Org-mode\n\n\n\nemacs\n\nproductivity\n\n\n\nHow I organise, search and retrieve my scientific papers using Emacs org-mode.\n\n\n\n\n\nNov 29, 2023\n\n\n\n\n\n\n\nAutomatically retrieving Bibtex information from DOI\n\n\n\nshell\n\nproductivity\n\n\n\ndoi2bib is a simple Python script I wrote that fetches bibtex information from the Crossref API using the provided DOI. It can also handle pre-prints published on Arxiv.\n\n\n\n\n\nNov 26, 2023\n\n\n\n\n\n\n\nCMS using Pandoc and Friends\n\n\n\nshell\n\nweb\n\n\n\nSome tools & techniques I use to run a no non-sense blog using static html pages. All powered by a sane file naming convension, plaintext documents writing in markdown and exported to html using pandoc and other unix cli tools.\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\nData Validation with TFDV\n\n\n\ndata\n\nresearch\n\nSE4AI\n\n\n\nIn this lecture we will go over the basics of data validation. The first half of this lecture will be a talk on the fundamentals of data validation. We will answer what is data validation?, why should we validate our data? and how we can validate our data?. The second half of the lecture will be a hands-on tutorial on using Tensorflow Data Validation, instructions & code for which can be found on this github repo.\n\n\n\n\n\nMay 16, 2022\n\n\n\n\n\n\n\nEffortless Parallel Execution with xargs & Friends\n\n\n\nshell\n\n\n\nRecently, I had to run Tensorflow Data Validation on over 500 public datasets from Kaggle to generate a baseline schema file for further analysis. I chose to do this using the xargs unix command.\n\n\n\n\n\nMay 8, 2022\n\n\n\n\n\n\n\nData Smells in Public Datasets\n\n\n\ndata\n\nresearch\n\nSE4AI\n\ntechnical debt\n\n\n\nIn this talk I will present our recent paper titled Data Smells in Public Datasets which was published at the 1st International Conference on AI Engineering (CAIN) 2022. I will first present the problem we are trying to solve along with the contributions that we made. I will present the methodology which was followed along with the results obtained. I will present a select few smells which I personally find interesting & hope will generate some discussion. Finally, we will conclude the talk with some high level takeaways from our study along with the limitations & future directions of work.\n\n\n\n\n\nMay 4, 2022\n\n\n\n\n\n\n\nThere and Back Again A Tale of Website Management\n\n\n\nshell\n\nvim\n\nweb\n\n\n\nManaging websites using markdown, shell and vim.\n\n\n\n\n\nMar 4, 2022\n\n\n\n\n\n\n\nTimestamps in the Shell (today)\n\n\n\nshell\n\nproductivity\n\n\n\nCreating timestamps in the terminal.\n\n\n\n\n\nMar 3, 2022\n\n\n\n\n\n\n\nAru’s Information Management System (AIMS)\n\n\n\nshell\n\nproductivity\n\n\n\nAIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\nPrivacy Preserving Deep Learning\n\n\n\nmachine learning\n\nresearch\n\nprivacy\n\n\n\nA talk on Privacy Preserving Deep Learning (PPDL) I gave to my research group. It was largly based on a literature review I did during my Msc.\n\n\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n\nResearch Workflow in Plaintext\n\n\n\nproductivity\n\nemacs\n\nresearch\n\n\n\nIn this talk I will go over how we can use Emacs and org-mode to craft a research workflow. We will look at how we can leverage the power of Emacs and org-mode to capture, store, search and retrieve research data, all in plain text! The talk will touch upon how org-mode can be used as an environment for literate programming and reproducible research. I do not assume any prior knowledge of emacs or org-mode and I want this to be more of a discussion rather than a talk. Please ask me questions as I go along and share your thoughts, tips and techniques with others!\n\n\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\nAru’s Org Capture Template (aocp.el)\n\n\n\nemacs\n\nproductivity\n\n\n\nAn Emacs package I wrote for managing bibliographic information.\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "blogs/aims/index.html",
    "href": "blogs/aims/index.html",
    "title": "Aru’s Information Management System (AIMS)",
    "section": "",
    "text": "AIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\nAlthough org-mode is great, the primary motivation for writing AIMs is because I was feeling a lot of resistance when trying to tune it to my workflow, primarily because of Elisp. Org-mode also requires that you use Emacs as your text editor. I did not appreciate the “vendor lock-in” enforced by org-mode.\nYou can find the latest version of the script on my dotfiles repo, below is the script as it stands on 2022-02-28.\n\n\naims\n\n#!/usr/bin/env bash\n\n1NOTESDIR=\"$HOME/org\"\nINBOX=\"$NOTESDIR/inbox.md\"\nTEMPLATESDIR=\"$XDG_DATA_HOME/aims\"\n[[ ! -e \"$INBOX\" ]] && touch \"$INBOX\"\n\n__capture() {\n  # Capture incoming info quickly. All items are appended to INBOX\n  # which defaults to `inbox.md' in NOTESDIR. Optionally a template\n  # can be specified using the --template| -t flag.\n2  local TEMPLATE=\"$TEMPLATESDIR/default\"\n\n  while [[ \"$1\" =~ ^-+.* && ! \"$1\" == \"--\" ]]; do\n    case \"$1\" in\n      --template | -t)\n        shift\n        TEMPLATE=\"$TEMPLATESDIR/$1\"\n        ;;\n      *)\n        echo \"Error: unknown option $1.\"\n        return 1\n        ;;\n    esac; shift\n  done\n\n3  local ITEM=$(mktemp)\n  if [[ -e \"$TEMPLATE\" && -x \"$TEMPLATE\" ]]; then\n    eval \"$TEMPLATE $ITEM\"\n  fi\n\n4  if eval \"$EDITOR -c 'set ft=markdown' $ITEM\"; then\n    [[ \"$1\" && -e \"$NOTESDIR/$1\" ]] && INBOX=\"$NOTESDIR/$1\"\n    cat \"$ITEM\" &gt;&gt; \"$INBOX\"\n    echo \"Info: captured in $INBOX.\"\n  fi\n\n5  echo \"Info: cleaning up $(rm -v \"$ITEM\")\"\n}\n\n__capture \"$@\"\n\n\n1\n\nStore all notes in $HOME/org/inbox.md, creating it if necessary. Also look for template scripts in ~/.local/share/aims.\n\n2\n\nParse the flags passed to AIMS. Currently it only supports the --template/-t flag which accepts the name of the template to use. Use the default template if none is provided. More on this later.\n\n3\n\nCreate a temporary file and insert the contents of the template.\n\n4\n\nEdit the temporary file using $EDITOR (here I assume its vim or neovim), setting the filetype to markdown. If the first positional argument passed to AIMS is a valid file inside $NOTESDIR then set that to the $INBOX file. Finally, prepend the contents of the temporary file to $INBOX file, if vim does not report an error.\n\n5\n\nCleanup, remove the temporary file.\n\n\nFor the time being, it only provides the capture functionality. A temporary file is used to compose the text first. Upon successful completion, the contents of the temporary file are appended to the default $INBOX file if no other files are specified.\nWhat I find really neat is the templating system. An arbitrary name for a template can be passed to aims using the --template (or -t for short) flag. aims looks for a shellscript with the same name in the ~/.local/share/aims directory and executes it if it exists. The beauty of this design is in its simplicity. Since templates are shellscripts, it gives us the full expressiveness of the shell. This is best demonstrated with some examples. Here is my default template as of 2022-02-28 which is used when no template is specified.\n\n\n~/.local/share/aims/default\n\n#!/usr/bin/env bash\n\n1[[ -z \"$1\" ]] && return 1\n\n2echo &gt;&gt; \"$1\"\necho \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\n\n\n1\n\nSanity check, ensure that a positional argument was passed (that is, the temporary file path).\n\n2\n\nInsert an empty line and a level 1 markdown header with a time stamp.\n\n\nIt simply adds a level 1 markdown header followed by a timestamp. Here is another for capturing bibtex information for research papers.\n\n\n\n\n\n\nTip\n\n\n\nI also wrote aocp.el, an emacs package to capture bibtex information of research papers using org-mode.\n\n\n#!/usr/bin/env bash\n\n[[ -z \"$1\" ]] && return 1\n\necho &gt;&gt; \"$1\"\n\n1BIBKEY=$(pbpaste | grep '^@.*' | sed 's/^@.*{\\(.*\\),/\\1/')\nif [[ -n \"$BIBKEY\" ]]; then\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')] $BIBKEY\" &gt;&gt; $1\nelse\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\nfi\n\n2echo &gt;&gt; \"$1\"\necho '+ **Problem Statement:**' &gt;&gt; \"$1\"\necho '+ **Solution**' &gt;&gt; \"$1\"\necho '+ **Results**' &gt;&gt; \"$1\"\necho '+ **Limitations**' &gt;&gt; \"$1\"\necho '+ **Remarks**' &gt;&gt; \"$1\"\necho &gt;&gt; \"$1\"\n3echo '```bibtex' &gt;&gt; \"$1\"\n\nif [[ -n \"$BIBKEY\" ]]; then\n  pbpaste | sed '/^$/d' &gt;&gt; \"$1\"\n  pbcopy &lt;(echo \"$BIBKEY\")\nfi\n\necho '```' &gt;&gt; \"$1\"\n\n1\n\nCheck that the bibtex information is currently in the system clipboard by attempting to extract the key using grep and sed. If a key was successfully extracted, then create a level 1 markdown header with a time stamp and the key. Otherwise, fall back to just a time stamp.\n\n2\n\nAdd my prompts for note-taking when reading scientific papers.\n\n3\n\nRemove empty lines and put the bibtex information in a markdown source block.\n\n\nThis one is a bit more involved but highlights the power of using shellscripts for templating. Given that a bibentry is copied in the clipboard, this template adds a level 1 markdown header with a timestamp and the bibkey. It adds my note-taking prompts and sticks the bibentry at the bottom.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/cms-pandoc/index.html",
    "href": "blogs/cms-pandoc/index.html",
    "title": "CMS using Pandoc and Friends",
    "section": "",
    "text": "ImportantMigrated to Quarto\n\n\n\nSince 2023-07-01, I have been using Quarto to manage my website.\nThe CMS system presented here works well. However, I felt the need for features such as code annotations and custom document layouts (to name a few) while still authoring content in plaintext. Quarto provides all this functionality (and more) without me having to dig around in pandoc’s documentation or write custom javascript.\nMy original website which I managed using the CMS system presented here, is open-sourced and can be viewed on Github.\n\n\nIn a prior post, I shared my humble system for running a static website using pandoc. Since that post, I have replaced several manual steps in the process with automated bash scripts.\n\nCreating and naming new posts\nI use the following human and machine readable naming convention for all my posts.\nYYYY-MM-DD--&lt;category&gt;--&lt;title&gt;\nWithin the post, I use yaml metadata to record additional information related to the post such as its title, date, author and a short abstract.\n\n\nmy-new-blog.md\n\n---\ntitle: foo bar baz\nauthor: John Doe\ndate: 2023-09-09\nabstract: |\n    This is the abstract for this post. This abstract shows up on the\n    index page automatically! Read on to learn how I do this.\n---\n\nAlthough the naming convention is clear, writing it is a bit cumbersome. Note that I also need to write the same information twice—once within the file in the yaml metadata, and again when naming the file. To reduce chances of human error, and make my life a bit easier, I automate the process of creating a new post using the following python script.\n\n\nbin/new\n\n#!/usr/bin/env python3\n\nimport os\nimport subprocess\nimport sys\nimport argparse\nfrom datetime import datetime\n\nEXT = \".md\"\nTIMESTAMP = datetime.now()\nTIMESTAMP = TIMESTAMP.__format__(\"%Y-%m-%d %a %H:%M\")\nTODAY = datetime.now()\nTODAY = TODAY.__format__(\"%Y-%m-%d\")\n\nparser = argparse.ArgumentParser()\n1parser.add_argument(\n    \"title\",\n    help=\"Title of new content\",\n)\n2parser.add_argument(\n    \"-t\",\n    \"--type\",\n    help=\"Type of content\",\n    choices=[\n        \"blog\",\n        \"talk\",\n    ],\n)\n3parser.add_argument(\n    \"-x\",\n    \"--noedit\",\n    help=\"Do not open new file in EDITOR\",\n    action=\"store_true\",\n)\n4parser.add_argument(\n    \"-f\",\n    \"--force\",\n    help=\"Do not ask for confirmation\",\n    action=\"store_true\",\n)\nargs = parser.parse_args()\n\nif args.type:\n    TYPE = args.type\nelse:\n    TYPE = \"blog\"\n\nTITLE = args.title.strip().lower().replace(\" \", \"-\")\nNAME = \"--\".join([TODAY, TYPE, TITLE])\nFILE = f\"_{TYPE}s/{NAME}{EXT}\"\n\nFRONTMATTER = [\n    \"---\",\n    \"\\n\",\n    f\"title: {TITLE}\",\n    \"\\n\",\n    f\"date: {TIMESTAMP}\",\n    \"\\n\",\n    f\"filename: {NAME}\",\n    \"\\n\",\n    \"author: Arumoy Shome\",\n    \"\\n\",\n    \"abstract: |\",\n    \"\\n\",\n    \"---\",\n]\n\nif not args.force:\n    confirm = input(f\"Create {FILE}? [y]es/[n]o: \")\n\n    if confirm.lower()[0] == \"n\":\n        sys.exit(\"Terminated by user\")\n\ntry:\n    with open(f\"{FILE}\", \"x\") as f:\n        f.writelines(FRONTMATTER)\nexcept FileExistsError:\n    sys.exit(f\"{FILE} already exists\")\n\nif not args.noedit:\n    subprocess.run([os.getenv(\"EDITOR\"), f\"{FILE}\"])\n\nsys.exit(f\"{FILE} created\")\n\n\n1\n\nAccept the title of the new post as the first positional argument. This argument is mandatory.\n\n2\n\nOptionally specify a type of post.\n\n3\n\nIf this flag is passed, don’t open the new file in $EDITOR.\n\n4\n\nIf this flag is passed, don’t ask for confirmation.\n\n\n\n\n\n\n\n\nTipPython argparse\n\n\n\nThe Python argparse module provides a convenient API to create commandline tools. This code is much more legible and understandable compared to how we parse arguments in say bash or zsh.\nFor instance, compare this to the argument parsing code I wrote in AIMS, my information management script.\n\n\nThe script has a title positional argument which is mandatory. Additionally, the script can also accept a type of the post using the --type or -t flag. With the --force or -f flag, the script does not ask for any confirmation when creating files. By default, the script will open the newly created post using the default editor. However, this can be bypassed by passing the --noedit or -x flag. The script automatically creates the yaml frontmatter for the post and names it in the specified format.\n\n\nAutomatically generating index pages\nI have two index pages on my website—the blogs page which list all the blogposts I have written and the talks page which lists all the talks I have given in the past. Previously, I was creating these pages manually. However, with a bit of unix shell scripting, I have now managed to do this automatically!\nI use the following script to generate the blogs and the talks index pages.\n\n\nbin/create-indices\n\n#!/usr/bin/env bash\n\n1# generate blogs.md\nTMP=$(mktemp)\n[[ -e blogs.md ]] && rm blogs.md\nfind _blogs -name '*.md' |\n  sort --reverse |\n  while read -r file; do\n    pandoc --template=_templates/index.md \"$file\" --to=markdown &gt;&gt;\"$TMP\"\n  done\n\ncat _templates/blogs-intro.md \"$TMP\" &gt;&gt;blogs.md\nrm \"$TMP\"\n\n2# generate talks.md\nTMP=$(mktemp)\n[[ -e talks.md ]] && rm talks.md\nfind _talks -name '*.md' |\n  sort --reverse |\n  while read -r file; do\n    pandoc --template=_templates/index.md \"$file\" --to=markdown &gt;&gt;\"$TMP\"\n  done\n\ncat _templates/talks-intro.md \"$TMP\" &gt;&gt;talks.md\nrm \"$TMP\"\n\n\n1\n\nSteps to generate blogs.md file. First clean slate by removing the file if it already exists. Find all markdown files in the _blogs directory, and run them through pandoc with a custom markdown template (explained in more details below). Append the entires in blogs.md in chronological order. Note as extra precaution, we use a temporary file to prevent accidental data loss.\n\n2\n\nSame as above, but create talks.md now.\n\n\nFirst we find all relevant markdown pages that we want to export to html using find. Next, we sort the results in chronological order such that the latest posts show up at the top of the page. The final part is the most interesting bit. We use pandoc’s templating system to extract the date, title and abstract of each file and generate an intermediate markdown file in the format that I want each post to show on the index page. Here is the template file that I use.\n\n\n_templates/index.md\n\n# ${date} ${title}\n$if(abstract)$\n\n${abstract}\n\n$endif$\n$if(filename)$\n[[html](${filename})]\n\n$endif$\n\nAll that is left to do is stitch everything together using cat to generate the final file.\n\n\nPutting everything together using make\nOnce the index pages are created, I use the following script to export all markdown files to html.\n\n\nbin/publish\n\n#!/usr/bin/env bash\n\nfind . -name \"*.md\" -not -path \"*_templates*\" |\n  while read -r file; do\n    pandoc --template=public -o docs/\"$(basename \"${file/%.md/.html}\")\" \"$file\"\n  done\n\nThe script finds all markdown files in the relevant directories, and converts them to html using pandoc. I use a custom template once again which includes some custom css and fonts of my choice.\nFinally, to automate the entire build process I use GNU make. I have a single all target which simply runs the create-indices and publish scripts in the right order.\n\n\nMakefile\n\nall:\n    bin/create-indices\n    bin/publish\n\n\n\nFurther optimisations\nThe create-indices script is currently sequential. You can imagine that this will keep getting slower as the number of posts increases. This step can be further optimised making the template extraction step parallel using xargs and then sorting the results.\nIn the publish script, we are converting all markdown files to html. Here, we can make the markdown file selection process smarter by using git ls-files. This will allow us to only select modified and untracked markdown files.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/visualisation-zoo/index.html",
    "href": "blogs/visualisation-zoo/index.html",
    "title": "Visualisation Zoo",
    "section": "",
    "text": "This is a collection of data visualisations I have created in the past from prior research publications. The title of this post is inspired by the Heer, Bostock, and Ogievetsky (2010) paper.\n\nHeer, Jeffrey, Michael Bostock, and Vadim Ogievetsky. 2010. “A Tour Through the Visualization Zoo.” Communications of the ACM 53 (6): 59–67. https://doi.org/10.1145/1743546.1743567.\n\nJoint Distribution of Categorical Variables\nThe following visualisation in Figure 1 comes from the very first Conference paper I wrote (Shome, Cruz, and Deursen 2022). The paper explored the presence of anti-patterns in popular ML datasets that lead to accumulation of technical debt in the downstream stages of the pipeline.\n\nShome, Arumoy, Luís Cruz, and Arie van Deursen. 2022. “Data Smells in Public Datasets.” In Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI. CAIN ’22. ACM. https://doi.org/10.1145/3522664.3528621.\nWe curated a catalogue of 14 such anti-patterns or “data-smells” and manually analysed their presence in 25 popular ML datasets from Kaggle.\nI created the visualisation using a JointGrid from the Seaborn library. The visualisation in the main subplot shows the distribution of the data-smells across all datasets that were analysed using a two-dimensional histogram. The visualisations in the marginal subplots shows a histogram of the corresponding categorical variables.\n\n\n\n\n\n\nFigure 1: Joint distribution of two categorical variables.\n\n\n\n\n\nHeatmap of Correlation Between Numerical Variables\n\n\n\n\n\n\n\nFigure 2: Heatmap of correlation between numerical variables\n\n\n\n\nThe next visualisation comes from our Shome, Cruz, and Deursen (2024) paper. Here we analysed the relationship between data dependent and model dependent fairness metrics. Figure 2 shows the results obtained from the empirical study conducted using 8 datasets and 4 ML models.\n\nShome, Arumoy, Luı̀s Cruz, and Arie van Deursen. 2024. “Data Vs. Model Machine Learning Fairness Testing: An Empirical Study.” In International Workshop on Deep Learning for Testing and Testing for Deep Learning. DeepTest ’24. IEEE/ACM. https://doi.org/10.1145/3643786.3648022.\nEach heatmap represents results obtained from a fairness metric (we used Disparate Impact and Statistical Parity Difference). The ML models are represented along the Y axis, while the datasets are along the X axis. Each block shows the correlation between the data and model variants of the corresponding fairness metric. The statistically significant cases are marked with an asterisk. The strength of the correlation is denoted using color–bright hues of red indicate positive correlation while cooler hues of blue represent negative correlation.\n\n\nOmniArt: Sentiments through Colours\n\n\n\n\nOverview of ACE: Art, Colour and Emotion browser\n\n\n\nThis is from a project where we used the OmniArt dataset to explore the relationship between colors and emotional tones in art through sentiment analysis. We used NLP techniques to analyze metadata from various artworks, focusing on how colors influence perceived sentiments based on painting descriptions. This work is also published as a conference paper with a video demonstration of the tool (Strezoski et al. 2019).\n\nStrezoski, Gjorgji, Arumoy Shome, Riccardo Bianchi, Shruti Rao, and Marcel Worring. 2019. “ACE: Art, Color and Emotion.” In Proceedings of the 27th ACM International Conference on Multimedia. MM ’19. ACM. https://doi.org/10.1145/3343031.3350588.\nWe created an interactive tool to allow users from varied backgrounds to intuitively explore the complex relationship between colour usage in art and the emotional sentiments those colours may evoke. In the image, you can see a snapshot of the tool which comprises of several interconnected components designed to facilitate interactive exploration of the data.\nThese visual components are designed to allow users from varied backgrounds to intuitively explore the complex relationship between color usage in art and the emotional sentiments those colors may evoke. This tool not only aids in art analysis but also makes the process accessible to a broader audience, enhancing understanding through interactive visual storytelling and data-driven insights.\n\n\nBuildings of Amsterdam\n\n\n\n\nModel of buildings in the city of Amsterdam\n\n\n\nThis one is from a large-scale data engineering project. Using 2 terabytes of point-cloud data, We created 3D models of all buildings in the Netherlands (well, we only managed to create a model for the city of Amsterdam).\nWe created an interactive web-based visualization that displays the 3D models of buildings across the Netherlands. Using a map of the Netherlands, we divided the data into tiles representing different areas. We made it into the “Hall of Fame” for the course, you can find an interactive demo hosted on the course website. Fair warning, the demo is pretty bad. So here is an image of the model for the beautiful city of Amsterdam.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/effortless-parallel-execution-xargs/index.html",
    "href": "blogs/effortless-parallel-execution-xargs/index.html",
    "title": "Effortless Parallel Execution with xargs & Friends",
    "section": "",
    "text": "Recently, I had to run Tensorflow Data Validation on over 500 public datasets from Kaggle to generate a baseline schema file for further analysis. I chose to do this using the xargs unix command.\nFollowing is a python script which generates the schema file and saves it to disk for a single csv dataset.\n\n\ncsv2schema.py\n\n#!/usr/bin/env python\n\nimport os\nimport sys\nimport tensorflow_data_validation as tfdv\nimport pandas as pd\n\n_CWD = os.path.dirname(__file__)\nDATADIR = os.path.abspath(os.path.join(_CWD, '..', 'data'))\nSTATSDIR = os.path.join(DATADIR, 'stats', 'train')\nSCHEMADIR = os.path.join(DATADIR, 'schema')\n\nname, _ = os.path.basename(sys.argv[1]).split('.')\n\nif os.path.isfile(os.path.join(SCHEMADIR, name+'.proto')):\n    print(name+'.proto', 'already exists, skipping...')\nelse:\n    frame = pd.read_csv(os.path.join(DATADIR, 'train', name+'.csv'))\n    stats = tfdv.generate_statistics_from_dataframe(frame)\n    schema = tfdv.infer_schema(stats)\n    tfdv.write_stats_text(stats, os.path.join(STATSDIR, name+'.proto'))\n    tfdv.write_schema_text(schema, os.path.join(SCHEMADIR, name+'.proto'))\n\nThe script accepts as argument a valid csv file (we assume that the file names are pruned and do not contain a period character within the name, but only to denote the extension). We read the file as a pandas dataframe, generate the statistics using tfdv.generate_statistics_from_dataframe function and infer a schema which is stored on disk for later analysis.\nFollowing is the bash shellscript wrapper which executes the python script presented above across several datasets using the find command. You may have to experiment with the -P flag which specifies the number of cores to distribute the execution across.\n\n\n\"csv2schema.bash\n\n#!/usr/bin/env bash\n\nmkdir -p data/{schema,stats/train}\n\nfind data/train -type f |\n    xargs -n 1 -P 4 ./bin/write-schema.py\n\nThat’s all there is to it! Write your main script with one file in mind, and distribute across several files using a combination of find and xargs.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html",
    "href": "blogs/privacy-preserving-deep-learning/index.html",
    "title": "Privacy Preserving Deep Learning",
    "section": "",
    "text": "Good morning all, thank you for being here. Today I wanted to share my research on Privacy Preserving Deep Learning (PPDL) which I conducted during my Msc. As an example, we will be looking specifically at PPDL for Medical Image Analysis (MIA). However, I think that this field of research is highly relevant these days and the techniques can be applied to any domain working with sensitive data (such as governance, finance and others)."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-mia",
    "href": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-mia",
    "title": "Privacy Preserving Deep Learning",
    "section": "Challenges of MIA",
    "text": "Challenges of MIA\nMIA concerns itself with the study of medical images in order to determine Regions of Interest(ROI) and image segmentation. MIA is usually performed by radiologists and requires intensive training and practise. For each patient, a radiologist may have to inspect more than 100 images. This is a repetitive task and may pose a cognitive strain, making the process prone to human errors.\nMIA is limited by the skills and cognitive fitness of the human performing the analysis. While humans may not be so good in performing repetitive tasks, machine will do them tirelessly and consistently.\nThis is where Deep Learning (DL) comes into the picture. DL gained popularity and momentum both in the industry and academia due to it’s success with image classification through the ImageNet challenge. It was soon adopted not only within MIA but also other fields of medical science. Although this is an active field of research, several challenges remain. One of the challenges is to train DL models without violating the privacy of the patients."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-dl-in-mia",
    "href": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-dl-in-mia",
    "title": "Privacy Preserving Deep Learning",
    "section": "Challenges of DL in MIA",
    "text": "Challenges of DL in MIA\nDL models require a lot of data for training, generally the more data used, better the prediction results. However, this quantity of data may not always be available at a single medical institute (for instance at small hospitals located in remote locations). Larger institutes who have sufficient data are able to train models however these models are subjected to bias. This is because the data does not accurately represent the entire population. Moreover, if the institute is specialized in a specific disease or observes a disease more frequently due to it’s unique geographic location, the data is biased.\nThus DL models need to be trained using data from several institutes. The traditional server-client architecture posses several privacy and security concerns. Once the institutes upload their data to a server, they may lose their data governance and ownership rights as the server may be located in a different geographical location with a different set of jurisdictions. The data upload and storage procedure may also not be transparent thus raising concerns regarding it’s safety and security.\nAlthough efforts have been made to create centralized medical data repositories (eICU Collaborative Research Database and The Cancer Genome Atlas), data privacy laws such as GDPR in Europe and HIPAA in the State pose a large overhead. The data in such repositories are anonymised, but this does not guarantee security against privacy leaks. Datasets have a unique statistical fingerprint making them vulnerable to linkage attacks.\nDistributed Learning addresses the data ownership and governance problems of centralised learning. However, it still needs to utilise data privacy and security techniques to ensure privacy of patient records. In the following sections of this talk we will take a closer look at Distributed Learning and data privacy techniques for PPDL."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#data-privacy-techniques",
    "href": "blogs/privacy-preserving-deep-learning/index.html#data-privacy-techniques",
    "title": "Privacy Preserving Deep Learning",
    "section": "Data Privacy Techniques",
    "text": "Data Privacy Techniques\nTo date, anonymisation and de-identification remain the predominant privacy-preserving technique for sharing sensitive information. Although there are not standardised methods of doing so, we commonly see 3 techniques in the literature:\n\nDe-identification or removal of sensitive information from datasets. This is not the best solution since it results in loss of valuable information which may be useful for analysis/training.\nPseudo-anonymisation which replaces sensitive information with unique pseudonyms. This is a bit better since we retain the unique linkages and correlations between data points which may be of interest to us.\nAnonymisation which simply put means de-identification with a cherry on top. A few additional things are done on top of de-identification to significantly reduce the probability of re-identification.\n\nDifferential Privacy (DP) preserves the privacy of patients by injecting a certain quantity of noise into the data. This allows for statistical analysis to be conducted without compromising sensitive information. A trade-off between privacy and performance occurs here. More perturbations in the dataset give higher privacy however may negatively affect the model’s performance.\nThe alternative is to encrypt the data or the model parameters. The state-of-the-art encryption schemes cannot be cracked using brute force techniques making them the most secure means of sharing sensitive information. The Homomorphic Encryption (HE) scheme allows certain operations (such as addition, subtraction and multiplication) to be carried out directly over the cyphertext. This is beneficial for neural networks as models can be trained directly using encrypted, unperturbed data with additional computational overhead."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#security-threats-attacks",
    "href": "blogs/privacy-preserving-deep-learning/index.html#security-threats-attacks",
    "title": "Privacy Preserving Deep Learning",
    "section": "Security Threats & Attacks",
    "text": "Security Threats & Attacks\nAnonymisation techniques are built into MIA software which may explains their popularity. However, anonymised data contain unique statistical fingerprints which make them vulnerable to linkage attacks. A famous example is the Netflix Prize Dataset where researchers were able to combine Netflix’s anonymised user subscription data with the imdb public dataset to identify specific people along with their political preferences and other sensitive information. Another well known example is linkage attack on anonymised hospital records from the State of California along with public voter records to identify the complete medical history of the Governor of California.\nIn traditional software, computers strictly follow a specific set of programmed instructions. In contrast, ML algorithms derive their own set of rules based on a substantial amount of data provided to them. This behaviour often leads to neural networks being interpreted as a black box, preventing users from understanding it’s inner workings. This black box behavior makes neural networks a potential target for exploitation. This identification of such threats and vulnerabilities must be prioritised.\nThese threats can be classified as ones targeting the data and others targeting the model. Adversarial examples are inputs that are often indistinguishable from typical inputs, yet contain intentional feature changes that lead to incorrect classification. Adversarial attacks are of significance since they question the robustness of DL models. They can have devastating consequences for DL applications in high-stakes applications such as medical imaging, facial recognition and autonomous vehicles.\nData poisoning refers to changing the training data such that the model can learn with malicious intent and manifest that as it’s predictions. Open access and public datasets are especially vulnerable to data poisoning. Such datasets are often used to validate proof-of-concept models by companies or worse, to validate bleeding edge innovations in research.\nIn addition to attacks which target the data, the DL model itself can be exploited. By observing the gradients and parameters of a trained network, parts of the dataset can be obtained. Model inversion, membership inference and reconstruction attacks often utilise this technique to obtain the training data or infer if a public dataset was used for training. Combined with linkage attacks, the presence of an individual in a dataset and their sensitive information can be obtained."
  },
  {
    "objectID": "blogs/yob/index.html",
    "href": "blogs/yob/index.html",
    "title": "Toggling background color in kitty and vim (yob)",
    "section": "",
    "text": "yob is a shell script that I wrote to toggle between a light and dark colorscheme in Kitty, my terminal of choice. Some additional configuration also allow me to sync the vim colorscheme to that of kitty. This is a much simpler version of this script by Greg Hurrell.\nThe name is inspired by the vim-unimpaired package for vim. The package adds keybindings prefixed with the yo* that toggle various vim settings. The b in yob stands for “background”.\nHere is the script in its entirety as of 2024-01-19. You can also find the latest version in my dotfiles repository.\n\n\nyob\n\n#!/usr/bin/env bash\n\n1LIGHT_THEME=\"colors/gruvbox-light.conf\"\nDARK_THEME=\"colors/gruvbox-dark.conf\"\nLOCATION=\"$HOME/.local/share/yob\"\nVIM_BG_FILE=\"$LOCATION/background\"\n\n__init() {\n  [[ ! -d \"$LOCATION\" ]] && mkdir -p \"$LOCATION\"\n  [[ ! -e \"$VIM_BG_FILE\" ]] && touch \"$VIM_BG_FILE\"\n}\n\n2__update() {\n  (\n  cd \"$HOME/.config/kitty\"\n  if [[ \"$1\" == \"light\" ]]; then\n    ln -sf \"$LIGHT_THEME\" current-theme.conf\n    echo \"light\" &gt;\"$VIM_BG_FILE\"\n  else\n    ln -sf \"$DARK_THEME\" current-theme.conf\n    echo \"dark\" &gt;\"$VIM_BG_FILE\"\n  fi\n  )\n\n  kitten @ set-colors --all --configured \"$HOME/.config/kitty/current-theme.conf\"\n}\n\n3__toggle() {\n  local CURRENT_BG\n  CURRENT_BG=\"$(head -n 1 \"$VIM_BG_FILE\")\"\n  if [[ \"$CURRENT_BG\" =~ \"light\" ]]; then\n    __update \"dark\"\n  else\n    __update \"light\"\n  fi\n}\n\nmain() {\n  if [[ ! \"$TERM\" =~ \"kitty\" ]]; then\n    echo \"yob: not running kitty, doing nothing.\"\n    exit 1\n  fi\n\n  if [[ ! -e \"$VIM_BG_FILE\" ]]; then\n    __init\n    __update \"dark\"\n    exit 0\n  fi\n\n  __toggle\n}\n\nmain \"$@\"\n\n\n1\n\nSome initial setup. Here we define the location of the light and dark colorscheme files for kitty (I choose to save them in the kitty config directory and check it into git so that they are available wherever I clone my dotfiles repo). We also store additional data for yob in ~/.local/share/yob/background as per XDG best practices for unix systems.\n\n2\n\nHere we create a symbolic link between the colorscheme file and current-theme.conf. The current-theme.conf file is picked up by kitty next time it starts. Finally, we set the theme for the existing kitty sessions using kitty @ set-colors.\n\n3\n\nThis function toggles between the light and dark themes based on information in ~/.local/share/yob/background.\n\n\n\nChoosing a colorscheme with light and dark variant\nAfter many experiments with various colorschemes in different lighting conditions, I picked Gruvbox as my theme of choice. There are several reasons for this decision:\n\nThe colorscheme has been around for a little over 10 years and is very stable (very few changes since 2018 as per the commits chart on github).\nBoth light and dark variants of the theme are legible in various lighting conditions.\nDue to its popularity, the theme is available in Kitty and Vim 9 out-of-the-box.\n\n\n\nPreserving kitty colors across sessions\nTo preserve the colorscheme across kitty sessions, yob symlinks the colorsheme files to current-theme.conf in kitty’s config directory. The following line ensures that kitty sources the right colorscheme next time kitty starts.\n\n\n~/.config/kitty/kitty.conf\n\ninclude current-theme.conf\n\n\n\nSyncing vim colors with kitty\nThe aru#set_background() function reads the first line of ~/.local/share/yob/background using the built-in readfile function in vim (see :help readfile) and updates the background.\n\n\n.vim/autoload/aru.vim\n\nfunction! aru#set_background() abort\n  let config_file = expand('~/.local/share/yob/background')\n  if filereadable(config_file)\n    let bg = readfile(config_file, '', 1)[0]\n  endif\n\n  execute 'set background=' .. bg\nendfunction\n\nIf we run yob from another shell, the colors in existing vim sessions does not update. To account for this I introduce an autocommand that is fired every time vim is started and when it gets focus.\n\n\n.vimrc\n\nset termguicolors\ncolor retrobox\n\nfunction! AruAutoBackground() abort\n  augroup AruAutoBackground\n    autocmd!\n    autocmd FocusGained,VimEnter * call aru#set_background()\n  augroup END\nendfunction\ncall AruAutoBackground()\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html",
    "href": "blogs/research-workflow-plaintext/index.html",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "In this talk I will go over how we can use Emacs and org-mode to craft a research workflow. We will look at how we can leverage the power of Emacs and org-mode to capture, store, search and retrieve research data, all in plain text! The talk will touch upon how org-mode can be used as an environment for literate programming and reproducible research. I do not assume any prior knowledge of emacs or org-mode and I want this to be more of a discussion rather than a talk. Please ask me questions as I go along and share your thoughts, tips and techniques with others!\n\n\nOutline of this talk is as follows:\n\nI will start with a brief history of Emacs and org-mode.\nA brief background on plaintext.\nWe will follow with some motivation for why we would want to use plaintext to manage information and some of my supporting philosophies.\nNext, we will jump into the building blocks of org-mode.\nWhich we will leverage to craft research workflows and introduce some automation into menial tasks.\nI will highlight some additional features of org-mode that we can use for literate programming and reproducible research.\nI will end this talk by pointing out additional resources that you may use to get started with Emacs.\n\n\n\n\nWhen I was preparing material for this talk, I thought: what better way to talk about emacs and org-mode than to do it in emacs itself. There is a lot of text compared to a traditional presentation, and there is justification for doing so:\n\nThis talk is highly interactive and there are demos throughout. It would have been a bit annoying having to constantly switch between emacs and my slides.\nThere is another reason however I will reveal this towards the end of the talk\n\n\n\n\nThe original talk was giving using an org document, from within Emacs. This version that you are reading however, is the markdown version, exported from the original org document. Since the original talk was designed to be interactive and dynamic, certain parts of this post may not make sense in this static HTML format."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#outline",
    "href": "blogs/research-workflow-plaintext/index.html#outline",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "Outline of this talk is as follows:\n\nI will start with a brief history of Emacs and org-mode.\nA brief background on plaintext.\nWe will follow with some motivation for why we would want to use plaintext to manage information and some of my supporting philosophies.\nNext, we will jump into the building blocks of org-mode.\nWhich we will leverage to craft research workflows and introduce some automation into menial tasks.\nI will highlight some additional features of org-mode that we can use for literate programming and reproducible research.\nI will end this talk by pointing out additional resources that you may use to get started with Emacs."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#a-note-for-the-audience",
    "href": "blogs/research-workflow-plaintext/index.html#a-note-for-the-audience",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "When I was preparing material for this talk, I thought: what better way to talk about emacs and org-mode than to do it in emacs itself. There is a lot of text compared to a traditional presentation, and there is justification for doing so:\n\nThis talk is highly interactive and there are demos throughout. It would have been a bit annoying having to constantly switch between emacs and my slides.\nThere is another reason however I will reveal this towards the end of the talk"
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#a-note-for-the-readers",
    "href": "blogs/research-workflow-plaintext/index.html#a-note-for-the-readers",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "The original talk was giving using an org document, from within Emacs. This version that you are reading however, is the markdown version, exported from the original org document. Since the original talk was designed to be interactive and dynamic, certain parts of this post may not make sense in this static HTML format."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#emacs",
    "href": "blogs/research-workflow-plaintext/index.html#emacs",
    "title": "Research Workflow in Plaintext",
    "section": "Emacs",
    "text": "Emacs\nEmacs is the other text editor that you may have heard of aside from Vim. There are several flavours of Emacs, the most popular being GNU Emacs which was created by Richard Stallman. The core, performance critical components of Emacs is written in C, however majority of it’s codebase is written in emacs-lisp (elisp) which is a variant of Lisp.\nThe first public release of Emacs was in 1970 (it has 20 years on Vim which was released in 1991), it is fully open-source, has had 27 stable releases, has a vibrant and large community and continues to be in active development to this date."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#org-mode",
    "href": "blogs/research-workflow-plaintext/index.html#org-mode",
    "title": "Research Workflow in Plaintext",
    "section": "org-mode",
    "text": "org-mode\norg-mode is an emacs package. created by Carsten Dominik (professor of Astronomy at UvA, faculty of natural sciences) in 2003. org-mode provides two core functionalities:\n\nIt provides a major-mode (Emacs lingo for syntax highlighting based on filetype) for plaintext files.\nAnd a slew of tools to organize and edit information in the form of lists (I like to think of them as trees)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#plaintext",
    "href": "blogs/research-workflow-plaintext/index.html#plaintext",
    "title": "Research Workflow in Plaintext",
    "section": "Plaintext",
    "text": "Plaintext\nWe have been talking about plaintext and I think it’s important to clarify what I mean. Plaintext in this context means text files which are unencrypted and written in a format (.{txt,md,org}) that is readable through a terminal emulator."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#why-use-plaintext",
    "href": "blogs/research-workflow-plaintext/index.html#why-use-plaintext",
    "title": "Research Workflow in Plaintext",
    "section": "Why use Plaintext?",
    "text": "Why use Plaintext?\nWhy would we want to use plaintext to store information? There are several benefits, and the word “Freedom” encapsulates everything well. By freedom, I mean:\n\nFreedom to do what we see fit with our data. Its stored as plaintext files on our system. We can choose to keep it that way, upload to our trusted cloud provider or even introduce encryption for additional security and privacy.\nFreedom from yet another proprietary software. There are solutions that market themselves as systems for note-taking, knowledge management or a “second brain” such as Notion and Evernote. But the problem is that they are in control of your data (stored on their servers) and they often store this information in a proprietary format resulting in a vendor lock-in.\nFreedom to choose (or craft) our own tools which are optimised for us and can scale with our ever-growing body of knowledge (remember, learning is a lifelong journey)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#plaintext-productivity",
    "href": "blogs/research-workflow-plaintext/index.html#plaintext-productivity",
    "title": "Research Workflow in Plaintext",
    "section": "Plaintext & Productivity",
    "text": "Plaintext & Productivity\nLooking at the current landscape of productivity tools, I do not see anything that aids in exercising the one true productivity system that actually matters: our brain. Ultimately, it all comes down to effort and time taken to read papers, understanding concepts, learning, and forming connections which give rise to ideas. By takes away the “fluff”, plaintext allows us to focus on the essential. The text. The ideas. The connections.\nThere is no “one size fits all” or “cookie cutter” solution when it comes to productivity. The problem with existing productivity software is that they enforce their own structure and constraints which does more harm than good. I have given this a fair amount of thought and experimentation. In my experience, a two step process of 1. Capturing information quickly and 2. The act of reviewing captured information and deliberate summarisation works best. To this end, we will see how org-mode can help us with this."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#importance-of-structure-standardisation",
    "href": "blogs/research-workflow-plaintext/index.html#importance-of-structure-standardisation",
    "title": "Research Workflow in Plaintext",
    "section": "Importance of Structure & Standardisation",
    "text": "Importance of Structure & Standardisation\nI made a profound observation while reflection on my current research workflow. I spent a lot of my time during my masters on finding tools and systems for managing knowledge. Extrinsic search for existing solutions/methods was not fruitful. Instead, I used whatever solution came naturally to me. Through several iterations and minor changes, I developed a set of rules to store the data in a consistent structure. From this structure, developing tools to automate the process developed organically. I think this is such a simple thing, which makes it so powerful. We see similar phenomenon in software as well. We have decades of research which boils down to doing things in a consistent and standardised manner."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#non-linear-nature-of-research",
    "href": "blogs/research-workflow-plaintext/index.html#non-linear-nature-of-research",
    "title": "Research Workflow in Plaintext",
    "section": "Non-linear Nature of Research",
    "text": "Non-linear Nature of Research\nThe main take-away for me was realising that research is dynamic, non-linear and personal. Finding the right tools is a journey which we must make ourselves. Using plaintext grants us the freedom to explore and experiment different options and techniques. It allows us to craft our own set of tools, standards and techniques which are curated towards how we think and operate."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#document-structure",
    "href": "blogs/research-workflow-plaintext/index.html#document-structure",
    "title": "Research Workflow in Plaintext",
    "section": "Document Structure",
    "text": "Document Structure\nWe can create a new header (or “tree” in org lingo) by prepending the header title with a *. Trees can be nested, a tree may have several other sub-trees within itself. Sub-trees are created by adding more *, the number indicates the depth. We can promote or demote headers quickly using the M-left and M-right keybindings.\nWe can hide or show specific sections of a document by pressing TAB while positioning the cursor (or “point” in emacs lingo) on the header. We can also cycle the visibility of the entire document using S-TAB.\nOrg provides a few handy keybindings to move between headers. C-c n (mnemonic: “next”) takes us to the next header while C-c p (mnemonic: “previous”) takes us to the previous header. C-c u (mnemonic: “up”) can be used to navigate to the parent of the current sub-tree. C-c f (mnemonic: “forward”) and C-c b (mnemonic: “back”) navigates between headers of the same level.\nWe can edit the structure of a org document quickly by using M-down and M-up to move headers."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#todo-states",
    "href": "blogs/research-workflow-plaintext/index.html#todo-states",
    "title": "Research Workflow in Plaintext",
    "section": "Todo States",
    "text": "Todo States\nOrg was originally built to manage tasks in plaintext. A header can be converted to a task by prepending it with a “TODO” keyword. Org also understands the notion of various stages that a task may go through during it’s lifecycle. We can change the todo state interactively using C-c C-t.\nThe todo states are customizable and we will talk more about the ones I have created and use regularly to organize scientific research."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#capturing-refiling",
    "href": "blogs/research-workflow-plaintext/index.html#capturing-refiling",
    "title": "Research Workflow in Plaintext",
    "section": "Capturing & Refiling",
    "text": "Capturing & Refiling\nNOTE this section requires some setup and plumbing before it can be replicated. Refer to the org manual on capture to get started.\nEarlier I mentioned the notion of capturing information and reviewing them at a later stage as a system for productivity (see the section on Motivation & Philosophy). Let’s dive deeper into the capturing segment and explore how org can help introduce some automation into this process.\nWe can invoke org-capture using C-c c which prompts us for a capture template. This is customizable and the most commonly used one is to capture tasks. Once we are satisfied with our content, we can use C-c C-c (this is also the universal keystroke you would use to tell emacs that you are “done” with something or to “confirm” something) to confirm, save and close the capture buffer. Later, I will also talk about the custom capture template that I have written for my research workflow.\nBy default, the captured text is saved in a predefined file, however we can also choose to save it somewhere else using org-refile (C-c C-w in the org-capture buffer)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#planning-structuring-papers",
    "href": "blogs/research-workflow-plaintext/index.html#planning-structuring-papers",
    "title": "Research Workflow in Plaintext",
    "section": "Planning & Structuring Papers",
    "text": "Planning & Structuring Papers\nI find org to be particularly useful for planning and structuring scientific papers. I also use it to capture my thoughts, ideas and notes for various projects that I am working on. Once I have accumulated a large body of text, I find the the “narrow” functionality provided by emacs to help me focus on a particular section or region of text."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#bibliography-management",
    "href": "blogs/research-workflow-plaintext/index.html#bibliography-management",
    "title": "Research Workflow in Plaintext",
    "section": "Bibliography Management",
    "text": "Bibliography Management\nBesides keeping track of tasks, I use org to manage bibliographic information of scientific publications. Let’s look at an example in a project that I worked on earlier.\nOrg allows us to assign metadata to org headers known as org-properties. For instance, I like to store the first author, last author, source of publication, year of publication and a link to the pdf file on my disk as properties. Using the org-sparse-tree command (bound to C-c / in org-mode buffers), we can quickly find what we are looking for."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#aocp.el",
    "href": "blogs/research-workflow-plaintext/index.html#aocp.el",
    "title": "Research Workflow in Plaintext",
    "section": "aocp.el",
    "text": "aocp.el\n\n\n\n\n\n\nTipaocp.el\n\n\n\nMore details regarding this package can be found in my prior post.\n\n\nAs you may expect, this task is repetitive and entering these properties manually becomes cumbersome. To automate this process, I wrote an emacs package: aocp.el. The package provides a few helper functions which are meant to be used in an org-capture template (refer to the project readme for more information)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#additional-resources",
    "href": "blogs/research-workflow-plaintext/index.html#additional-resources",
    "title": "Research Workflow in Plaintext",
    "section": "Additional Resources",
    "text": "Additional Resources\nA few resources and pointers to help you get started:\n\nStart with the tutorial: C-h t.\nRead the manual: C-h r or alternatively C-h i to open the documentation viewer (info) and select “Emacs”.\nUse the excellent and extensive emacs help system, some of the most frequent ones that I use are: C-h f, C-h v, C-h o and C-h m.\nConsult the emacs wiki. I usually use it as reference once I have read about a particular topic in the manual.\nWatch Harry Schwartz’s excellent talk on getting started with org-mode.\nWatch Howard Abrams’ excellent talk on emacs introduction and demonstration.\nConsult my emacs config files if you prefer code."
  },
  {
    "objectID": "blogs/bibliography-management-orgmode/index.html",
    "href": "blogs/bibliography-management-orgmode/index.html",
    "title": "Managing Scientific Bibliography using Emacs Org-mode",
    "section": "",
    "text": "Front Matter and setup\nI store all my bibliographic information in a single bib.org file. I show a hypothetical version of this file below, with two entries. I store each paper, as a level 1 header. I use the bibtex key as the title for the header.\n\n\nbib.org\n\n#+title: Bibliography\n#+tags: test viz data self notebook survey\n#+tags: [ test : fair ]\n\n* TODO [#A] amershi2015modeltracker :test:viz:\n\n#+begin_src bibtex\n@InProceedings{   amershi2015modeltracker,\n  series        = {CHI ’15},\n  title         = {ModelTracker: Redesigning Performance Analysis Tools for\n                  Machine Learning},\n  url           = {http://dx.doi.org/10.1145/2702123.2702509},\n  doi           = {10.1145/2702123.2702509},\n  booktitle     = {Proceedings of the 33rd Annual ACM Conference on Human\n                  Factors in Computing Systems},\n  publisher     = {ACM},\n  author        = {Amershi, Saleema and Chickering, Max and Drucker, Steven\n                  M. and Lee, Bongshin and Simard, Patrice and Suh, Jina},\n  year          = {2015},\n  month         = apr,\n  collection    = {CHI ’15}\n}\n#+end_src\n\n* DONE [#A] chen2022fairness       :test:fair:survey:\n:LOGBOOK:\n- State \"DONE\"       from \"TODO\"       [2022-09-19 Mon 14:14]\n:END:\n\nSome notes that I may have regarding this paper.\n\n#+begin_src bibtex\n@Misc{            chen2023fairness,\n  title         = {Fairness Testing: A Comprehensive Survey and Analysis of\n                  Trends},\n  author        = {Zhenpeng Chen and Jie M. Zhang and Max Hort and Federica\n                  Sarro and Mark Harman},\n  year          = {2023},\n  eprint        = {2207.10223},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.SE}\n}\n#+end_src\n\n\n\nRetrieving bibtex information from Crossref\nMy search for papers always begins on Google Scholar. For papers that I find interesting, I retreive the bibtex information using the doi2bib script. The script accepts the DOI as an argument, and prints the bibtex information obtained from Crossref.\nThe script also accepts a --preprint flag, in which case, it accepts an Arxiv ID and obtains the bibtex information from Arxiv directly.\n\n\n\n\n\n\nTipScientific Paper Discovery\n\n\n\nYou can find more information on how I discovery scientific papers in this blogpost.\n\n\n\n\n\n\n\n\nTipdoi2bib\n\n\n\nYou can find more details regarding the doi2bib script in this blogpost.\n\n\n\n\nCapturing bibtex information using org-capture\nEmacs org-mode has a nifty capture feature that allows the user to quickly capture information. I have the following capture template to save bibtex information into the bib.org file above.\n\n\nbib.txt\n\n* %?\n\n#+begin_src bibtex\n#+end_src\n\nI have the following org-capture configuration in my init.el file.\n\n\ninit.el\n\n(org-capture-templates\n`((\"p\" \"Paper\" entry (file aru/org-bib-file)\n   \"%[~/.emacs.d/org-templates/bib.txt]\" :prepend t)))\n\nIn Emacs, I hit the keystrokes C-c c followed by the p key to initiate the capture sequence. Org-mode automatically inserts the capture template shown above. It creates a new level 1 header and inserts an empty bibtex source block.\nTo populate the source block, I hit C-c ' (see org-special-edit for more information on this keybinding). With C-u M-|, I run the doi2bib command along with the DOI to add the output of the command into the source block. C-c ' closes the special edit buffer and returns back to bib.org.\n\n\n\n\n\n\nNoteEvil Mode\n\n\n\nI now use evil-mode which provides vim keybindings within Emacs. I populate the source block using the :read! command.\n\n\n\n\nMapping of orgmode features and my usage\nIn the following table I summarise how I use the built-in orgmode features for organising the bibliographic information.\n\n\n\n\n\n\n\nFeature\nPurpose\n\n\n\n\nTODO keywords\nI mark papers that I want to read with the TODO state. Papers that I have already read are marked with the DONE state.\n\n\nPriority\nPapers that I find interesting, and cite frequently are marked with the A priority.\n\n\nTags\nI use tags to broadly classify the papers based on topics relevant for my Phd. You can see the tags I use in the example bib.org file provided above.\n\n\n\n\n\nSearching and Retrieving\nI use org-agenda to search and retrieve papers of interest. For instance, I can filter papers that I need to read by asking org-agenda for papers that are marked with the TODO state (see org-todo-list). I can produce a list of all papers that have the testing and data tag (see org-tags-view). More complex search queries can be constructed using the org advanced search commands: for instance, give me all papers on testing that were written by author X in the year of 2001 (see org advanced search syntax).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/rename/index.html",
    "href": "blogs/rename/index.html",
    "title": "Renaming files so they make sense (rename)",
    "section": "",
    "text": "Often I need to work with files that have really long and obscure names. rename is a Python script I wrote that renames a given file using a sane format. By default, it makes the following changes:\n\nRemove all whitespace and punctuation marks\nRemove English stopwords\nLowercase all characters and\nHyphenate everything\n\nHere is the full script as of 2024-11-29, you can find the latest version of the script in my dotfiles repository.\n\n\nrename\n\n#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport re\n\n\nwith open(\n    os.path.join(os.getenv(\"HOME\"), \".local/share/rename\", \"stopwords-en.txt\")\n) as f:\n    STOPWORDS = f.readlines()\n    STOPWORDS = [word.strip(\"\\n\") for word in STOPWORDS]\n\n\ndef rename(src: str) -&gt; str:\n    dst = src\n    dst = dst.lower()\n    dst = re.sub(r\"\\s+\", \"-\", dst)  # replace 1 or more whitespace with hyphen\n    dst = re.sub(\n        r\"[^a-zA-Z0-9]+\", \"-\", dst\n    )  # replace 1 or more punctuations with hyphen\n    dst = re.sub(\"([A-Z][a-z]+)\", r\"-\\1\", re.sub(\"([A-Z]+)\", r\"-\\1\", dst))\n    dst = dst.split(\"-\")\n    dst = [word for word in dst if word]  # remove empty words\n    dst = [word for word in dst if word not in STOPWORDS]\n    return \"-\".join(dst)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Rename files the way I like it.\")\n    parser.add_argument(\"source\", type=str, help=\"File or directory to rename\")\n    parser.add_argument(\n        \"-f\",\n        \"--force\",\n        action=\"store_true\",\n        default=False,\n        help=\"Don't ask for confirmation\",\n    )\n\n    args = parser.parse_args()\n\n    head, tail = os.path.split(args.source)\n    src, ext = os.path.splitext(tail)\n\n    dst = rename(src)\n    dst = os.path.join(head, dst)\n    dst = f\"{dst}{ext}\"\n\n    if not dst:\n        print(\"rename: no more words left!\")\n        exit(1)\n\n    if not args.force:\n        response = input(f\"rename: {args.source} --&gt; {dst}? [y, n]: \")\n        match response:\n            case \"n\":\n                print(\"rename: aborted by user.\")\n                exit(0)\n            case _:\n                print(\"rename: incorrect response, please type one of [y, n]\")\n                exit(1)\n\n    print(f\"rename: {args.source} --&gt; {dst}\")\n    os.rename(src=args.source, dst=dst)\n\nThe stopwords are read from a text file so adding new stopwords (or words you never want to see in your filenames) easy. I store this file in my dotfiles repo to ensure that the rename script works on any machine where I clone my system configuration files.\nBy default, the script will ask for confirmation before renaming a file. The --force flag can be passed to bypass this. This makes it convenient to rename files in bulk using standard unix tools such as find and xargs. For instance, below how I show how you can rename all Docx files in a directory automatically.\nfind * -name '*.docx' -print0 |\nxargs -0 -n1 rename --force\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arumoy Shome",
    "section": "",
    "text": "I am a researcher at TU Delft, working towards my doctorate as part of the Software Engineering Research Group. My research interests lie at the intersection of Data Science, Software Engineering and Design. Here I am developing tools, techniques and best practises which allow us to build, test, deploy, scale and maintain modern software systems with Machine Learning (ML) components. While we must look to the micro to identify and understand a problem, I try to keep an eye towards the current era of big-data and large-scale ML as a long-term area of research. You can find a list of my publications here.\nI completed my Msc. in Computer Science from the Vrije Universiteit Amsterdam and Universiteit van Amsterdam in the Netherlands, specializing in Big Data Engineering. As part of my thesis, I conducted interdisciplinary research with Nikhef and The Netherlands eScience Center to improve the data processing pipeline of the KM3NeT Neutrino Telescope using Graph Convolutional Neural Networks.\nI obtained my Bachelor’s in The Applied Sciences from the University of Waterloo in Canada, specializing in System Design Engineering with an option in Entrepreneurship. Here, I learned to bring creative solutions to complex problems with multiple facets such as society, economics, environment and politics. I also learned how to nurture an idea at it’s inception, develop it using systems theory and successfully bring it to the market as a finished product.\nMy professional experience is two pronged. I have over 5 years of software development experience through professional web development positions at and small companies. I am also versed with driven, research oriented projects rooted in diverse topics such as Large Scale Data Engineering, Data Visualization and Artificial Intelligence.\nI enjoy sharing knowledge and having discussions and am always happy to engage with other publicly. Sometimes, I write about my day-to-day challenges as a software engineering, data scientist & researcher.\nAll materials (excluding links to external websites or third party products) on this website are open sourced under the Creative Commons BY 4.0 license.\n\n\n Back to top"
  }
]