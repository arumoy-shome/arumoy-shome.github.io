[
  {
    "objectID": "blogs/research-workflow/index.html",
    "href": "blogs/research-workflow/index.html",
    "title": "Research Workflow",
    "section": "",
    "text": "Here I outline a workflow which I have developed to handle the non-linear nature of scientific research. There are days when one reads several papers without making much sense of them. However, a single paper the next day can result in a clear picture of the domain. The workflow acknowledges the fact that there are many more failures rather than successes in research. However, a failure is still an outcome—and this is what helps us decide what to do next—so it should still be recorded.\nResearch workflows are highly personal. If you decide to adopt any or all ideas I present here, it may require some tweaking so that it works for you. Years of performing an extrinsic search for existing solutions was unfruitful. Instead, I simply started reading papers and let the process develop organically. Soon I noticed some common patterns and standardisation which allowed me to automate some aspects of the process.\nThere are 3 interdependent phases to this process described below.\n\nFinding papers\nI save the bibtex entry in a bib file (generally in the project root along with the other latex files). This is because the bibtex-mode in Emacs provides powerful search options, templates for adding entries, sorting entries, cleaning up, and much more. Besides, the bib files ultimately gets used in the report so it makes sense to group the two together and keep them in the same location.\n\n\n\n\n\n\nUPDATE 2021-06-15 Tue\n\n\n\nI no longer save bibtex entry in a bib file. Instead, I store them with my notes in the same org file in source code blocks. I use org-tangle to generate the bib file when needed.\n\n\nFirst, I use good ol’ Google Scholar (along with a plugin to enable my institution’s proxy) to find papers. Once, I find a paper I open it and read the abstract. If the abstract appeals to my current research focus or is interesting to me in general, I download it and save it in a folder which syncs across my devices. Next, I grab the bibtex entry and create a task using org-capture where I save the bibtex entry, one liner on why I found the paper interesting. I add the location to the pdf on disk, I set the title of the task as the bibtex entry and I save it to my inbox (or relevant project org file).\n\n\n\n\n\n\nUPDATE 2021-06-15 Tue\n\n\n\nI have written an Elisp package that automates all of this. With the bibtex entry in my clipboard, I now issue an org-capture template which creates a new task with the bibkey as the header, adds relevant properties such as first author last author and source of publication, and sticks the bibtex entry in a source block (see aocp.el).\n\n\nIn scientific paper discovery I describe my process of finding relevant papers in more detail.\n\n\nReading papers\nI read the papers on my laptop. My screen is usually split 50-50 between Emacs (where I take notes) and a pdf viewer. I do not make any highlights because its easy to abuse and it ties the information to the pdf viewer application that I am using.\nInstead, I take notes in plain text (as of 2020, I have been using org-mode, an Emacs major mode to do so). There are several, rationales for doing this digitally. First, I require these notes to be searchable and link-able to other notes. Second, research projects often span for a long duration. Having digital notes ensures that I do not have notes distributed amongst several analog papers/journals. Lastly, I take notes on papers in a specific manner. These notes have a specific purpose - and thus a carefully constructed structure as described in the next section - and thus come under the category of hard information (I talk more about productivity and how I categorise information in productivity).\nI use the 3 pass technique as presented by S. Keshav in his paper titled “How to read a paper” (2007). I employ the first pass as a screening method to identify if a given paper is worth reading in full. During the first pass, I read the introduction and conclusion and briefly glance over the sections of the paper and the diagrams. The goal here is to determine the problem that the paper is trying to solve and get a preliminary idea of the proposed solution and results. Generally, I can determine all the above characteristics just by the reading the introduction and conclusion if the paper is well written.\nDuring the second pass, I read the results, discussion and limitation sections in full. During this phase I {edit, elaborate} my earlier notes made during first pass and also add my personal remarks pertaining to the paper. When reading scientific work in detail, it is often helpful to read at a macro and micro level. For instance, before reading a section from top to bottom - in a linear sense - I often find it helpful to first scan the sub-sections and draw a tree diagram of the topics. This allows me to have a visual representation of the section which often times is enough to quickly conduct a mental cost-benefit analysis of reading the section in detail.\nI rarely read a paper in it’s entirety. I think this is essential when you peer-review a paper (which I do not do yet as of [2021-06-30 Wed]) but unnecessary for day-to-day scientific work.\n\n\nWriting notes\nNotes on paper being read will always be in relation to the current research focus/goal. A paper may be read multiple times but will have different notes if they were made with reference to different research questions.\nFollowing this rationale, I store the notes in an org file pertaining to the project I am working on. I store the pdfs in a central location, synced with a cloud service provider so as to have offline access.\nAnd following are the aspects that I look out for when reading papers.\n+ problem statement :: What is the problem the paper is trying to\n  solve? Why is this problem important? Why should I care? This\n  generally leads to a good material for the introduction of my\n  paper as well.\n+ solution :: What is the solution the paper proposes? How is it\n  better compared to existing ones (do the authors try to show its\n  worth by comparing to existing solutions)? Why should I care about\n  it?\n+ results :: What were the analytical experiments conducted and what\n  were their results? What datasets were used? What models were used\n  and how did they perform? How was their performance evaluated?\n\n  Generally, in my field these questions can be answered by reading\n  the results section, very rarely are results disclosed in the\n  conclusion section. Note that falls out of the 'first pass' content\n  so I may come back to this only if I need this information for the\n  task at hand.\n+ limitations :: Does the paper identify its limitations? what\n  future work can be done?\n+ remarks :: My personal remarks on the paper. Was it well written? Is\n  it scientifically sound? Does it present a good overview of the\n  problem it is trying to solve? This critical examination and\n  cross-checking is essential when conducting a literature review.\nAfter writing down notes on the topics above, I try to write a one line summary of the paper along with my remarks. These are always guided by the research question or work at hand.\nThe “why should I care?” flavor questions will generally be guided by the research question or personal interest that I may have at the time of reading.\nIt’s worth elaborating on the writing process itself because this in itself is a highly non-linear and dynamic activity. With regards to my personal writing, I have noticed that the quantity and quality of notes are inversely proportional. In the early stages of a project, or when I am trying to familiarise myself with a new research topic, I tend to write more notes which are primitive (meaning they emphasise on the first principles) and are similar to the papers I am reading at that point in time. However, with exposure to more papers, the notes become more concise and opinionated (they reflect my thoughts and ideas on the matter).\nI also employ writing on paper and typing in an iterative, cyclic manner to formulate concise and clear text which express my discoveries, ideas and opinions.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/project-management/index.html",
    "href": "blogs/project-management/index.html",
    "title": "Project Management",
    "section": "",
    "text": "I think every project has 3 distinct components.\n\nKnowledge. Which may include notes, wikis, task lists, meeting notes, reviews, planning, etc.\nProduct. Which for me tends to be software, data, visualizations, etc.\nPublish. Which for research can be a paper or for product driven projects can be blog posts, company announcements, etc.\n\nThere are several tools for project management. The one that worked best for me was Basecamp and I have stolen many of their philosophies to craft my own.\nThe principles of managing a project is fairly simple. First, identify who are involved in the project, what you are going to do, how you are going to do it and by when you are going to do it.\n\nThe Who\nThe who become the stakeholders or clients of the project and need to be kept up to date on the progress, potential problems, approached for input, etc. Effective and clear communication is of utmost importance here.\n\n\nThe What\nThe what is the goal of the project. Some of the questions that we may ask are: What are we trying to solve? What is our research question? What are we building? etc.\n\n\nThe How\nThe how is the task list. Try to visualise the entire lifecycle of the project (to the best of your abilities given your current knowledge and technical capabilities) and write down the distinct phases of the project. Next, for each phase write down a list of tasks (and sub tasks). Keep a fluid mindset, projects change, clients want different things, research is dynamic. The idea is to start with a set of tasks and revise as and when necessary.\n\n\nThe When\nThe when is the date of completion. Deadlines are important for proper goal and expectation setting. It’s important to have a final deadline for the entire project, but also to have intermediate deadlines for smaller batches/mini-projects. Divide and conquer is the name of the game here.\nSome recommend a few weeks (4-6) for these batches/mini-projects. For research, I have found that taking things at a week-by-week basis works best and is the most flexible method of working given how dynamic research can be.\nFor each of these batches, I like to have a single focus of work. I then extract the relevant tasks that I should get done in order to accomplish the focus.\nDetermining focus is subjective and external factors such as other engagements play a role. The goal is to always plan sufficient work so as to not under deliver. I actively try not to plan too many things so as to feel overwhelmed or anxious.\n\n\n\n\n\n\nUPDATE 2021-09-05\n\n\n\nSome notes on attaining focus can be found here.\n\n\nAt the end of each batch/cycle/week I conduct a review. I think about what went well (so things I was able to finish), what did not go well (problems, challenges, etc.) and what can be improved (mostly for personal reflection). During the review I triage the task list, revise project scope, deliverables, etc.\n\n\n\n\n\n\nNote\n\n\n\nYou can find more about my reviewing process here.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/focus/index.html",
    "href": "blogs/focus/index.html",
    "title": "Focus",
    "section": "",
    "text": "Usually around late July - early August, I experience what I call “Productivity Anxiety”. It starts by me siting down in front of my laptop to work but not being able to identify what to work on. After an hour or so of contemplation, I start feeling guilty of not working. I know that if I can’t get myself to work on something I should get up and do something else. But then I would be missing out on work wouldn’t I? And so this vicious cycle continues over days, weeks, sometimes entire months.\nThe quickest way to get over this mental block is to work on something of importance, something with stakes where you have something to lose. Work projects are a good place to start. However that’s not enough. The other part is knowing what to work on, or what to focus our time and energy on.\nThe assumption here is that we have a functional task/information management system in place. This system allows us to collect all tasks in a centralised location which makes finding things to do next easier. Task management is not enough though, we must establish a notion of priority next. I find goals and project deadlines are a good place to start to derive some prioritisation of tasks/projects at a macro level (think year, quarter or month). For long running projects that stretch over several months or years (which is fairly common in research projects) I find setting pseudo-deadlines a great way to establish what to work on next.\n\n\n\n\n\n\nTip\n\n\n\nSee my prior post on productivity and research workflow for how I manage information.\n\n\nAlthough it’s easier to see progress over a longer period of time such as months or a year, we must approach work in smaller chunks such as over a few days or weeks. At this micro scale, I find assigning a theme per day helps minimise context switch, gain focus and stop wasting time trying to identify what to work on next. This theme can simply be a specific project or a group of similar tasks. For instance, I dedicate Mondays to writing and Fridays to reading papers. The rest of the days I break based on the current projects I am working on.\n\n\n\n\n\n\nTip\n\n\n\nSee project management for how I manage projects.\n\n\nTime blocking sounds like a wonderful productivity technique on paper, but in practise it does not work. First, it takes a lot of time to plan out work blocks every week. Second, I simply do not want to live my life based on what a piece of software tells me to do. Life is dynamic and time blocking does not accommodate emergencies or impromptu decisions. I don’t pre-plan work blocks. Instead, when I do find a block of uninterrupted time, I dedicate 100% of my energy to the task(s) at hand. I turn off all distractions and minimise breaks. Usually a 15 min stretch/coffee/bathroom break in the middle of a 3 hour block works well.\nThe overarching secret to the success of this system is the act of periodic review. The reviews help me adjust the priorities and tasks for the upcoming week/month/year. I also find tracking the time spent on the work blocks valuable. This paints a nice overview of how I spent my time during the last day/week/month and helps identify any “leaks” which need to be addressed.\n\n\n\n\n\n\nTip\n\n\n\nNotes on my reviewing process can be found here.\n\n\nPerhaps the most important realisation I have had is that enjoying the free time, the mundane things such as cleaning and also doing absolutely nothing is equally important as working. Being in the moment and mindful when spending time away from the screen has made the single biggest difference in my work-life balance. Even though I have a constant hum of work to do, I am able to remain sane and produce a consistent, high quality of work by simply being present in the moment.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/scientific-paper-discovery/index.html",
    "href": "blogs/scientific-paper-discovery/index.html",
    "title": "Scientific Paper Discovery",
    "section": "",
    "text": "In this article I describe my process for discovering relevant and important papers in a new scientific field. Since I am working towards a doctoral degree, I need to read a lot to develop a deep understanding of my field of research and stay current on the latest developments.\n\nWhere to Start\nThe starting point is always the most difficult. If possible, the best place to start is through recommendation from your peers/supervisor/advisor. If this is not possible, then coming up with a few good keywords from your research question is the next best approach. Reading a few non-scientific work (such as wikipedia) might be necessary to obtain the “buzz words” of technical jargon which are required to formulate the search queries.\nThe right way to familiarize oneself with a new field is through a systematic literature review. However, I find that to be too big a commitment for a new field of research (I may not stick with it if I don’t find it that interesting). So the smarter (and quicker way) to familiarize oneself with the literature is to read the related papers.\n\n\nFinding More Papers\nI categorise this search process into two parts: 1. Intrinsic and 2. Extrinsic.\nWe can find relevant papers from within the current paper we are reading. This can be done by first reading the related work section of the paper and second by reviewing the reference section of the paper. The related work section often leads to papers that are at a similar level of technical depth as the current paper. The papers in the reference section tend to be more general and provide a good high level overview of the field.\nWe can also find relevant papers from the search engine (Google Scholar in my case). The cited by feature of GS is a good place to find forward references or the papers which are citing the current paper. These tend to be more specific and bleeding edge as they came after the current paper. The related articles are also a good place to find similar bodies of work. Google Scholar and Connected Papers are good resources to explore this.\nUsually, 5-10 good, well reputed papers is sufficient to start with. And then, reading their related papers (and in turn their related papers, so on and so forth as necessary), should result in a collection of bibliography which paints a good picture of the field.\n\n\nStaying Current\nOne also needs to stay current on the field. For this, Google Scholar alerts are a good start. By now the “position papers” and important authors in the field should be known. Following these authors on Google Scholar and creating an alert for when a position paper gets cited is a good way to stay up to speed. Another approach is to keep an eye on the relevant conferences and find papers directly from there.\n\n\nConcluding Remarks and Acknowledgements\nI was surprised to find that not a lot of information on this matter is easily available. It is similar to how students are not taught how to learn effectively, before they start attending university classes. The emphasis on the “meta”, the methodology of doing something is lacking. I cannot take credit for the wisdom above, a lot of it came from conversations with my peers and Google Scholar help website.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/nutrition/index.html",
    "href": "blogs/nutrition/index.html",
    "title": "Nutrition",
    "section": "",
    "text": "This document provides information regarding diet and nutrition. The premise is to identify underlying principles that determine optimal dietary requirements for weight loss and gain. For the prior goal, I am interested in nutrition which will provide optimal recovery and minimal muscle loss. For the later, I am interested in maximising muscle gain whilst minimising fat gain.\nNutrition is simply thermodynamics: Energy in must equal energy out for a system in equilibrium. If we consider that our body is the system, then the food we consume (energy in) must equal the energy required for our body to function (energy out). Logic thus dictates that when we eat less food than required, we lose weight and conversely, gain weight when we eat more.\nThe quantity of food/caloric intake required to maintain our bodyweight can be estimated from a TDEE calculator (easily available online, I have been using this one). From there, research has shown that a 10-20% increase/decrease in caloric intake will result in optimal weight gain/loss.\nEmphasis must also be put on the macro and micro nutrients. Specifically, research shows that 120gm of protein minimum results in optimal muscular recovery. Up to 160gm or 0.8/lb of bodyweight (whichever is larger) has been proven to result in optimal muscle gain when trying to gain weight. For the micro nutrients, it goes without saying, eat vegetables as much as possible! I personally also take vitamin D, omega-3 and multi-vitamin supplements. Vegetables also help me feel full longer.\nSome lessons I have learned from experience. Having a good diet is not a temporary setting and is precisely the reason why diets do not work. Having a good diet requires time, patience and a lot of experimentation to find out what works for the individual. Having a good diet is only the first part, having a good diet which is also sustainable is the key challenge (remember, you need to eat until you die). Finally, I think this requires a fair share of psychological strengthening. I noticed that for me, it took years until I found a good balance of food that works for me and learned to control my hunger/cravings.\nFollowing are some no-nonsense, community driven, public information that I keep coming back to time-and-time again. r/bodyweightfitness wiki contains links to valuable bodyweight fitness routines. The r/fitness wiki has several important pages. Specifically, the entries on weight/fat loss, muscle building and the FAQ page.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aocp/index.html",
    "href": "blogs/aocp/index.html",
    "title": "Aru’s Org Capture Template (aocp.el)",
    "section": "",
    "text": "After observing my workflow of managing bibliographic information in Emacs, I extracted the repeated actions into an Emacs package.\nTo gain some perspective on my workflow, see my prior article on my research workflow.\nThe package is available on github with two alternative installation methods: 1. By manually downloading the aocp.el file and sticking it in your Emacs load-path, or 2. Using straight.el which is what I recommend.\nThe package works under the assumption that you manage your bibliographic information in org-mode (a major-mode for Emacs). The functions made available through this package are intended to be used in an org-capture template, they are not meant to be called interactively (ie. by using M-x).\nAssuming that you have a bibtex entry in your kill-ring (either by killing text within Emacs or by coping text from an external application into your clipboard), this package will do the following:\n\nExtract the bibkey\nExtract the first author\nExtract the last author\nExtract the source of publication\n\n* TODO %(aocp--get-bibkey nil)\n  :PROPERTIES:\n  :PDF: file:~/Documents/papers/%(aocp--get-bibkey t).pdf\n  :FIRST_AUTHOR: %(aocp--get-first-author)\n  :LAST_AUTHOR: %(aocp--get-last-author)\n  :SOURCE: %(aocp--get-source)\n  :END:\n%?\n+ problem statement ::\n+ solution ::\n+ results ::\n+ limitations ::\n+ remarks ::\n\n  #+begin_src bibtex :tangle yes\n  %c\n  #+end_src\nAssuming you have the above template in paper.txt, you can configure org as follows (replace your-org-inbox-file appropriately):\n(setq org-capture-templates\n    '((\"p\" \"Paper\" entry (file+headline your-org-inbox-file \"Inbox\")\n    \"%[~/.emacs.d/org-templates/paper.txt]\")))\nWith this in place, you can quickly collect all bibliographic information within an org file. Leveraging the powerful functionality provided by org-properties, one can quickly find relevant papers. For instance, I can look up all papers by author X or all papers by author X published at Y.\nA nice little tip is to download a local copy of the pdf and save them all in a folder. To make this easier, aocp.el also pushes the bibkey to the kill-ring. So all that is left to do is click the download button and paste the bibkey as the file name. This ensure 1. That you have all pdfs names consistently and 2. You have a link to the pdf from your org file (see the :PDF: property in the template above) which you can open by hitting C-c C-o over the link. You do not need to poke around in the directory containing the pdfs, all the context is available in the org file and should be the point of entry for all your bibliographic needs!\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/productivity/index.html",
    "href": "blogs/productivity/index.html",
    "title": "Productivity",
    "section": "",
    "text": "Productivity to me comes from proper information management. Information can come in many forms: email, thoughts/ideas, deadlines, events, project planning, online links, and many more."
  },
  {
    "objectID": "blogs/productivity/index.html#capturing",
    "href": "blogs/productivity/index.html#capturing",
    "title": "Productivity",
    "section": "Capturing",
    "text": "Capturing\nI find that absolutely nothing can beat a pen and a paper for this job (see the section on the benefits of writing on paper below).\nFor the rare occasions when pen and paper cannot do the job (such as saving a link to an online article to be read later or when we are on the move), a digital solution is required. Any app will do, as long as it has a “quick capture” mechanism (most todo apps do these days). You can also simply email yourself if you prefer not to have another dependency, another moving part, another “cog” in your system."
  },
  {
    "objectID": "blogs/productivity/index.html#reviewing",
    "href": "blogs/productivity/index.html#reviewing",
    "title": "Productivity",
    "section": "Reviewing",
    "text": "Reviewing\nNow that I have captured information, what do we do with it? 90% of it is just noise and there is a fine line between expanding knowledge and hoarding knowledge. Here, I find it important to maintain a reference of sort to determine what is important and what is not.\nI find that writing on paper acts as a natural buffer to ‘noise’.\nI review my notes on a weekly, monthly, quarterly and yearly basis. The goal of each review is to progressively summarise the captured ideas. So, weekly reviews summarize daily logs, monthly reviews summarize the weekly reviews, and so on and so forth. This progressive summarization is a deliberate practise of writing that allows me to develop my ideas."
  },
  {
    "objectID": "logs.html",
    "href": "logs.html",
    "title": "Codelog",
    "section": "",
    "text": "Bits and pieces of information I frequently use. Meant for quick lookup and reference.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nPacking and Unpacking **kwargs\n\n\n\n\n\n\n\npython\n\n\n\n\nPacking and unpacking keyword arguments in Python.\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\nRipgrep commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nRipgrep (rg) commands I frequently use.\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\nDired Commands\n\n\n\n\n\n\n\nemacs\n\n\n\n\nDired commands I frequently use.\n\n\n\n\n\n\nApr 23, 2021\n\n\n\n\n\n\n\n\nPython context file\n\n\n\n\n\n\n\npython\n\n\n\n\nPython context file for managing imports.\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\nshell path_finder\n\n\n\n\n\n\n\nshell\n\n\n\n\nHow $PATH is constructed on OSX.\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\nPrivate Link Sharing\n\n\n\n\n\n\n\nweb\n\n\n\n\nPoor man’s document sharing with private links, sort of what you get with Google Docs.\n\n\n\n\n\n\nJan 1, 2018\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Following is a list of public talks I have given in the past.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nabstract\n\n\n\n\n\n\nBridging the Gap Between Visual and Analytical Machine Learning Testing\n\n\nJun 2, 2023\n\n\nLightening talk at SEN Symposium 2023. Presented my research direction, work done so far, challenges and next steps. Content was mostly similar to the talk below. \n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nMay 15, 2023\n\n\nPresentation for our poster titled Towards Understanding Machine Learning Testing in Practice which was published in CAIN 2023. \n\n\n\n\nData Validation with TFDV\n\n\nMay 16, 2022\n\n\nIn this lecture we will go over the basics of data validation. The first half of this lecture will be a talk on the fundamentals of data validation. We will answer what is data validation?, why should we validate our data? and how we can validate our data?. The second half of the lecture will be a hands-on tutorial on using Tensorflow Data Validation, instructions & code for which can be found on this github repo. \n\n\n\n\nData Smells in Public Datasets\n\n\nMay 4, 2022\n\n\nIn this talk I will present our recent paper titled Data Smells in Public Datasets which was published at the 1st International Conference on AI Engineering (CAIN) 2022. I will first present the problem we are trying to solve along with the contributions that we made. I will present the methodology which was followed along with the results obtained. I will present a select few smells which I personally find interesting & hope will generate some discussion. Finally, we will conclude the talk with some high level takeaways from our study along with the limitations & future directions of work. \n\n\n\n\nPrivacy Preserving Deep Learning\n\n\nSep 7, 2021\n\n\nA talk on Privacy Preserving Deep Learning (PPDL) I gave to my research group. It was largly based on a literature review I did during my Msc. \n\n\n\n\nResearch Workflow in Plaintext\n\n\nJul 12, 2021\n\n\nIn this talk I will go over how we can use Emacs and org-mode to craft a research workflow. We will look at how we can leverage the power of Emacs and org-mode to capture, store, search and retrieve research data, all in plain text! The talk will touch upon how org-mode can be used as an environment for literate programming and reproducible research. I do not assume any prior knowledge of emacs or org-mode and I want this to be more of a discussion rather than a talk. Please ask me questions as I go along and share your thoughts, tips and techniques with others! \n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "talks/bridging-gap-between-visual-analytical-ml-testing/index.html",
    "href": "talks/bridging-gap-between-visual-analytical-ml-testing/index.html",
    "title": "Bridging the Gap Between Visual and Analytical ML Testing",
    "section": "",
    "text": "Introduction\nHello everyone and thank you for being here.\nI am Arumoy. I am a PhD Candidate at the Software Engineering Research Group at TU Delft. I have the privilege of working with excellent researchers such as Luis and Arie (who are somewhere in the audience). And after 2 years, I have found my calling.\nIn this talk, I am going to present the current vision I have for my PhD. I hope to generate some interesting discussions and get some feedback along the way.\n\n\nImplicit expectations to explicit tests\nHopefully, I don’t need to convience you that testing is important. We are software engineers, attending a software engineering conference afterall.\nThere is a wonderful paper by Zhang et al. (2020) that summarises the existing literature on ML testing. However, what has been ignored—until now—is the role of visualisations and how we use visual tests in the earlier, more “data-centric” stages of the ML lifecycle.\n\nZhang, Jie M, Mark Harman, Lei Ma, and Yang Liu. 2020. “Machine Learning Testing: Survey, Landscapes and Horizons.” IEEE Transactions on Software Engineering.\nVisualisations enable a rapid, exploratory form of analysis. Practitioners use “visual tests” to check for data properties. These visualisations tell a story. They are there for a reason. The expertise and domain knowledge is embedded within the visualisation.\nThis works really well when we are working on our laptop, on say an assignment. But visualisations do not scale well across organisation changes or when we want to move towards a large-scale production system. Visual tests tend to be left behind as latent expectations, rather than explicit failing tests. This research gap between moving from visual to analytical tests is where we wish to contribute.\nVisualisations become latent expectations rather than explicit tests. And this gap between going from latent visualisations to more analytical tests is exactly the research gap where we wish to contribute.\n\n\nThe hunt for data properties\nThe good news is that we have a rich source of data—jupyter notebooks. We are using a two-pronged approach. The first step—which I have been working on for the past month—is to collect these visualisations or data properties manually. We are exploring two sources of data, namely github and kaggle.\nOnce we have found a sufficient quantity of data properties, we will scale it to a larger subset. We are aware of two such datasets proposed by Quaranta, Calefato, and Lanubile (2021) and Pimentel et al. (2019) which contains notebooks from Kaggle and Github respectively.\n\nQuaranta, Luigi, Fabio Calefato, and Filippo Lanubile. 2021. “KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle.” In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), 550–54. IEEE.\n\nPimentel, João Felipe, Leonardo Murta, Vanessa Braganholo, and Juliana Freire. 2019. “A Large-Scale Study about Quality and Reproducibility of Jupyter Notebooks.” In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR), 507–17. IEEE.\n\n\nNext steps and beyond\nOur immediate objective is to provide a format definition of “visual tests” along with examples of alternative analytical tests that can be used by the practitioners.\nOur ultimate research goal is to recommend such analytical tests automatically to the practitioner. Here it becomes a mining challenge which jupyter notebooks contain three sources of information: text, code and images.\nWe see several opportunities to collaborate with researchers working in other areas. Besides ML testing, I see implications in reproducibity and code quality of jupyter notebooks, explainable AI and HCI.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Following is a list of projects I am currently working on, or have worked on in the past.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nDescription\n\n\nWebsite\n\n\n\n\n\n\nAutomatic Trailer Generation Using Genetic Algorithms\n\n\nJan 1, 2023\n\n\nIn collaboration with RTL and ICT with Industry, we developed a prototype to automatically generate a trailer for a given episode of Goede Tijd, Slechte Tijd, a popular Dutch TV Show. Unfortunately, the project is not open source. \n\n\nundefined\n\n\n\n\naocp.el\n\n\nJan 1, 2023\n\n\nAn emacs package to manage bibliographic information in org-mode (a major mode for emacs). The project is available on [Github]. \n\n\nhttps://github.com/arumoy-shome/aocp.el\n\n\n\n\nvim-zettel\n\n\nJan 1, 2020\n\n\nA (neo)vim plugin to manange plaintext notes. \n\n\nhttps://github.com/arumoy-shome/vim-zettel\n\n\n\n\nvim-text-lists\n\n\nJan 1, 2020\n\n\nA (neo)vim plugin that provides functions for working with plain text lists. \n\n\nhttps://github.com/arumoy-shome/vim-text-lists\n\n\n\n\n3D Kadaster\n\n\nJan 1, 2018\n\n\nA 3D model of all buildings in the Netherlands using point cloud dataset and large scale compute. \n\n\nhttps://arumoy.me/3d-kadaster\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arumoy Shome",
    "section": "",
    "text": "I am a researcher at TU Delft, working towards my doctorate as part of the Software Engineering Research Group. My research interests lie at the intersection of Data Science, Software Engineering and Design. Here I am developing tools, techniques and best practises which allow us to build, test, deploy, scale and maintain modern software systems with Machine Learning (ML) components. While we must look to the micro to identify and understand a problem, I try to keep an eye towards the current era of big-data and large-scale ML as a long-term area of research. You can find a list of my publications here.\nI completed my Msc. in Computer Science from the Vrije Universiteit Amsterdam and Universiteit van Amsterdam in the Netherlands, specializing in Big Data Engineering. As part of my thesis, I conducted interdisciplinary research with Nikhef and The Netherlands eScience Center to improve the data processing pipeline of the KM3NeT Neutrino Telescope using Graph Convolutional Neural Networks.\nI obtained my Bachelor’s in The Applied Sciences from the University of Waterloo in Canada, specializing in System Design Engineering with an option in Entrepreneurship. Here, I learned to bring creative solutions to complex problems with multiple facets such as society, economics, environment and politics. I also learned how to nurture an idea at it’s inception, develop it using systems theory and successfully bring it to the market as a finished product.\nMy professional experience is two pronged. I have over 5 years of software development experience through professional web development positions at and small companies. I am also versed with driven, research oriented projects rooted in diverse topics such as Large Scale Data Engineering, Data Visualization and Artificial Intelligence.\nI enjoy sharing knowledge and having discussions and am always happy to engage with other publicly. Sometimes, I write about my day-to-day challenges as a software engineering, data scientist & researcher.\nAll materials (excluding links to external websites or third party products) on this website are open sourced under the Creative Commons BY-NC-SA 3.0 license.\n\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Following is a list of my scientific publications.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Venue\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Authors\n        \n         \n          affiliation\n        \n         \n          doi\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nVenue\n\n\nDate\n\n\nAuthors\n\n\naffiliation\n\n\ndoi\n\n\n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nProceedings of 2nd International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 15, 2023\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://arxiv.org/abs/2305.04988\n\n\n\n\nData Smells in Public Datasets\n\n\nProceedings of 1st International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 1, 2022\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://doi.org/10.1145/3522664.3528621\n\n\n\n\nPrivacy Preserving Deep Learning for Medical Imaging\n\n\nMsc. Systematic Literature Review, unpublished\n\n\nDec 1, 2020\n\n\nArumoy Shome,Saba Amiri,Adam Belloum\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://arumoy.me/literature-study/ppdl.pdf\n\n\n\n\nKM3NeT Neutrino Detection using Deep Learning\n\n\nMsc. Thesis, unpublished\n\n\nOct 29, 2020\n\n\nArumoy Shome,Adam Belloum,Ben van Werkhoven,Ronald Bruijn\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://arumoy.me/km3net/km3net.pdf\n\n\n\n\nACE: Art, Color and Emotions\n\n\nProceedings of the 27th ACM Conference on Multimedia\n\n\nOct 1, 2019\n\n\nGjorgji Strezoski,Arumoy Shome,Riccardo Bianchi,Shruti Rao,Marcel Worring\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://dl.acm.org/doi/abs/10.1145/3343031.3350588\n\n\n\n\nImproving the Cognitive Assessment of Individuals with Down Syndrome\n\n\nBsc. Capstone Project, unpublished\n\n\nJan 1, 2018\n\n\nMaathusan Rajendram,Arumoy Shome,Mira Sleiman\n\n\nUniversity of Waterloo\n\n\nhttps://arumoy.me/elevate/report.pdf\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "logs/shell-path-helper.html",
    "href": "logs/shell-path-helper.html",
    "title": "shell path_finder",
    "section": "",
    "text": "The /etc/profile file is executed each time a login shell is started. If we investigate this file, we see the following:\n\n\n/etc/profile\n\n# System-wide .profile for sh(1)\n\nif [ -x /usr/libexec/path_helper ]; then\n    eval `/usr/libexec/path_helper -s`\nfi\n\nif [ \"${BASH-no}\" != \"no\" ]; then\n    [ -r /etc/bashrc ] && . /etc/bashrc\nfi\n\nSo each time a login shell is started, path_helper is run. In short, it takes the paths listed in /etc/paths & etc/paths.d/, appends the existing PATH and clears the duplicates1.1 This Stack overflow post explains how path_helper operates in mode details.\nTmux2 always runs as a login shell. Thus a common problem within tmux is duplicate entries =PATH= (in a different order than what we specify in our shell config files).2 https://github.com/tmux/tmux/wiki\nThus, it’s okay to append to =PATH= in the shell config files, but prepending causes unwanted side-effects. We could edit =etc/paths= manually, but this requires sudo and is generally not advised.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/dired-commands.html",
    "href": "logs/dired-commands.html",
    "title": "Dired Commands",
    "section": "",
    "text": "% m\nmark the files matching the provided regex (also useful % d which marks the files for deletion)\n\n\n% g\nmark the files whose contents match the provided regex (essentially dired interface to grep)\n\n\nt\ntoggle the mark (I usually follow this with k to kill the lines, and g to restore)\n\n\n! or &\nrun shell command in current dir, if marked files are present pass them to the command as arguments\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/packing-unpacking-kwargs.html",
    "href": "logs/packing-unpacking-kwargs.html",
    "title": "Packing and Unpacking **kwargs",
    "section": "",
    "text": "In python, we can unpack keyword arguments using the **kwargs in the function definition like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\n\n\nTurns out, that we can convert the unpacked dict into back into keyword arguments and pass it along to another function as well! This is done like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\n    meet(**kwargs)\n\ndef meet(a=None, b=None):\n    print(\"Nice to meet you again, {} & {}\".format(a, b))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\nNice to meet you again, Aru & Ura\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "This page contains links to some of my writings on topics that interest me. Usually, they are inspired by problems that I experience in my day-to-day life.\nI like pondering over the act or the process of doing something.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nPacking and Unpacking **kwargs\n\n\n\n\n\n\n\npython\n\n\n\n\nPacking and unpacking keyword arguments in Python.\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\nRipgrep commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nRipgrep (rg) commands I frequently use.\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\nAru’s Information Management System (AIMS)\n\n\n\n\n\nAIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\n\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\nReflections on Scientific Research\n\n\n\n\n\n\n\nresearch\n\n\n\n\nReflections on the scientific process and what constitutes being a good researcher.\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\nFocus\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nNotes on attaining focus.\n\n\n\n\n\n\nSep 5, 2021\n\n\n\n\n\n\n\n\nProject Management\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nNotes on project management.\n\n\n\n\n\n\nAug 27, 2021\n\n\n\n\n\n\n\n\nNutrition\n\n\n\n\n\nNotes on diet and nutrition.\n\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\nAru’s Org Capture Template (aocp.el)\n\n\n\n\n\nAn Emacs package I wrote for managing bibliographic information.\n\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\nScientific Paper Discovery\n\n\n\n\n\n\n\nresearch\n\n\n\n\nMy process of discovering relevant and important papers in a new scientific field.\n\n\n\n\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\nResearch Workflow\n\n\n\n\n\n\n\nproductivity\n\n\nresearch\n\n\n\n\nAn outline of my research workflow which I have developed to handle the non-linear nature of scientific work.\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nProductivity to me comes from proper information management. Information can come in many forms: email, thoughts/ideas, deadlines, events, project planning, online links, and many more.\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\nDired Commands\n\n\n\n\n\n\n\nemacs\n\n\n\n\nDired commands I frequently use.\n\n\n\n\n\n\nApr 23, 2021\n\n\n\n\n\n\n\n\nPython context file\n\n\n\n\n\n\n\npython\n\n\n\n\nPython context file for managing imports.\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\nshell path_finder\n\n\n\n\n\n\n\nshell\n\n\n\n\nHow $PATH is constructed on OSX.\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\nPrivate Link Sharing\n\n\n\n\n\n\n\nweb\n\n\n\n\nPoor man’s document sharing with private links, sort of what you get with Google Docs.\n\n\n\n\n\n\nJan 1, 2018\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "All materials (excluding links to external websites and third party websites) on this website are open sourced under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported license. In brief, you are free to share and adapt all materials in any way you wish provided that you cite my work, don’t sell the adapted work for money and share it under the same license.\nThe above text was adapted from this website. You can also read the full license.\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/python-context-file.html",
    "href": "logs/python-context-file.html",
    "title": "Python context file",
    "section": "",
    "text": "Stolen from The Hitchhiker’s Guide to Python1, I use a context.py file to import python source code located in another directory. This is especially useful for importing source code into test files (which are typically located under the test/ directory for me) and for ipython notebooks (which are typically located under the notebooks directory for me).1 https://docs.python-guide.org/writing/structure/\nStick the following snippet in a context.py file in the directory where the source code is required to be imported.\n\n\ncontext.py\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport sample\n\nAnd in the file where the module need to be imported, stick the following.\n\n\nyour-python-file.py\n\nfrom .context import sample\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/private-link-sharing.html",
    "href": "logs/private-link-sharing.html",
    "title": "Private Link Sharing",
    "section": "",
    "text": "Poor man’s document sharing with private links, sort of like what you get with Google Docs. The solution is very simple and I stumbled upon it when I was trying to device a solution for publishing my non-scientific work. The following snippet, when put in the html source, prevents web crawlers from indexing the page. Thus, the page can be hosted on a web server (I use Github Pages) but won’t show up on a web search engine (such as Google, Bing, Yahoo, etc.).\n\n\nindex.html\n\n&lt;meta name=\"robots\" content=\"noindex\" /&gt;\n\nThe page is thus only accessible to those who possess the page url. There are several strategies to generate unique urls. I tend to prepend a timestamp to all my file names. You could also generate a uuid for your file names using the uuidgen command on *nix like operating systems.\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/ripgrep-commands.html",
    "href": "logs/ripgrep-commands.html",
    "title": "Ripgrep commands",
    "section": "",
    "text": "--no-ignore\ndon’t ignore patterns in .gitignore file\n\n\n--hidden\ndon’t ignore hidden (dot) files\n\n\n--text/-a\ndon’t ignore binary files\n\n\n--follow\ndon’t ignore symlinks\n\n\n--fixed-string/-F\ntreat pattern as literal string (not regex)\n\n\n--type/-t\nlimit search scope to specific filetype\n\n\n--type-not/-T\ninverse of --type\n\n\n--type-list\nprint builtin types\n\n\n--glob/-g\ninclude manually specified glob patterns in search scope, use ! in glob pattern to inverse\n\n\n\nRipgrep follows the Rust regex syntax, more details can be found here but the usual stuff mostly apply as well.\nBy default, rg performs a recursive search in the current directory while respecting the glob patterns in the .gitignore file. It also ignores hidden (dot) files, binary files and symbolic links. The --no-ignore and --hidden flags can be used to change that. Binary files can be searched using the --text/-a flag and symlinks can be followed with --follow.\nTo ignore certain glob patterns in rg (but not in git) a =.ignore= file can be placed within the directory.\nAn example of an inverse glob pattern is shown below.\n  rg hello -g '!*.md' # don't search in md files\nA note on rg’s types: These are customisable and a hand full of them are built in. rg --type-list lists them. New types can be added using the --type-add flag. To persist the new type, create a shell alias or define a rg config file.\nAs previously mentioned, rg can be configured using a config file. The name does not matter since rg looks for the file using the RIPGREP_CONFIG_PATH environment variable.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/private-link-sharing/index.html",
    "href": "blogs/private-link-sharing/index.html",
    "title": "Private Link Sharing",
    "section": "",
    "text": "Poor man’s document sharing with private links, sort of like what you get with Google Docs. The solution is very simple and I stumbled upon it when I was trying to device a solution for publishing my non-scientific work. The following snippet, when put in the html source, prevents web crawlers from indexing the page. Thus, the page can be hosted on a web server (I use Github Pages) but won’t show up on a web search engine (such as Google, Bing, Yahoo, etc.).\n\n\nindex.html\n\n&lt;meta name=\"robots\" content=\"noindex\" /&gt;\n\nThe page is thus only accessible to those who possess the page url. There are several strategies to generate unique urls. I tend to prepend a timestamp to all my file names. You could also generate a uuid for your file names using the uuidgen command on *nix like operating systems.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/shell-path-finder/index.html",
    "href": "blogs/shell-path-finder/index.html",
    "title": "shell path_finder",
    "section": "",
    "text": "The /etc/profile file is executed each time a login shell is started. If we investigate this file, we see the following:\n\n\n/etc/profile\n\n# System-wide .profile for sh(1)\n\nif [ -x /usr/libexec/path_helper ]; then\n    eval `/usr/libexec/path_helper -s`\nfi\n\nif [ \"${BASH-no}\" != \"no\" ]; then\n    [ -r /etc/bashrc ] && . /etc/bashrc\nfi\n\nSo each time a login shell is started, path_helper is run. In short, it takes the paths listed in /etc/paths & etc/paths.d/, appends the existing PATH and clears the duplicates1.1 This Stack overflow post explains how path_helper operates in mode details.\nTmux2 always runs as a login shell. Thus a common problem within tmux is duplicate entries =PATH= (in a different order than what we specify in our shell config files).2 https://github.com/tmux/tmux/wiki\nThus, it’s okay to append to =PATH= in the shell config files, but prepending causes unwanted side-effects. We could edit =etc/paths= manually, but this requires sudo and is generally not advised.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/ripgrep-commands/index.html",
    "href": "blogs/ripgrep-commands/index.html",
    "title": "Ripgrep commands",
    "section": "",
    "text": "--no-ignore\ndon’t ignore patterns in .gitignore file\n\n\n--hidden\ndon’t ignore hidden (dot) files\n\n\n--text/-a\ndon’t ignore binary files\n\n\n--follow\ndon’t ignore symlinks\n\n\n--fixed-string/-F\ntreat pattern as literal string (not regex)\n\n\n--type/-t\nlimit search scope to specific filetype\n\n\n--type-not/-T\ninverse of --type\n\n\n--type-list\nprint builtin types\n\n\n--glob/-g\ninclude manually specified glob patterns in search scope, use ! in glob pattern to inverse\n\n\n\nRipgrep follows the Rust regex syntax, more details can be found here but the usual stuff mostly apply as well.\nBy default, rg performs a recursive search in the current directory while respecting the glob patterns in the .gitignore file. It also ignores hidden (dot) files, binary files and symbolic links. The --no-ignore and --hidden flags can be used to change that. Binary files can be searched using the --text/-a flag and symlinks can be followed with --follow.\nTo ignore certain glob patterns in rg (but not in git) a =.ignore= file can be placed within the directory.\nAn example of an inverse glob pattern is shown below.\n  rg hello -g '!*.md' # don't search in md files\nA note on rg’s types: These are customisable and a hand full of them are built in. rg --type-list lists them. New types can be added using the --type-add flag. To persist the new type, create a shell alias or define a rg config file.\nAs previously mentioned, rg can be configured using a config file. The name does not matter since rg looks for the file using the RIPGREP_CONFIG_PATH environment variable.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/packing-unpacking-kwargs/index.html",
    "href": "blogs/packing-unpacking-kwargs/index.html",
    "title": "Packing and Unpacking **kwargs",
    "section": "",
    "text": "In python, we can unpack keyword arguments using the **kwargs in the function definition like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\n\n\nTurns out, that we can convert the unpacked dict into back into keyword arguments and pass it along to another function as well! This is done like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\n    meet(**kwargs)\n\ndef meet(a=None, b=None):\n    print(\"Nice to meet you again, {} & {}\".format(a, b))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\nNice to meet you again, Aru & Ura\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/python-context-file/index.html",
    "href": "blogs/python-context-file/index.html",
    "title": "Python context file",
    "section": "",
    "text": "Stolen from The Hitchhiker’s Guide to Python1, I use a context.py file to import python source code located in another directory. This is especially useful for importing source code into test files (which are typically located under the test/ directory for me) and for ipython notebooks (which are typically located under the notebooks directory for me).1 https://docs.python-guide.org/writing/structure/\nStick the following snippet in a context.py file in the directory where the source code is required to be imported.\n\n\ncontext.py\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport sample\n\nAnd in the file where the module need to be imported, stick the following.\n\n\nyour-python-file.py\n\nfrom .context import sample\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/dired-commands/index.html",
    "href": "blogs/dired-commands/index.html",
    "title": "Dired Commands",
    "section": "",
    "text": "% m\nmark the files matching the provided regex (also useful % d which marks the files for deletion)\n\n\n% g\nmark the files whose contents match the provided regex (essentially dired interface to grep)\n\n\nt\ntoggle the mark (I usually follow this with k to kill the lines, and g to restore)\n\n\n! or &\nrun shell command in current dir, if marked files are present pass them to the command as arguments\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/reflections-scientific-research/index.html",
    "href": "blogs/reflections-scientific-research/index.html",
    "title": "Reflections on Scientific Research",
    "section": "",
    "text": "I have been working as a researcher for several months now and have been working towards my first publication for the past few weeks. I have a few points of reflection on the scientific process and what constitutes as a good researcher.\n\nReading\nThis has been a challenge for me so far. I started my PhD with a literature review and during those initial 3 months I read a lot of papers. However once the analysis started, the reading dropped down to 0 papers per week. I am not sure if it is possible to keep a consistent reading practise, however reading at least 1 or 2 papers per week is ideal. Reading not only helps to be inspired and spark new ideas, but also makes for good content for the related work & discussion section of your paper. Moving forward, I want to review recent publications from the top journals of my field, and try to reflect upon what they did well, what I will do differently and identify interesting intersections between their topic and my research interests.\n\n\nWriting\nI started to use org-mode at the start of my PhD, capture and organize my thoughts and ideas. This worked out well because when the time came to start writing, I already had a pretty good outline for the paper ready. One thing that was still a challenge (at least for the first week of writing) is that the process itself was very slow. I don’t think this can be helped/improved. This is simply my process and all I can really do it work through it. Moving forward, I will try to plan out content for the report throughout the project.\n\n\nPaper Discovery\nThis is relevant for reading & writing. I think my current system works really well (see scientific paper discovery).\n\n\nExperimentation & Methodology\nThis could have been planned better. While writing the report I realised several loops & flaws in my data collection and methodology. My supervisor has recommended a book (see reference below) which I will read prior to our next project. Moving forward, I will also try to think about the dataset and its design earlier.\n\n\nGeneral Thoughts & Remarks\nI am curious to understand how my supervisor was able to see the potential in the idea we ended up pursuing versus the ones I proposed. Is this something that comes with experience? How can I spot a good (scientific research) idea?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aims/index.html",
    "href": "blogs/aims/index.html",
    "title": "Aru’s Information Management System (AIMS)",
    "section": "",
    "text": "AIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\nAlthough org-mode is great, the primary motivation for writing AIMs is because I was feeling a lot of resistance when trying to tune it to my workflow, primarily because of Elisp. Org-mode also requires that you use Emacs as your text editor. I did not appreciate the “vendor lock-in” enforced by org-mode.\nYou can find the latest version of the script on my dotfiles repo, below is the script as it stands on 2022-02-28.\n\n\naims\n\n#!/usr/bin/env bash\n\n1NOTESDIR=\"$HOME/org\"\nINBOX=\"$NOTESDIR/inbox.md\"\nTEMPLATESDIR=\"$XDG_DATA_HOME/aims\"\n[[ ! -e \"$INBOX\" ]] && touch \"$INBOX\"\n\n__capture() {\n  # Capture incoming info quickly. All items are appended to INBOX\n  # which defaults to `inbox.md' in NOTESDIR. Optionally a template\n  # can be specified using the --template| -t flag.\n2  local TEMPLATE=\"$TEMPLATESDIR/default\"\n\n  while [[ \"$1\" =~ ^-+.* && ! \"$1\" == \"--\" ]]; do\n    case \"$1\" in\n      --template | -t)\n        shift\n        TEMPLATE=\"$TEMPLATESDIR/$1\"\n        ;;\n      *)\n        echo \"Error: unknown option $1.\"\n        return 1\n        ;;\n    esac; shift\n  done\n\n3  local ITEM=$(mktemp)\n  if [[ -e \"$TEMPLATE\" && -x \"$TEMPLATE\" ]]; then\n    eval \"$TEMPLATE $ITEM\"\n  fi\n\n4  if eval \"$EDITOR -c 'set ft=markdown' $ITEM\"; then\n    [[ \"$1\" && -e \"$NOTESDIR/$1\" ]] && INBOX=\"$NOTESDIR/$1\"\n    cat \"$ITEM\" &gt;&gt; \"$INBOX\"\n    echo \"Info: captured in $INBOX.\"\n  fi\n\n5  echo \"Info: cleaning up $(rm -v \"$ITEM\")\"\n}\n\n__capture \"$@\"\n\n\n1\n\nStore all notes in $HOME/org/inbox.md, creating it if necessary. Also look for template scripts in ~/.local/share/aims.\n\n2\n\nParse the flags passed to AIMS. Currently it only supports the --template/-t flag which accepts the name of the template to use. Use the default template if none is provided. More on this later.\n\n3\n\nCreate a temporary file and insert the contents of the template.\n\n4\n\nEdit the temporary file using $EDITOR (here I assume its vim or neovim), setting the filetype to markdown. If the first positional argument passed to AIMS is a valid file inside $NOTESDIR then set that to the $INBOX file. Finally, prepend the contents of the temporary file to $INBOX file, if vim does not report an error.\n\n5\n\nCleanup, remove the temporary file.\n\n\nFor the time being, it only provides the capture functionality. A temporary file is used to compose the text first. Upon successful completion, the contents of the temporary file are appended to the default $INBOX file if no other files are specified.\nWhat I find really neat is the templating system. An arbitrary name for a template can be passed to aims using the --template (or -t for short) flag. aims looks for a shellscript with the same name in the ~/.local/share/aims directory and executes it if it exists. The beauty of this design is in its simplicity. Since templates are shellscripts, it gives us the full expressiveness of the shell. This is best demonstrated with some examples. Here is my default template as of 2022-02-28 which is used when no template is specified.\n\n\n~/.local/share/aims/default\n\n#!/usr/bin/env bash\n\n1[[ -z \"$1\" ]] && return 1\n\n2echo &gt;&gt; \"$1\"\necho \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\n\n\n1\n\nSanity check, ensure that a positional argument was passed (that is, the temporary file path).\n\n2\n\nInsert an empty line and a level 1 markdown header with a time stamp.\n\n\nIt simply adds a level 1 markdown header followed by a timestamp. Here is another for capturing bibtex information for research papers.\n#!/usr/bin/env bash\n\n[[ -z \"$1\" ]] && return 1\n\necho &gt;&gt; \"$1\"\n\n1BIBKEY=$(pbpaste | grep '^@.*' | sed 's/^@.*{\\(.*\\),/\\1/')\nif [[ -n \"$BIBKEY\" ]]; then\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')] $BIBKEY\" &gt;&gt; $1\nelse\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\nfi\n\n2echo &gt;&gt; \"$1\"\necho '+ **Problem Statement:**' &gt;&gt; \"$1\"\necho '+ **Solution**' &gt;&gt; \"$1\"\necho '+ **Results**' &gt;&gt; \"$1\"\necho '+ **Limitations**' &gt;&gt; \"$1\"\necho '+ **Remarks**' &gt;&gt; \"$1\"\necho &gt;&gt; \"$1\"\n3echo '```bibtex' &gt;&gt; \"$1\"\n\nif [[ -n \"$BIBKEY\" ]]; then\n  pbpaste | sed '/^$/d' &gt;&gt; \"$1\"\n  pbcopy &lt;(echo \"$BIBKEY\")\nfi\n\necho '```' &gt;&gt; \"$1\"\n\n1\n\nCheck that the bibtex information is currently in the system clipboard by attempting to extract the key using grep and sed. If a key was succussfully extracted, then create a level 1 markdown header with a time stamp and the key. Otherwise, fall back to just a time stamp.\n\n2\n\nAdd my prompts for note-taking when reading scientific papers.\n\n3\n\nRemove empty lines and put the bibtex information in a markdown source block.\n\n\nThis one is a bit more involved but highlights the power of using shellscripts for templating. Given that a bibentry is copied in the clipboard, this template adds a level 1 markdown header with a timestamp and the bibkey. It adds my note-taking prompts and sticks the bibentry at the bottom.\n\n\n\n Back to top"
  }
]