[
  {
    "objectID": "blogs/research-workflow/index.html",
    "href": "blogs/research-workflow/index.html",
    "title": "Research Workflow",
    "section": "",
    "text": "Here I outline a workflow which I have developed to handle the non-linear nature of scientific research. There are days when one reads several papers without making much sense of them. However, a single paper the next day can result in a clear picture of the domain. The workflow acknowledges the fact that there are many more failures rather than successes in research. However, a failure is still an outcome—and this is what helps us decide what to do next—so it should still be recorded.\nResearch workflows are highly personal. If you decide to adopt any or all ideas I present here, it may require some tweaking so that it works for you. Years of performing an extrinsic search for existing solutions was unfruitful. Instead, I simply started reading papers and let the process develop organically. Soon I noticed some common patterns and standardisation which allowed me to automate some aspects of the process.\nThere are 3 interdependent phases to this process described below.\n\nFinding papers\nI save the bibtex entry in a bib file (generally in the project root along with the other latex files). This is because the bibtex-mode in Emacs provides powerful search options, templates for adding entries, sorting entries, cleaning up, and much more. Besides, the bib files ultimately gets used in the report so it makes sense to group the two together and keep them in the same location.\n\n\n\n\n\n\nUPDATE 2021-06-15 Tue\n\n\n\nI no longer save bibtex entry in a bib file. Instead, I store them with my notes in the same org file in source code blocks. I use org-tangle to generate the bib file when needed.\n\n\nFirst, I use good ol’ Google Scholar (along with a plugin to enable my institution’s proxy) to find papers. Once, I find a paper I open it and read the abstract. If the abstract appeals to my current research focus or is interesting to me in general, I download it and save it in a folder which syncs across my devices. Next, I grab the bibtex entry and create a task using org-capture where I save the bibtex entry, one liner on why I found the paper interesting. I add the location to the pdf on disk, I set the title of the task as the bibtex entry and I save it to my inbox (or relevant project org file).\n\n\n\n\n\n\nUPDATE 2021-06-15 Tue\n\n\n\nI have written an Elisp package that automates all of this. With the bibtex entry in my clipboard, I now issue an org-capture template which creates a new task with the bibkey as the header, adds relevant properties such as first author last author and source of publication, and sticks the bibtex entry in a source block (see aocp.el).\n\n\nIn scientific paper discovery I describe my process of finding relevant papers in more detail.\n\n\nReading papers\nI read the papers on my laptop. My screen is usually split 50-50 between Emacs (where I take notes) and a pdf viewer. I do not make any highlights because its easy to abuse and it ties the information to the pdf viewer application that I am using.\nInstead, I take notes in plain text (as of 2020, I have been using org-mode, an Emacs major mode to do so). There are several, rationales for doing this digitally. First, I require these notes to be searchable and link-able to other notes. Second, research projects often span for a long duration. Having digital notes ensures that I do not have notes distributed amongst several analog papers/journals. Lastly, I take notes on papers in a specific manner. These notes have a specific purpose - and thus a carefully constructed structure as described in the next section - and thus come under the category of hard information (I talk more about productivity and how I categorise information in productivity).\nI use the 3 pass technique as presented by S. Keshav in his paper titled “How to read a paper” (2007). I employ the first pass as a screening method to identify if a given paper is worth reading in full. During the first pass, I read the introduction and conclusion and briefly glance over the sections of the paper and the diagrams. The goal here is to determine the problem that the paper is trying to solve and get a preliminary idea of the proposed solution and results. Generally, I can determine all the above characteristics just by the reading the introduction and conclusion if the paper is well written.\nDuring the second pass, I read the results, discussion and limitation sections in full. During this phase I {edit, elaborate} my earlier notes made during first pass and also add my personal remarks pertaining to the paper. When reading scientific work in detail, it is often helpful to read at a macro and micro level. For instance, before reading a section from top to bottom - in a linear sense - I often find it helpful to first scan the sub-sections and draw a tree diagram of the topics. This allows me to have a visual representation of the section which often times is enough to quickly conduct a mental cost-benefit analysis of reading the section in detail.\nI rarely read a paper in it’s entirety. I think this is essential when you peer-review a paper (which I do not do yet as of [2021-06-30 Wed]) but unnecessary for day-to-day scientific work.\n\n\nWriting notes\nNotes on paper being read will always be in relation to the current research focus/goal. A paper may be read multiple times but will have different notes if they were made with reference to different research questions.\nFollowing this rationale, I store the notes in an org file pertaining to the project I am working on. I store the pdfs in a central location, synced with a cloud service provider so as to have offline access.\nAnd following are the aspects that I look out for when reading papers.\n+ problem statement :: What is the problem the paper is trying to\n  solve? Why is this problem important? Why should I care? This\n  generally leads to a good material for the introduction of my\n  paper as well.\n+ solution :: What is the solution the paper proposes? How is it\n  better compared to existing ones (do the authors try to show its\n  worth by comparing to existing solutions)? Why should I care about\n  it?\n+ results :: What were the analytical experiments conducted and what\n  were their results? What datasets were used? What models were used\n  and how did they perform? How was their performance evaluated?\n\n  Generally, in my field these questions can be answered by reading\n  the results section, very rarely are results disclosed in the\n  conclusion section. Note that falls out of the 'first pass' content\n  so I may come back to this only if I need this information for the\n  task at hand.\n+ limitations :: Does the paper identify its limitations? what\n  future work can be done?\n+ remarks :: My personal remarks on the paper. Was it well written? Is\n  it scientifically sound? Does it present a good overview of the\n  problem it is trying to solve? This critical examination and\n  cross-checking is essential when conducting a literature review.\nAfter writing down notes on the topics above, I try to write a one line summary of the paper along with my remarks. These are always guided by the research question or work at hand.\nThe “why should I care?” flavor questions will generally be guided by the research question or personal interest that I may have at the time of reading.\nIt’s worth elaborating on the writing process itself because this in itself is a highly non-linear and dynamic activity. With regards to my personal writing, I have noticed that the quantity and quality of notes are inversely proportional. In the early stages of a project, or when I am trying to familiarise myself with a new research topic, I tend to write more notes which are primitive (meaning they emphasise on the first principles) and are similar to the papers I am reading at that point in time. However, with exposure to more papers, the notes become more concise and opinionated (they reflect my thoughts and ideas on the matter).\nI also employ writing on paper and typing in an iterative, cyclic manner to formulate concise and clear text which express my discoveries, ideas and opinions.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/project-management/index.html",
    "href": "blogs/project-management/index.html",
    "title": "Project Management",
    "section": "",
    "text": "I think every project has 3 distinct components.\n\nKnowledge. Which may include notes, wikis, task lists, meeting notes, reviews, planning, etc.\nProduct. Which for me tends to be software, data, visualizations, etc.\nPublish. Which for research can be a paper or for product driven projects can be blog posts, company announcements, etc.\n\nThere are several tools for project management. The one that worked best for me was Basecamp and I have stolen many of their philosophies to craft my own.\nThe principles of managing a project is fairly simple. First, identify who are involved in the project, what you are going to do, how you are going to do it and by when you are going to do it.\n\nThe Who\nThe who become the stakeholders or clients of the project and need to be kept up to date on the progress, potential problems, approached for input, etc. Effective and clear communication is of utmost importance here.\n\n\nThe What\nThe what is the goal of the project. Some of the questions that we may ask are: What are we trying to solve? What is our research question? What are we building? etc.\n\n\nThe How\nThe how is the task list. Try to visualise the entire lifecycle of the project (to the best of your abilities given your current knowledge and technical capabilities) and write down the distinct phases of the project. Next, for each phase write down a list of tasks (and sub tasks). Keep a fluid mindset, projects change, clients want different things, research is dynamic. The idea is to start with a set of tasks and revise as and when necessary.\n\n\nThe When\nThe when is the date of completion. Deadlines are important for proper goal and expectation setting. It’s important to have a final deadline for the entire project, but also to have intermediate deadlines for smaller batches/mini-projects. Divide and conquer is the name of the game here.\nSome recommend a few weeks (4-6) for these batches/mini-projects. For research, I have found that taking things at a week-by-week basis works best and is the most flexible method of working given how dynamic research can be.\nFor each of these batches, I like to have a single focus of work. I then extract the relevant tasks that I should get done in order to accomplish the focus.\nDetermining focus is subjective and external factors such as other engagements play a role. The goal is to always plan sufficient work so as to not under deliver. I actively try not to plan too many things so as to feel overwhelmed or anxious.\n\n\n\n\n\n\nUPDATE 2021-09-05\n\n\n\nSome notes on attaining focus can be found here.\n\n\nAt the end of each batch/cycle/week I conduct a review. I think about what went well (so things I was able to finish), what did not go well (problems, challenges, etc.) and what can be improved (mostly for personal reflection). During the review I triage the task list, revise project scope, deliverables, etc.\n\n\n\n\n\n\nNote\n\n\n\nYou can find more about my reviewing process here.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/focus/index.html",
    "href": "blogs/focus/index.html",
    "title": "Focus",
    "section": "",
    "text": "Usually around late July - early August, I experience what I call “Productivity Anxiety”. It starts by me siting down in front of my laptop to work but not being able to identify what to work on. After an hour or so of contemplation, I start feeling guilty of not working. I know that if I can’t get myself to work on something I should get up and do something else. But then I would be missing out on work wouldn’t I? And so this vicious cycle continues over days, weeks, sometimes entire months.\nThe quickest way to get over this mental block is to work on something of importance, something with stakes where you have something to lose. Work projects are a good place to start. However that’s not enough. The other part is knowing what to work on, or what to focus our time and energy on.\nThe assumption here is that we have a functional task/information management system in place. This system allows us to collect all tasks in a centralised location which makes finding things to do next easier. Task management is not enough though, we must establish a notion of priority next. I find goals and project deadlines are a good place to start to derive some prioritisation of tasks/projects at a macro level (think year, quarter or month). For long running projects that stretch over several months or years (which is fairly common in research projects) I find setting pseudo-deadlines a great way to establish what to work on next.\n\n\n\n\n\n\nTip\n\n\n\nSee my prior post on productivity and research workflow for how I manage information.\n\n\nAlthough it’s easier to see progress over a longer period of time such as months or a year, we must approach work in smaller chunks such as over a few days or weeks. At this micro scale, I find assigning a theme per day helps minimise context switch, gain focus and stop wasting time trying to identify what to work on next. This theme can simply be a specific project or a group of similar tasks. For instance, I dedicate Mondays to writing and Fridays to reading papers. The rest of the days I break based on the current projects I am working on.\n\n\n\n\n\n\nTip\n\n\n\nSee project management for how I manage projects.\n\n\nTime blocking sounds like a wonderful productivity technique on paper, but in practise it does not work. First, it takes a lot of time to plan out work blocks every week. Second, I simply do not want to live my life based on what a piece of software tells me to do. Life is dynamic and time blocking does not accommodate emergencies or impromptu decisions. I don’t pre-plan work blocks. Instead, when I do find a block of uninterrupted time, I dedicate 100% of my energy to the task(s) at hand. I turn off all distractions and minimise breaks. Usually a 15 min stretch/coffee/bathroom break in the middle of a 3 hour block works well.\nThe overarching secret to the success of this system is the act of periodic review. The reviews help me adjust the priorities and tasks for the upcoming week/month/year. I also find tracking the time spent on the work blocks valuable. This paints a nice overview of how I spent my time during the last day/week/month and helps identify any “leaks” which need to be addressed.\n\n\n\n\n\n\nTip\n\n\n\nNotes on my reviewing process can be found here.\n\n\nPerhaps the most important realisation I have had is that enjoying the free time, the mundane things such as cleaning and also doing absolutely nothing is equally important as working. Being in the moment and mindful when spending time away from the screen has made the single biggest difference in my work-life balance. Even though I have a constant hum of work to do, I am able to remain sane and produce a consistent, high quality of work by simply being present in the moment.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/scientific-paper-discovery/index.html",
    "href": "blogs/scientific-paper-discovery/index.html",
    "title": "Scientific Paper Discovery",
    "section": "",
    "text": "In this article I describe my process for discovering relevant and important papers in a new scientific field. Since I am working towards a doctoral degree, I need to read a lot to develop a deep understanding of my field of research and stay current on the latest developments.\n\nWhere to Start\nThe starting point is always the most difficult. If possible, the best place to start is through recommendation from your peers/supervisor/advisor. If this is not possible, then coming up with a few good keywords from your research question is the next best approach. Reading a few non-scientific work (such as wikipedia) might be necessary to obtain the “buzz words” of technical jargon which are required to formulate the search queries.\nThe right way to familiarize oneself with a new field is through a systematic literature review. However, I find that to be too big a commitment for a new field of research (I may not stick with it if I don’t find it that interesting). So the smarter (and quicker way) to familiarize oneself with the literature is to read the related papers.\n\n\nFinding More Papers\nI categorise this search process into two parts: 1. Intrinsic and 2. Extrinsic.\nWe can find relevant papers from within the current paper we are reading. This can be done by first reading the related work section of the paper and second by reviewing the reference section of the paper. The related work section often leads to papers that are at a similar level of technical depth as the current paper. The papers in the reference section tend to be more general and provide a good high level overview of the field.\nWe can also find relevant papers from the search engine (Google Scholar in my case). The cited by feature of GS is a good place to find forward references or the papers which are citing the current paper. These tend to be more specific and bleeding edge as they came after the current paper. The related articles are also a good place to find similar bodies of work. Google Scholar and Connected Papers are good resources to explore this.\nUsually, 5-10 good, well reputed papers is sufficient to start with. And then, reading their related papers (and in turn their related papers, so on and so forth as necessary), should result in a collection of bibliography which paints a good picture of the field.\n\n\nStaying Current\nOne also needs to stay current on the field. For this, Google Scholar alerts are a good start. By now the “position papers” and important authors in the field should be known. Following these authors on Google Scholar and creating an alert for when a position paper gets cited is a good way to stay up to speed. Another approach is to keep an eye on the relevant conferences and find papers directly from there.\n\n\nConcluding Remarks and Acknowledgements\nI was surprised to find that not a lot of information on this matter is easily available. It is similar to how students are not taught how to learn effectively, before they start attending university classes. The emphasis on the “meta”, the methodology of doing something is lacking. I cannot take credit for the wisdom above, a lot of it came from conversations with my peers and Google Scholar help website.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/nutrition/index.html",
    "href": "blogs/nutrition/index.html",
    "title": "Nutrition",
    "section": "",
    "text": "This document provides information regarding diet and nutrition. The premise is to identify underlying principles that determine optimal dietary requirements for weight loss and gain. For the prior goal, I am interested in nutrition which will provide optimal recovery and minimal muscle loss. For the later, I am interested in maximising muscle gain whilst minimising fat gain.\nNutrition is simply thermodynamics: Energy in must equal energy out for a system in equilibrium. If we consider that our body is the system, then the food we consume (energy in) must equal the energy required for our body to function (energy out). Logic thus dictates that when we eat less food than required, we lose weight and conversely, gain weight when we eat more.\nThe quantity of food/caloric intake required to maintain our bodyweight can be estimated from a TDEE calculator (easily available online, I have been using this one). From there, research has shown that a 10-20% increase/decrease in caloric intake will result in optimal weight gain/loss.\nEmphasis must also be put on the macro and micro nutrients. Specifically, research shows that 120gm of protein minimum results in optimal muscular recovery. Up to 160gm or 0.8/lb of bodyweight (whichever is larger) has been proven to result in optimal muscle gain when trying to gain weight. For the micro nutrients, it goes without saying, eat vegetables as much as possible! I personally also take vitamin D, omega-3 and multi-vitamin supplements. Vegetables also help me feel full longer.\nSome lessons I have learned from experience. Having a good diet is not a temporary setting and is precisely the reason why diets do not work. Having a good diet requires time, patience and a lot of experimentation to find out what works for the individual. Having a good diet is only the first part, having a good diet which is also sustainable is the key challenge (remember, you need to eat until you die). Finally, I think this requires a fair share of psychological strengthening. I noticed that for me, it took years until I found a good balance of food that works for me and learned to control my hunger/cravings.\nFollowing are some no-nonsense, community driven, public information that I keep coming back to time-and-time again. r/bodyweightfitness wiki contains links to valuable bodyweight fitness routines. The r/fitness wiki has several important pages. Specifically, the entries on weight/fat loss, muscle building and the FAQ page.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aocp/index.html",
    "href": "blogs/aocp/index.html",
    "title": "Aru’s Org Capture Template (aocp.el)",
    "section": "",
    "text": "After observing my workflow of managing bibliographic information in Emacs, I extracted the repeated actions into an Emacs package.\nTo gain some perspective on my workflow, see my prior article on my research workflow.\nThe package is available on github with two alternative installation methods: 1. By manually downloading the aocp.el file and sticking it in your Emacs load-path, or 2. Using straight.el which is what I recommend.\nThe package works under the assumption that you manage your bibliographic information in org-mode (a major-mode for Emacs). The functions made available through this package are intended to be used in an org-capture template, they are not meant to be called interactively (ie. by using M-x).\nAssuming that you have a bibtex entry in your kill-ring (either by killing text within Emacs or by coping text from an external application into your clipboard), this package will do the following:\n\nExtract the bibkey\nExtract the first author\nExtract the last author\nExtract the source of publication\n\n* TODO %(aocp--get-bibkey nil)\n  :PROPERTIES:\n  :PDF: file:~/Documents/papers/%(aocp--get-bibkey t).pdf\n  :FIRST_AUTHOR: %(aocp--get-first-author)\n  :LAST_AUTHOR: %(aocp--get-last-author)\n  :SOURCE: %(aocp--get-source)\n  :END:\n%?\n+ problem statement ::\n+ solution ::\n+ results ::\n+ limitations ::\n+ remarks ::\n\n  #+begin_src bibtex :tangle yes\n  %c\n  #+end_src\nAssuming you have the above template in paper.txt, you can configure org as follows (replace your-org-inbox-file appropriately):\n(setq org-capture-templates\n    '((\"p\" \"Paper\" entry (file+headline your-org-inbox-file \"Inbox\")\n    \"%[~/.emacs.d/org-templates/paper.txt]\")))\nWith this in place, you can quickly collect all bibliographic information within an org file. Leveraging the powerful functionality provided by org-properties, one can quickly find relevant papers. For instance, I can look up all papers by author X or all papers by author X published at Y.\nA nice little tip is to download a local copy of the pdf and save them all in a folder. To make this easier, aocp.el also pushes the bibkey to the kill-ring. So all that is left to do is click the download button and paste the bibkey as the file name. This ensure 1. That you have all pdfs names consistently and 2. You have a link to the pdf from your org file (see the :PDF: property in the template above) which you can open by hitting C-c C-o over the link. You do not need to poke around in the directory containing the pdfs, all the context is available in the org file and should be the point of entry for all your bibliographic needs!\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/productivity/index.html",
    "href": "blogs/productivity/index.html",
    "title": "Productivity",
    "section": "",
    "text": "Productivity to me comes from proper information management. Information can come in many forms: email, thoughts/ideas, deadlines, events, project planning, online links, and many more."
  },
  {
    "objectID": "blogs/productivity/index.html#capturing",
    "href": "blogs/productivity/index.html#capturing",
    "title": "Productivity",
    "section": "Capturing",
    "text": "Capturing\nI find that absolutely nothing can beat a pen and a paper for this job (see the section on the benefits of writing on paper below).\nFor the rare occasions when pen and paper cannot do the job (such as saving a link to an online article to be read later or when we are on the move), a digital solution is required. Any app will do, as long as it has a “quick capture” mechanism (most todo apps do these days). You can also simply email yourself if you prefer not to have another dependency, another moving part, another “cog” in your system."
  },
  {
    "objectID": "blogs/productivity/index.html#reviewing",
    "href": "blogs/productivity/index.html#reviewing",
    "title": "Productivity",
    "section": "Reviewing",
    "text": "Reviewing\nNow that I have captured information, what do we do with it? 90% of it is just noise and there is a fine line between expanding knowledge and hoarding knowledge. Here, I find it important to maintain a reference of sort to determine what is important and what is not.\nI find that writing on paper acts as a natural buffer to ‘noise’.\nI review my notes on a weekly, monthly, quarterly and yearly basis. The goal of each review is to progressively summarise the captured ideas. So, weekly reviews summarize daily logs, monthly reviews summarize the weekly reviews, and so on and so forth. This progressive summarization is a deliberate practise of writing that allows me to develop my ideas."
  },
  {
    "objectID": "logs.html",
    "href": "logs.html",
    "title": "Codelog",
    "section": "",
    "text": "Bits and pieces of information I frequently use. Meant for quick lookup and reference.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nPacking and Unpacking **kwargs\n\n\n\n\n\n\n\npython\n\n\n\n\nPacking and unpacking keyword arguments in Python.\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\nRipgrep commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nRipgrep (rg) commands I frequently use.\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\nDired Commands\n\n\n\n\n\n\n\nemacs\n\n\n\n\nDired commands I frequently use.\n\n\n\n\n\n\nApr 23, 2021\n\n\n\n\n\n\n\n\nPython context file\n\n\n\n\n\n\n\npython\n\n\n\n\nPython context file for managing imports.\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\nshell path_finder\n\n\n\n\n\n\n\nshell\n\n\n\n\nHow $PATH is constructed on OSX.\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\nPrivate Link Sharing\n\n\n\n\n\n\n\nweb\n\n\n\n\nPoor man’s document sharing with private links, sort of what you get with Google Docs.\n\n\n\n\n\n\nJan 1, 2018\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Following is a list of public talks I have given in the past.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nabstract\n\n\n\n\n\n\nBridging the Gap Between Visual and Analytical Machine Learning Testing\n\n\nJun 2, 2023\n\n\nLightening talk at SEN Symposium 2023. Presented my research direction, work done so far, challenges and next steps. Content was mostly similar to the talk below. \n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nMay 15, 2023\n\n\nPresentation for our poster titled Towards Understanding Machine Learning Testing in Practice which was published in CAIN 2023. \n\n\n\n\nData Validation with TFDV\n\n\nMay 16, 2022\n\n\nIn this lecture we will go over the basics of data validation. The first half of this lecture will be a talk on the fundamentals of data validation. We will answer what is data validation?, why should we validate our data? and how we can validate our data?. The second half of the lecture will be a hands-on tutorial on using Tensorflow Data Validation, instructions & code for which can be found on this github repo. \n\n\n\n\nData Smells in Public Datasets\n\n\nMay 4, 2022\n\n\nIn this talk I will present our recent paper titled Data Smells in Public Datasets which was published at the 1st International Conference on AI Engineering (CAIN) 2022. I will first present the problem we are trying to solve along with the contributions that we made. I will present the methodology which was followed along with the results obtained. I will present a select few smells which I personally find interesting & hope will generate some discussion. Finally, we will conclude the talk with some high level takeaways from our study along with the limitations & future directions of work. \n\n\n\n\nPrivacy Preserving Deep Learning\n\n\nSep 7, 2021\n\n\nA talk on Privacy Preserving Deep Learning (PPDL) I gave to my research group. It was largly based on a literature review I did during my Msc. \n\n\n\n\nResearch Workflow in Plaintext\n\n\nJul 12, 2021\n\n\nIn this talk I will go over how we can use Emacs and org-mode to craft a research workflow. We will look at how we can leverage the power of Emacs and org-mode to capture, store, search and retrieve research data, all in plain text! The talk will touch upon how org-mode can be used as an environment for literate programming and reproducible research. I do not assume any prior knowledge of emacs or org-mode and I want this to be more of a discussion rather than a talk. Please ask me questions as I go along and share your thoughts, tips and techniques with others! \n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "talks/bridging-gap-between-visual-analytical-ml-testing/index.html",
    "href": "talks/bridging-gap-between-visual-analytical-ml-testing/index.html",
    "title": "Bridging the Gap Between Visual and Analytical ML Testing",
    "section": "",
    "text": "Introduction\nHello everyone and thank you for being here.\nI am Arumoy. I am a PhD Candidate at the Software Engineering Research Group at TU Delft. I have the privilege of working with excellent researchers such as Luis and Arie (who are somewhere in the audience). And after 2 years, I have found my calling.\nIn this talk, I am going to present the current vision I have for my PhD. I hope to generate some interesting discussions and get some feedback along the way.\n\n\nImplicit expectations to explicit tests\nHopefully, I don’t need to convience you that testing is important. We are software engineers, attending a software engineering conference afterall.\nThere is a wonderful paper by Zhang et al. (2020) that summarises the existing literature on ML testing. However, what has been ignored—until now—is the role of visualisations and how we use visual tests in the earlier, more “data-centric” stages of the ML lifecycle.\n\nZhang, Jie M, Mark Harman, Lei Ma, and Yang Liu. 2020. “Machine Learning Testing: Survey, Landscapes and Horizons.” IEEE Transactions on Software Engineering.\nVisualisations enable a rapid, exploratory form of analysis. Practitioners use “visual tests” to check for data properties. These visualisations tell a story. They are there for a reason. The expertise and domain knowledge is embedded within the visualisation.\nThis works really well when we are working on our laptop, on say an assignment. But visualisations do not scale well across organisation changes or when we want to move towards a large-scale production system. Visual tests tend to be left behind as latent expectations, rather than explicit failing tests. This research gap between moving from visual to analytical tests is where we wish to contribute.\nVisualisations become latent expectations rather than explicit tests. And this gap between going from latent visualisations to more analytical tests is exactly the research gap where we wish to contribute.\n\n\nThe hunt for data properties\nThe good news is that we have a rich source of data—jupyter notebooks. We are using a two-pronged approach. The first step—which I have been working on for the past month—is to collect these visualisations or data properties manually. We are exploring two sources of data, namely github and kaggle.\nOnce we have found a sufficient quantity of data properties, we will scale it to a larger subset. We are aware of two such datasets proposed by Quaranta, Calefato, and Lanubile (2021) and Pimentel et al. (2019) which contains notebooks from Kaggle and Github respectively.\n\nQuaranta, Luigi, Fabio Calefato, and Filippo Lanubile. 2021. “KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle.” In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), 550–54. IEEE.\n\nPimentel, João Felipe, Leonardo Murta, Vanessa Braganholo, and Juliana Freire. 2019. “A Large-Scale Study about Quality and Reproducibility of Jupyter Notebooks.” In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR), 507–17. IEEE.\n\n\nNext steps and beyond\nOur immediate objective is to provide a format definition of “visual tests” along with examples of alternative analytical tests that can be used by the practitioners.\nOur ultimate research goal is to recommend such analytical tests automatically to the practitioner. Here it becomes a mining challenge which jupyter notebooks contain three sources of information: text, code and images.\nWe see several opportunities to collaborate with researchers working in other areas. Besides ML testing, I see implications in reproducibity and code quality of jupyter notebooks, explainable AI and HCI.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Following is a list of projects I am currently working on, or have worked on in the past.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nDescription\n\n\nWebsite\n\n\n\n\n\n\nAutomatic Trailer Generation Using Genetic Algorithms\n\n\nJan 1, 2023\n\n\nIn collaboration with RTL and ICT with Industry, we developed a prototype to automatically generate a trailer for a given episode of Goede Tijd, Slechte Tijd, a popular Dutch TV Show. Unfortunately, the project is not open source. \n\n\nundefined\n\n\n\n\naocp.el\n\n\nJan 1, 2023\n\n\nAn emacs package to manage bibliographic information in org-mode (a major mode for emacs). The project is available on [Github]. \n\n\nhttps://github.com/arumoy-shome/aocp.el\n\n\n\n\nvim-zettel\n\n\nJan 1, 2020\n\n\nA (neo)vim plugin to manange plaintext notes. \n\n\nhttps://github.com/arumoy-shome/vim-zettel\n\n\n\n\nvim-text-lists\n\n\nJan 1, 2020\n\n\nA (neo)vim plugin that provides functions for working with plain text lists. \n\n\nhttps://github.com/arumoy-shome/vim-text-lists\n\n\n\n\n3D Kadaster\n\n\nJan 1, 2018\n\n\nA 3D model of all buildings in the Netherlands using point cloud dataset and large scale compute. \n\n\nhttps://arumoy.me/3d-kadaster\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arumoy Shome",
    "section": "",
    "text": "I am a researcher at TU Delft, working towards my doctorate as part of the Software Engineering Research Group. My research interests lie at the intersection of Data Science, Software Engineering and Design. Here I am developing tools, techniques and best practises which allow us to build, test, deploy, scale and maintain modern software systems with Machine Learning (ML) components. While we must look to the micro to identify and understand a problem, I try to keep an eye towards the current era of big-data and large-scale ML as a long-term area of research. You can find a list of my publications here.\nI completed my Msc. in Computer Science from the Vrije Universiteit Amsterdam and Universiteit van Amsterdam in the Netherlands, specializing in Big Data Engineering. As part of my thesis, I conducted interdisciplinary research with Nikhef and The Netherlands eScience Center to improve the data processing pipeline of the KM3NeT Neutrino Telescope using Graph Convolutional Neural Networks.\nI obtained my Bachelor’s in The Applied Sciences from the University of Waterloo in Canada, specializing in System Design Engineering with an option in Entrepreneurship. Here, I learned to bring creative solutions to complex problems with multiple facets such as society, economics, environment and politics. I also learned how to nurture an idea at it’s inception, develop it using systems theory and successfully bring it to the market as a finished product.\nMy professional experience is two pronged. I have over 5 years of software development experience through professional web development positions at and small companies. I am also versed with driven, research oriented projects rooted in diverse topics such as Large Scale Data Engineering, Data Visualization and Artificial Intelligence.\nI enjoy sharing knowledge and having discussions and am always happy to engage with other publicly. Sometimes, I write about my day-to-day challenges as a software engineering, data scientist & researcher.\nAll materials (excluding links to external websites or third party products) on this website are open sourced under the Creative Commons BY-NC-SA 3.0 license.\n\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Following is a list of my scientific publications.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Venue\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Authors\n        \n         \n          affiliation\n        \n         \n          doi\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nVenue\n\n\nDate\n\n\nAuthors\n\n\naffiliation\n\n\ndoi\n\n\n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nProceedings of 2nd International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 15, 2023\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://arxiv.org/abs/2305.04988\n\n\n\n\nData Smells in Public Datasets\n\n\nProceedings of 1st International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 1, 2022\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://doi.org/10.1145/3522664.3528621\n\n\n\n\nPrivacy Preserving Deep Learning for Medical Imaging\n\n\nMsc. Systematic Literature Review, unpublished\n\n\nDec 1, 2020\n\n\nArumoy Shome,Saba Amiri,Adam Belloum\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://arumoy.me/literature-study/ppdl.pdf\n\n\n\n\nKM3NeT Neutrino Detection using Deep Learning\n\n\nMsc. Thesis, unpublished\n\n\nOct 29, 2020\n\n\nArumoy Shome,Adam Belloum,Ben van Werkhoven,Ronald Bruijn\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://arumoy.me/km3net/km3net.pdf\n\n\n\n\nACE: Art, Color and Emotions\n\n\nProceedings of the 27th ACM Conference on Multimedia\n\n\nOct 1, 2019\n\n\nGjorgji Strezoski,Arumoy Shome,Riccardo Bianchi,Shruti Rao,Marcel Worring\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://dl.acm.org/doi/abs/10.1145/3343031.3350588\n\n\n\n\nImproving the Cognitive Assessment of Individuals with Down Syndrome\n\n\nBsc. Capstone Project, unpublished\n\n\nJan 1, 2018\n\n\nMaathusan Rajendram,Arumoy Shome,Mira Sleiman\n\n\nUniversity of Waterloo\n\n\nhttps://arumoy.me/elevate/report.pdf\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "logs/shell-path-helper.html",
    "href": "logs/shell-path-helper.html",
    "title": "shell path_finder",
    "section": "",
    "text": "The /etc/profile file is executed each time a login shell is started. If we investigate this file, we see the following:\n\n\n/etc/profile\n\n# System-wide .profile for sh(1)\n\nif [ -x /usr/libexec/path_helper ]; then\n    eval `/usr/libexec/path_helper -s`\nfi\n\nif [ \"${BASH-no}\" != \"no\" ]; then\n    [ -r /etc/bashrc ] && . /etc/bashrc\nfi\n\nSo each time a login shell is started, path_helper is run. In short, it takes the paths listed in /etc/paths & etc/paths.d/, appends the existing PATH and clears the duplicates1.1 This Stack overflow post explains how path_helper operates in mode details.\nTmux2 always runs as a login shell. Thus a common problem within tmux is duplicate entries =PATH= (in a different order than what we specify in our shell config files).2 https://github.com/tmux/tmux/wiki\nThus, it’s okay to append to =PATH= in the shell config files, but prepending causes unwanted side-effects. We could edit =etc/paths= manually, but this requires sudo and is generally not advised.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/dired-commands.html",
    "href": "logs/dired-commands.html",
    "title": "Dired Commands",
    "section": "",
    "text": "% m\nmark the files matching the provided regex (also useful % d which marks the files for deletion)\n\n\n% g\nmark the files whose contents match the provided regex (essentially dired interface to grep)\n\n\nt\ntoggle the mark (I usually follow this with k to kill the lines, and g to restore)\n\n\n! or &\nrun shell command in current dir, if marked files are present pass them to the command as arguments\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/packing-unpacking-kwargs.html",
    "href": "logs/packing-unpacking-kwargs.html",
    "title": "Packing and Unpacking **kwargs",
    "section": "",
    "text": "In python, we can unpack keyword arguments using the **kwargs in the function definition like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\n\n\nTurns out, that we can convert the unpacked dict into back into keyword arguments and pass it along to another function as well! This is done like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\n    meet(**kwargs)\n\ndef meet(a=None, b=None):\n    print(\"Nice to meet you again, {} & {}\".format(a, b))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\nNice to meet you again, Aru & Ura\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "This page contains links to some of my writings on topics that interest me. Usually, they are inspired by problems that I experience in my day-to-day life.\nI like pondering over the act or the process of doing something.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nNavigating large markdown files in vim\n\n\n\n\n\n\n\nshell\n\n\nvim\n\n\n\n\nStrategies to navigate large plaintext (specifically markdown) files in vim.\n\n\n\n\n\n\nApr 29, 2023\n\n\n\n\n\n\n\n\nCMS using Pandoc and Friends\n\n\n\n\n\n\n\nshell\n\n\nweb\n\n\n\n\nSome tools & techniques I use to run a no non-sense blog using static html pages. All powered by a sane file naming convension, plaintext documents writing in markdown and exported to html using pandoc and other unix cli tools.\n\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\nPacking and Unpacking **kwargs\n\n\n\n\n\n\n\npython\n\n\n\n\nPacking and unpacking keyword arguments in Python.\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\nRipgrep commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nRipgrep (rg) commands I frequently use.\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\nEffortless Parallel Execution with xargs & Friends\n\n\n\n\n\n\n\nshell\n\n\n\n\nRecently, I had to run Tensorflow Data Validation on over 500 public datasets from Kaggle to generate a baseline schema file for further analysis. I chose to do this using the xargs unix command.\n\n\n\n\n\n\nMay 8, 2022\n\n\n\n\n\n\n\n\nThere and Back Again A Tale of Website Management\n\n\n\n\n\n\n\nshell\n\n\nvim\n\n\nweb\n\n\n\n\nManaging websites using markdown, shell and vim.\n\n\n\n\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\nTimestamps in the Shell (today)\n\n\n\n\n\n\n\nshell\n\n\nproductivity\n\n\n\n\nCreating timestamps in the terminal.\n\n\n\n\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\nAru’s Information Management System (AIMS)\n\n\n\n\n\n\n\nshell\n\n\nproductivity\n\n\n\n\nAIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\n\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\nReflections on Scientific Research\n\n\n\n\n\n\n\nresearch\n\n\n\n\nReflections on the scientific process and what constitutes being a good researcher.\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\nFocus\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nNotes on attaining focus.\n\n\n\n\n\n\nSep 5, 2021\n\n\n\n\n\n\n\n\nProject Management\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nNotes on project management.\n\n\n\n\n\n\nAug 27, 2021\n\n\n\n\n\n\n\n\nNutrition\n\n\n\n\n\n\n\nmisc\n\n\n\n\nNotes on diet and nutrition.\n\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\nAru’s Org Capture Template (aocp.el)\n\n\n\n\n\n\n\nemacs\n\n\nproductivity\n\n\n\n\nAn Emacs package I wrote for managing bibliographic information.\n\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\nScientific Paper Discovery\n\n\n\n\n\n\n\nresearch\n\n\n\n\nMy process of discovering relevant and important papers in a new scientific field.\n\n\n\n\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\nResearch Workflow\n\n\n\n\n\n\n\nproductivity\n\n\nresearch\n\n\n\n\nAn outline of my research workflow which I have developed to handle the non-linear nature of scientific work.\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nProductivity to me comes from proper information management. Information can come in many forms: email, thoughts/ideas, deadlines, events, project planning, online links, and many more.\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\nDired Commands\n\n\n\n\n\n\n\nemacs\n\n\n\n\nDired commands I frequently use.\n\n\n\n\n\n\nApr 23, 2021\n\n\n\n\n\n\n\n\nPython context file\n\n\n\n\n\n\n\npython\n\n\n\n\nPython context file for managing imports.\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\nshell path_finder\n\n\n\n\n\n\n\nshell\n\n\n\n\nHow $PATH is constructed on OSX.\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\nNavigating in Vim\n\n\n\n\n\n\n\nvim\n\n\n\n\nStrategies to navigate files in vim.\n\n\n\n\n\n\nAug 6, 2019\n\n\n\n\n\n\n\n\nSearching in Vim\n\n\n\n\n\n\n\nvim\n\n\n\n\nVarious strategies for searching text within vim.\n\n\n\n\n\n\nMay 13, 2019\n\n\n\n\n\n\n\n\nPrivate Link Sharing\n\n\n\n\n\n\n\nweb\n\n\n\n\nPoor man’s document sharing with private links, sort of what you get with Google Docs.\n\n\n\n\n\n\nJan 1, 2018\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "All materials (excluding links to external websites and third party websites) on this website are open sourced under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported license. In brief, you are free to share and adapt all materials in any way you wish provided that you cite my work, don’t sell the adapted work for money and share it under the same license.\nThe above text was adapted from this website. You can also read the full license.\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/python-context-file.html",
    "href": "logs/python-context-file.html",
    "title": "Python context file",
    "section": "",
    "text": "Stolen from The Hitchhiker’s Guide to Python1, I use a context.py file to import python source code located in another directory. This is especially useful for importing source code into test files (which are typically located under the test/ directory for me) and for ipython notebooks (which are typically located under the notebooks directory for me).1 https://docs.python-guide.org/writing/structure/\nStick the following snippet in a context.py file in the directory where the source code is required to be imported.\n\n\ncontext.py\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport sample\n\nAnd in the file where the module need to be imported, stick the following.\n\n\nyour-python-file.py\n\nfrom .context import sample\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/private-link-sharing.html",
    "href": "logs/private-link-sharing.html",
    "title": "Private Link Sharing",
    "section": "",
    "text": "Poor man’s document sharing with private links, sort of like what you get with Google Docs. The solution is very simple and I stumbled upon it when I was trying to device a solution for publishing my non-scientific work. The following snippet, when put in the html source, prevents web crawlers from indexing the page. Thus, the page can be hosted on a web server (I use Github Pages) but won’t show up on a web search engine (such as Google, Bing, Yahoo, etc.).\n\n\nindex.html\n\n&lt;meta name=\"robots\" content=\"noindex\" /&gt;\n\nThe page is thus only accessible to those who possess the page url. There are several strategies to generate unique urls. I tend to prepend a timestamp to all my file names. You could also generate a uuid for your file names using the uuidgen command on *nix like operating systems.\n\n\n\n Back to top"
  },
  {
    "objectID": "logs/ripgrep-commands.html",
    "href": "logs/ripgrep-commands.html",
    "title": "Ripgrep commands",
    "section": "",
    "text": "--no-ignore\ndon’t ignore patterns in .gitignore file\n\n\n--hidden\ndon’t ignore hidden (dot) files\n\n\n--text/-a\ndon’t ignore binary files\n\n\n--follow\ndon’t ignore symlinks\n\n\n--fixed-string/-F\ntreat pattern as literal string (not regex)\n\n\n--type/-t\nlimit search scope to specific filetype\n\n\n--type-not/-T\ninverse of --type\n\n\n--type-list\nprint builtin types\n\n\n--glob/-g\ninclude manually specified glob patterns in search scope, use ! in glob pattern to inverse\n\n\n\nRipgrep follows the Rust regex syntax, more details can be found here but the usual stuff mostly apply as well.\nBy default, rg performs a recursive search in the current directory while respecting the glob patterns in the .gitignore file. It also ignores hidden (dot) files, binary files and symbolic links. The --no-ignore and --hidden flags can be used to change that. Binary files can be searched using the --text/-a flag and symlinks can be followed with --follow.\nTo ignore certain glob patterns in rg (but not in git) a =.ignore= file can be placed within the directory.\nAn example of an inverse glob pattern is shown below.\n  rg hello -g '!*.md' # don't search in md files\nA note on rg’s types: These are customisable and a hand full of them are built in. rg --type-list lists them. New types can be added using the --type-add flag. To persist the new type, create a shell alias or define a rg config file.\nAs previously mentioned, rg can be configured using a config file. The name does not matter since rg looks for the file using the RIPGREP_CONFIG_PATH environment variable.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/private-link-sharing/index.html",
    "href": "blogs/private-link-sharing/index.html",
    "title": "Private Link Sharing",
    "section": "",
    "text": "Poor man’s document sharing with private links, sort of like what you get with Google Docs. The solution is very simple and I stumbled upon it when I was trying to device a solution for publishing my non-scientific work. The following snippet, when put in the html source, prevents web crawlers from indexing the page. Thus, the page can be hosted on a web server (I use Github Pages) but won’t show up on a web search engine (such as Google, Bing, Yahoo, etc.).\n\n\nindex.html\n\n&lt;meta name=\"robots\" content=\"noindex\" /&gt;\n\nThe page is thus only accessible to those who possess the page url. There are several strategies to generate unique urls. I tend to prepend a timestamp to all my file names. You could also generate a uuid for your file names using the uuidgen command on *nix like operating systems.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/shell-path-finder/index.html",
    "href": "blogs/shell-path-finder/index.html",
    "title": "shell path_finder",
    "section": "",
    "text": "The /etc/profile file is executed each time a login shell is started. If we investigate this file, we see the following:\n\n\n/etc/profile\n\n# System-wide .profile for sh(1)\n\nif [ -x /usr/libexec/path_helper ]; then\n    eval `/usr/libexec/path_helper -s`\nfi\n\nif [ \"${BASH-no}\" != \"no\" ]; then\n    [ -r /etc/bashrc ] && . /etc/bashrc\nfi\n\nSo each time a login shell is started, path_helper is run. In short, it takes the paths listed in /etc/paths & etc/paths.d/, appends the existing PATH and clears the duplicates1.1 This Stack overflow post explains how path_helper operates in mode details.\nTmux2 always runs as a login shell. Thus a common problem within tmux is duplicate entries =PATH= (in a different order than what we specify in our shell config files).2 https://github.com/tmux/tmux/wiki\nThus, it’s okay to append to =PATH= in the shell config files, but prepending causes unwanted side-effects. We could edit =etc/paths= manually, but this requires sudo and is generally not advised.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/ripgrep-commands/index.html",
    "href": "blogs/ripgrep-commands/index.html",
    "title": "Ripgrep commands",
    "section": "",
    "text": "--no-ignore\ndon’t ignore patterns in .gitignore file\n\n\n--hidden\ndon’t ignore hidden (dot) files\n\n\n--text/-a\ndon’t ignore binary files\n\n\n--follow\ndon’t ignore symlinks\n\n\n--fixed-string/-F\ntreat pattern as literal string (not regex)\n\n\n--type/-t\nlimit search scope to specific filetype\n\n\n--type-not/-T\ninverse of --type\n\n\n--type-list\nprint builtin types\n\n\n--glob/-g\ninclude manually specified glob patterns in search scope, use ! in glob pattern to inverse\n\n\n\nRipgrep follows the Rust regex syntax, more details can be found here but the usual stuff mostly apply as well.\nBy default, rg performs a recursive search in the current directory while respecting the glob patterns in the .gitignore file. It also ignores hidden (dot) files, binary files and symbolic links. The --no-ignore and --hidden flags can be used to change that. Binary files can be searched using the --text/-a flag and symlinks can be followed with --follow.\nTo ignore certain glob patterns in rg (but not in git) a =.ignore= file can be placed within the directory.\nAn example of an inverse glob pattern is shown below.\n  rg hello -g '!*.md' # don't search in md files\nA note on rg’s types: These are customisable and a hand full of them are built in. rg --type-list lists them. New types can be added using the --type-add flag. To persist the new type, create a shell alias or define a rg config file.\nAs previously mentioned, rg can be configured using a config file. The name does not matter since rg looks for the file using the RIPGREP_CONFIG_PATH environment variable.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/packing-unpacking-kwargs/index.html",
    "href": "blogs/packing-unpacking-kwargs/index.html",
    "title": "Packing and Unpacking **kwargs",
    "section": "",
    "text": "In python, we can unpack keyword arguments using the **kwargs in the function definition like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\n\n\nTurns out, that we can convert the unpacked dict into back into keyword arguments and pass it along to another function as well! This is done like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\n    meet(**kwargs)\n\ndef meet(a=None, b=None):\n    print(\"Nice to meet you again, {} & {}\".format(a, b))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\nNice to meet you again, Aru & Ura\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/python-context-file/index.html",
    "href": "blogs/python-context-file/index.html",
    "title": "Python context file",
    "section": "",
    "text": "Stolen from The Hitchhiker’s Guide to Python1, I use a context.py file to import python source code located in another directory. This is especially useful for importing source code into test files (which are typically located under the test/ directory for me) and for ipython notebooks (which are typically located under the notebooks directory for me).1 https://docs.python-guide.org/writing/structure/\nStick the following snippet in a context.py file in the directory where the source code is required to be imported.\n\n\ncontext.py\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport sample\n\nAnd in the file where the module need to be imported, stick the following.\n\n\nyour-python-file.py\n\nfrom .context import sample\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/dired-commands/index.html",
    "href": "blogs/dired-commands/index.html",
    "title": "Dired Commands",
    "section": "",
    "text": "% m\nmark the files matching the provided regex (also useful % d which marks the files for deletion)\n\n\n% g\nmark the files whose contents match the provided regex (essentially dired interface to grep)\n\n\nt\ntoggle the mark (I usually follow this with k to kill the lines, and g to restore)\n\n\n! or &\nrun shell command in current dir, if marked files are present pass them to the command as arguments\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/reflections-scientific-research/index.html",
    "href": "blogs/reflections-scientific-research/index.html",
    "title": "Reflections on Scientific Research",
    "section": "",
    "text": "I have been working as a researcher for several months now and have been working towards my first publication for the past few weeks. I have a few points of reflection on the scientific process and what constitutes as a good researcher.\n\nReading\nThis has been a challenge for me so far. I started my PhD with a literature review and during those initial 3 months I read a lot of papers. However once the analysis started, the reading dropped down to 0 papers per week. I am not sure if it is possible to keep a consistent reading practise, however reading at least 1 or 2 papers per week is ideal. Reading not only helps to be inspired and spark new ideas, but also makes for good content for the related work & discussion section of your paper. Moving forward, I want to review recent publications from the top journals of my field, and try to reflect upon what they did well, what I will do differently and identify interesting intersections between their topic and my research interests.\n\n\nWriting\nI started to use org-mode at the start of my PhD, capture and organize my thoughts and ideas. This worked out well because when the time came to start writing, I already had a pretty good outline for the paper ready. One thing that was still a challenge (at least for the first week of writing) is that the process itself was very slow. I don’t think this can be helped/improved. This is simply my process and all I can really do it work through it. Moving forward, I will try to plan out content for the report throughout the project.\n\n\nPaper Discovery\nThis is relevant for reading & writing. I think my current system works really well (see scientific paper discovery).\n\n\nExperimentation & Methodology\nThis could have been planned better. While writing the report I realised several loops & flaws in my data collection and methodology. My supervisor has recommended a book (see reference below) which I will read prior to our next project. Moving forward, I will also try to think about the dataset and its design earlier.\n\n\nGeneral Thoughts & Remarks\nI am curious to understand how my supervisor was able to see the potential in the idea we ended up pursuing versus the ones I proposed. Is this something that comes with experience? How can I spot a good (scientific research) idea?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aims/index.html",
    "href": "blogs/aims/index.html",
    "title": "Aru’s Information Management System (AIMS)",
    "section": "",
    "text": "AIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\nAlthough org-mode is great, the primary motivation for writing AIMs is because I was feeling a lot of resistance when trying to tune it to my workflow, primarily because of Elisp. Org-mode also requires that you use Emacs as your text editor. I did not appreciate the “vendor lock-in” enforced by org-mode.\nYou can find the latest version of the script on my dotfiles repo, below is the script as it stands on 2022-02-28.\n\n\naims\n\n#!/usr/bin/env bash\n\n1NOTESDIR=\"$HOME/org\"\nINBOX=\"$NOTESDIR/inbox.md\"\nTEMPLATESDIR=\"$XDG_DATA_HOME/aims\"\n[[ ! -e \"$INBOX\" ]] && touch \"$INBOX\"\n\n__capture() {\n  # Capture incoming info quickly. All items are appended to INBOX\n  # which defaults to `inbox.md' in NOTESDIR. Optionally a template\n  # can be specified using the --template| -t flag.\n2  local TEMPLATE=\"$TEMPLATESDIR/default\"\n\n  while [[ \"$1\" =~ ^-+.* && ! \"$1\" == \"--\" ]]; do\n    case \"$1\" in\n      --template | -t)\n        shift\n        TEMPLATE=\"$TEMPLATESDIR/$1\"\n        ;;\n      *)\n        echo \"Error: unknown option $1.\"\n        return 1\n        ;;\n    esac; shift\n  done\n\n3  local ITEM=$(mktemp)\n  if [[ -e \"$TEMPLATE\" && -x \"$TEMPLATE\" ]]; then\n    eval \"$TEMPLATE $ITEM\"\n  fi\n\n4  if eval \"$EDITOR -c 'set ft=markdown' $ITEM\"; then\n    [[ \"$1\" && -e \"$NOTESDIR/$1\" ]] && INBOX=\"$NOTESDIR/$1\"\n    cat \"$ITEM\" &gt;&gt; \"$INBOX\"\n    echo \"Info: captured in $INBOX.\"\n  fi\n\n5  echo \"Info: cleaning up $(rm -v \"$ITEM\")\"\n}\n\n__capture \"$@\"\n\n\n1\n\nStore all notes in $HOME/org/inbox.md, creating it if necessary. Also look for template scripts in ~/.local/share/aims.\n\n2\n\nParse the flags passed to AIMS. Currently it only supports the --template/-t flag which accepts the name of the template to use. Use the default template if none is provided. More on this later.\n\n3\n\nCreate a temporary file and insert the contents of the template.\n\n4\n\nEdit the temporary file using $EDITOR (here I assume its vim or neovim), setting the filetype to markdown. If the first positional argument passed to AIMS is a valid file inside $NOTESDIR then set that to the $INBOX file. Finally, prepend the contents of the temporary file to $INBOX file, if vim does not report an error.\n\n5\n\nCleanup, remove the temporary file.\n\n\nFor the time being, it only provides the capture functionality. A temporary file is used to compose the text first. Upon successful completion, the contents of the temporary file are appended to the default $INBOX file if no other files are specified.\nWhat I find really neat is the templating system. An arbitrary name for a template can be passed to aims using the --template (or -t for short) flag. aims looks for a shellscript with the same name in the ~/.local/share/aims directory and executes it if it exists. The beauty of this design is in its simplicity. Since templates are shellscripts, it gives us the full expressiveness of the shell. This is best demonstrated with some examples. Here is my default template as of 2022-02-28 which is used when no template is specified.\n\n\n~/.local/share/aims/default\n\n#!/usr/bin/env bash\n\n1[[ -z \"$1\" ]] && return 1\n\n2echo &gt;&gt; \"$1\"\necho \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\n\n\n1\n\nSanity check, ensure that a positional argument was passed (that is, the temporary file path).\n\n2\n\nInsert an empty line and a level 1 markdown header with a time stamp.\n\n\nIt simply adds a level 1 markdown header followed by a timestamp. Here is another for capturing bibtex information for research papers.\n\n\n\n\n\n\nTip\n\n\n\nI also wrote aocp.el, an emacs package to capture bibtex information of research papers using org-mode.\n\n\n#!/usr/bin/env bash\n\n[[ -z \"$1\" ]] && return 1\n\necho &gt;&gt; \"$1\"\n\n1BIBKEY=$(pbpaste | grep '^@.*' | sed 's/^@.*{\\(.*\\),/\\1/')\nif [[ -n \"$BIBKEY\" ]]; then\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')] $BIBKEY\" &gt;&gt; $1\nelse\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\nfi\n\n2echo &gt;&gt; \"$1\"\necho '+ **Problem Statement:**' &gt;&gt; \"$1\"\necho '+ **Solution**' &gt;&gt; \"$1\"\necho '+ **Results**' &gt;&gt; \"$1\"\necho '+ **Limitations**' &gt;&gt; \"$1\"\necho '+ **Remarks**' &gt;&gt; \"$1\"\necho &gt;&gt; \"$1\"\n3echo '```bibtex' &gt;&gt; \"$1\"\n\nif [[ -n \"$BIBKEY\" ]]; then\n  pbpaste | sed '/^$/d' &gt;&gt; \"$1\"\n  pbcopy &lt;(echo \"$BIBKEY\")\nfi\n\necho '```' &gt;&gt; \"$1\"\n\n1\n\nCheck that the bibtex information is currently in the system clipboard by attempting to extract the key using grep and sed. If a key was succussfully extracted, then create a level 1 markdown header with a time stamp and the key. Otherwise, fall back to just a time stamp.\n\n2\n\nAdd my prompts for note-taking when reading scientific papers.\n\n3\n\nRemove empty lines and put the bibtex information in a markdown source block.\n\n\nThis one is a bit more involved but highlights the power of using shellscripts for templating. Given that a bibentry is copied in the clipboard, this template adds a level 1 markdown header with a timestamp and the bibkey. It adds my note-taking prompts and sticks the bibentry at the bottom.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/today/index.html",
    "href": "blogs/today/index.html",
    "title": "Timestamps in the Shell (today)",
    "section": "",
    "text": "I often work with text files containing pros (such as blog posts and git commit messages) and require adding a timestamp containing the current date, day & time.\nI wrote today, a shellscript which returns the current date in various formats. Here is the script as of 2022-03-09, the latest version can be found in my dotfiles.\n\n\ntoday\n\n#!/usr/bin/env bash\n\n# today: return today's date in various formats\n\n# Usage: today [OPTS]\n# Without any options, today will print today's date in %Y-%m-%d\n# format. Following are the supported options:\n#    --with-day: %Y-%m-%d %a\n#    --with-time: %Y-%m-%d %H:%M\n#    -l | --long: %Y-%m-%d %a %H:%M\n#    -h | --human: %a %b %d, %Y\n#    -s | --stamp: enclose the date in square braces\n\n# NOTE: when using both --with-day & --with-time, the order in which\n# the options are passed matters. For example:\n#\n# today --with-day --with-time will produce\n#    2022-03-03 Thu 02:20\n#\n# But today --with-time --with-day will produce\n# 2022-03-03 02:21 Thu\n\n# NOTE: when using --human option, all other options are ignored.\n\nmain() {\n  local FMT='%Y-%m-%d'\n  local STAMP_FLAG=1 # false\n\n  while [[ \"$1\" =~ ^- && ! \"$1\" == \"--\" ]]; do\n    case \"$1\" in\n      --with-day)\n        FMT=\"$FMT %a\"\n        ;;\n      --with-time)\n        FMT=\"$FMT %H:%M\"\n        ;;\n      -s | --stamp)\n        STAMP_FLAG=0 # true\n        ;;\n      -l | --long) # short for --with-day --with-time\n        FMT=\"$FMT %a %H:%M\"\n        ;;\n      -h | --human) # alternate format, ignore other flags\n        FMT=\"%a %b %d, %Y\"\n        ;;\n      *)\n        echo \"Error: unknown option $1.\"\n        return 1\n    esac; shift # only shift here since we only pass flags\n  done\n\n  local OUT=$(date +\"$FMT\")\n\n  [[ \"$STAMP_FLAG\" -eq 0 ]] && OUT=\"[$OUT]\"\n\n  echo \"$OUT\"\n}\n\nmain \"$@\"\n\nWithout any arguments, today prints the date in ‘%Y-%m-%d’ format. Using the following optional flags the output can be manipulated.\n\n\n\nflag\noutput\n\n\n\n\n–with-day\n‘%Y-%m-%d %a’\n\n\n–with-time\n‘%Y-%m-%d %H:%M’\n\n\n–long\n‘%Y-%m-%d %a %H:%M’\n\n\n–human\n‘%a %b %d, %Y’\n\n\n\nThe --stamp flag can be used to optionally wrap the output in square braces.\nI frequently use this to add a timestamp to my git commit messages.\ngit commit -m \"feat: timestamps from the shell $(today -l -s)\"\nOr insert a timestamp into the current buffer I am editing in vim.\n:r! today -l -s\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/website-management-pandoc/index.html",
    "href": "blogs/website-management-pandoc/index.html",
    "title": "There and Back Again A Tale of Website Management",
    "section": "",
    "text": "After years of using orgmode along with the org-publish package to run my website, I came back to markdown, shell & vim.\nIn my humble shell dwelling days—before I began my journey into Emacs land—I was using Jekyll. Rather, I was fighting with it. Github requires a CNAME file in the directory from which the website should be served. Now, the github-pages gem can be used to instruct Github Pages (GHP) to automatically build and serve the website. But I faced several challenges getting the compatible versions of the github-pages, jekyll and ruby to match.\nI decided to forgo this madness and just use html & css to build my website. I used org-publish to accomplish this using the following setup in my init.el.\nSee the documentation for org-publish-project-alist on how to setup org-publish.\n\n\n~/.emacs.d/init.el\n\n(setq org-publish-project-alist\n '((\"org\" :components (\"org-posts\" \"org-static\"))\n   (\"website-posts\"\n    :base-directory \"~/code/arumoy\"\n    :base-extension \"org\"\n    :publishing-directory \"~/code/arumoy/docs/\"\n    :section-numbers nil\n    :auto-preamble t\n    :auto-sitemap t\n    :html-head \"&lt;link rel=\\\"stylesheet\\\" href=\\\"assets/css/main.css\\\" type=\\\"text/css\\\"/&gt;\"\n    :publishing-function org-html-publish-to-html)\n   (\"website-static\"\n    :base-directory \"~/code/arumoy/assets\"\n    :base-extension \"css\\\\|js\\\\|png\\\\|jpg\\\\|gif\\\\|pdf\\\\|mp3\\\\|ogg\\\\|swf\"\n    :publishing-directory \"~/code/arumoy/docs/assets/\"\n    :recursive t\n    :publishing-function org-publish-attachment)\n   (\"website-cname\"\n    :base-directory \"~/code/arumoy/\"\n    :base-extension \"\"\n    :publishing-directory \"~/code/arumoy/docs/\"\n    :include (\"CNAME\")\n    :publishing-function org-publish-attachment)\n   (\"website\" :components (\"website-posts\" \"website-static\" \"website-cname\"))))\n\nSince org-publish wipes the :publishing-directory clean prior to each build, I copy the CNAME file back in there.\nI was very pleased with its simplicity and its text-centric nature. The fact that it just worked out of the box was a pleasant surprise. However this intricate setup only worked in Emacs and this did not sit well with me. So I decided to find a more universal solution and landed on Pandoc.\nPandoc has the --standalone flag which produces a document which is valid on its own (think HTML documents with header and footer). One can write custom templates to produce documents styled to their liking. The default template can be viewed using pandoc -R FORMAT. A custom template can be specified using the --template flag. See section on templates in the pandoc manual for more info.\nFollowing the advice laid out by https://jgthms.com/web-design-in-4-minutes/, I designed a minimal pandoc custom template which you can find my in dotfiles repo.\nMy current workflow comprises of authoring content in markdown which I edit in vim. I use GNU make to automate the html generation using pandoc. The contents of my Makefile are as follows.\n\n\nMakefile\n\n# Taken from &lt;https://gist.github.com/kristopherjohnson/7466917&gt;\n\nSRCFILES:= $(wildcard *.md)\nPUBFILES=$(SRCFILES:.md=.html)\n\n%.html: %.md\n    pandoc --template=public -o docs/$@ $&lt;\n\n# Targets and dependencies\n\n.PHONY: all clean\n\nall : $(PUBFILES)\n\nclean:\n    rm $(PUBFILES)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/effortless-parallel-execution-xargs/index.html",
    "href": "blogs/effortless-parallel-execution-xargs/index.html",
    "title": "Effortless Parallel Execution with xargs & Friends",
    "section": "",
    "text": "Recently, I had to run Tensorflow Data Validation on over 500 public datasets from Kaggle to generate a baseline schema file for further analysis. I chose to do this using the xargs unix command.\nFollowing is a python script which generates the schema file and saves it to disk for a single csv dataset.\n\n\ncsv2schema.py\n\n#!/usr/bin/env python\n\nimport os\nimport sys\nimport tensorflow_data_validation as tfdv\nimport pandas as pd\n\n_CWD = os.path.dirname(__file__)\nDATADIR = os.path.abspath(os.path.join(_CWD, '..', 'data'))\nSTATSDIR = os.path.join(DATADIR, 'stats', 'train')\nSCHEMADIR = os.path.join(DATADIR, 'schema')\n\nname, _ = os.path.basename(sys.argv[1]).split('.')\n\nif os.path.isfile(os.path.join(SCHEMADIR, name+'.proto')):\n    print(name+'.proto', 'already exists, skipping...')\nelse:\n    frame = pd.read_csv(os.path.join(DATADIR, 'train', name+'.csv'))\n    stats = tfdv.generate_statistics_from_dataframe(frame)\n    schema = tfdv.infer_schema(stats)\n    tfdv.write_stats_text(stats, os.path.join(STATSDIR, name+'.proto'))\n    tfdv.write_schema_text(schema, os.path.join(SCHEMADIR, name+'.proto'))\n\nThe script accepts as argument a valid csv file (we assume that the file names are pruned and do not contain a period character within the name, but only to denote the extension). We read the file as a pandas dataframe, generate the statistics using tfdv.generate_statistics_from_dataframe function and infer a schema which is stored on disk for later analysis.\nFollowing is the bash shellscript wrapper which executes the python script presented above across several datasets using the find command. You may have to experiment with the -P flag which specifies the number of cores to distribute the execution across.\n\n\n\"csv2schema.bash\n\n#!/usr/bin/env bash\n\nmkdir -p data/{schema,stats/train}\n\nfind data/train -type f |\n    xargs -n 1 -P 4 ./bin/write-schema.py\n\nThat’s all there is to it! Write your main script with one file in mind, and distribute across several files using a combination of find and xargs.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/cms-pandoc/index.html",
    "href": "blogs/cms-pandoc/index.html",
    "title": "CMS using Pandoc and Friends",
    "section": "",
    "text": "Migrated to Quarto\n\n\n\nSince 2023-07-01, I have been using Quarto to manage my website.\nThe CMS system presented here works well. However, I felt the need for features such as code annotations and custom document layouts (to name a few) while still authoring content in plaintext. Quarto provides all this functionality (and more) without me having to dig around in pandoc’s documentation or write custom javascript.\nMy original website which I managed using the CMS system presented here, is open-sourced and can be viewed on Github.\n\n\nIn a prior post, I shared my humble system for running a static website using pandoc. Since that post, I have replaced several manual steps in the process with automated bash scripts.\n\nCreating and naming new posts\nI use the following human and machine readable naming convention for all my posts.\nYYYY-MM-DD--&lt;category&gt;--&lt;title&gt;\nWithin the post, I use yaml metadata to record additional information related to the post such as its title, date, author and a short abstract.\n\n\nmy-new-blog.md\n\n---\ntitle: foo bar baz\nauthor: John Doe\ndate: 2023-09-09\nabstract: |\n    This is the abstract for this post. This abstract shows up on the\n    index page automatically! Read on to learn how I do this.\n---\n\nAlthough the naming convention is clear, writing it is a bit cumbersome. Note that I also need to write the same information twice—once within the file in the yaml metadata, and again when naming the file. To reduce chances of human error, and make my life a bit easier, I automate the process of creating a new post using the following python script.\n\n\nbin/new\n\n#!/usr/bin/env python3\n\nimport os\nimport subprocess\nimport sys\nimport argparse\nfrom datetime import datetime\n\nEXT = \".md\"\nTIMESTAMP = datetime.now()\nTIMESTAMP = TIMESTAMP.__format__(\"%Y-%m-%d %a %H:%M\")\nTODAY = datetime.now()\nTODAY = TODAY.__format__(\"%Y-%m-%d\")\n\nparser = argparse.ArgumentParser()\n1parser.add_argument(\n    \"title\",\n    help=\"Title of new content\",\n)\n2parser.add_argument(\n    \"-t\",\n    \"--type\",\n    help=\"Type of content\",\n    choices=[\n        \"blog\",\n        \"talk\",\n    ],\n)\n3parser.add_argument(\n    \"-x\",\n    \"--noedit\",\n    help=\"Do not open new file in EDITOR\",\n    action=\"store_true\",\n)\n4parser.add_argument(\n    \"-f\",\n    \"--force\",\n    help=\"Do not ask for confirmation\",\n    action=\"store_true\",\n)\nargs = parser.parse_args()\n\nif args.type:\n    TYPE = args.type\nelse:\n    TYPE = \"blog\"\n\nTITLE = args.title.strip().lower().replace(\" \", \"-\")\nNAME = \"--\".join([TODAY, TYPE, TITLE])\nFILE = f\"_{TYPE}s/{NAME}{EXT}\"\n\nFRONTMATTER = [\n    \"---\",\n    \"\\n\",\n    f\"title: {TITLE}\",\n    \"\\n\",\n    f\"date: {TIMESTAMP}\",\n    \"\\n\",\n    f\"filename: {NAME}\",\n    \"\\n\",\n    \"author: Arumoy Shome\",\n    \"\\n\",\n    \"abstract: |\",\n    \"\\n\",\n    \"---\",\n]\n\nif not args.force:\n    confirm = input(f\"Create {FILE}? [y]es/[n]o: \")\n\n    if confirm.lower()[0] == \"n\":\n        sys.exit(\"Terminated by user\")\n\ntry:\n    with open(f\"{FILE}\", \"x\") as f:\n        f.writelines(FRONTMATTER)\nexcept FileExistsError:\n    sys.exit(f\"{FILE} already exists\")\n\nif not args.noedit:\n    subprocess.run([os.getenv(\"EDITOR\"), f\"{FILE}\"])\n\nsys.exit(f\"{FILE} created\")\n\n\n1\n\nAccept the title of the new post as the first positional argument. This argument is mandatory.\n\n2\n\nOptionally specify a type of post.\n\n3\n\nIf this flag is passed, don’t open the new file in $EDITOR.\n\n4\n\nIf this flag is passed, don’t ask for confirmation.\n\n\n\n\n\n\n\n\nPython argparse\n\n\n\nThe Python argparse module provides a convenient API to create commandline tools. This code is much more legible and understandable compared to how we parse arguments in say bash or zsh.\nFor instance, compare this to the argument parsing code I wrote in AIMS, my information management script.\n\n\nThe script has a title positional argument which is mandatory. Additionally, the script can also accept a type of the post using the --type or -t flag. With the --force or -f flag, the script does not ask for any confirmation when creating files. By default, the script will open the newly created post using the default editor. However, this can be bypassed by passing the --noedit or -x flag. The script automatically creates the yaml frontmatter for the post and names it in the specified format.\n\n\nAutomatically generating index pages\nI have two index pages on my website—the blogs page which list all the blogposts I have written and the talks page which lists all the talks I have given in the past. Previously, I was creating these pages manually. However, with a bit of unix shell scripting, I have now managed to do this automatically!\nI use the following script to generate the blogs and the talks index pages.\n\n\nbin/create-indices\n\n#!/usr/bin/env bash\n\n1# generate blogs.md\nTMP=$(mktemp)\n[[ -e blogs.md ]] && rm blogs.md\nfind _blogs -name '*.md' |\n  sort --reverse |\n  while read -r file; do\n    pandoc --template=_templates/index.md \"$file\" --to=markdown &gt;&gt;\"$TMP\"\n  done\n\ncat _templates/blogs-intro.md \"$TMP\" &gt;&gt;blogs.md\nrm \"$TMP\"\n\n2# generate talks.md\nTMP=$(mktemp)\n[[ -e talks.md ]] && rm talks.md\nfind _talks -name '*.md' |\n  sort --reverse |\n  while read -r file; do\n    pandoc --template=_templates/index.md \"$file\" --to=markdown &gt;&gt;\"$TMP\"\n  done\n\ncat _templates/talks-intro.md \"$TMP\" &gt;&gt;talks.md\nrm \"$TMP\"\n\n\n1\n\nSteps to generate blogs.md file. First clean slate by removing the file if it already exists. Find all markdown files in the _blogs directory, and run them through pandoc with a custom markdown template (explained in more details below). Append the entires in blogs.md in chronological order. Note as extra precaution, we use a temporary file to prevent accidental data loss.\n\n2\n\nSame as above, but create talks.md now.\n\n\nFirst we find all relevant markdown pages that we want to export to html using find. Next, we sort the results in chronological order such that the latest posts show up at the top of the page. The final part is the most interesting bit. We use pandoc’s templating system to extract the date, title and abstract of each file and generate an intermediate markdown file in the format that I want each post to show on the index page. Here is the template file that I use.\n\n\n_templates/index.md\n\n# ${date} ${title}\n$if(abstract)$\n\n${abstract}\n\n$endif$\n$if(filename)$\n[[html](${filename})]\n\n$endif$\n\nAll that is left to do is stitch everything together using cat to generate the final file.\n\n\nPutting everything together using make\nOnce the index pages are created, I use the following script to export all markdown files to html.\n\n\nbin/publish\n\n#!/usr/bin/env bash\n\nfind . -name \"*.md\" -not -path \"*_templates*\" |\n  while read -r file; do\n    pandoc --template=public -o docs/\"$(basename \"${file/%.md/.html}\")\" \"$file\"\n  done\n\nThe script finds all markdown files in the relevant directories, and converts them to html using pandoc. I use a custom template once again which includes some custom css and fonts of my choice.\nFinally, to automate the entire build process I use GNU make. I have a single all target which simply runs the create-indices and publish scripts in the right order.\n\n\nMakefile\n\nall:\n    bin/create-indices\n    bin/publish\n\n\n\nFurther optimisations\nThe create-indices script is currently sequential. You can imagine that this will keep getting slower as the number of posts increases. This step can be further optimised making the template extraction step parallel using xargs and then sorting the results.\nIn the publish script, we are converting all markdown files to html. Here, we can make the markdown file selection process smarter by using git ls-files. This will allow us to only select modified and untracked markdown files.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/navigating-large-markdown-files-vim/index.html",
    "href": "blogs/navigating-large-markdown-files-vim/index.html",
    "title": "Navigating large markdown files in vim",
    "section": "",
    "text": "Here are three strategies that I use to navigate large markdown files in vim.\n\nFolding\nRelatively new versions of vim already include the markdown runtime files package by the infamous tpope. Looking at the source code, we can see that we can enable folding in markdown files by adding the following configuration to our vimrc file.\n\n\n~/.vimrc\"\n\nlet g:markdown_folding = 1\n\nNow we can use the vim’s standard folding keybindings to collapse or open sections of a large markdown file. See :help folding in vim for more information.\n\n\nMovement by text-object\nThe plugin also includes a pair of handy normal mode keybindings to navigate between the sections of a markdown document.\n\n[[: go to previous section\n]]: go to next section\n\n\n\nNavigating using ctags\nUse the universal ctags program to generate a tags files for your markdown document.\nNote that most unix like operating systems already include a ctags executable but this does not support markdown files. There is also exuberent ctags which provides the same binary however also does not support markdown.\nYou can generate a tags files for your markdown document by running the following command in your terminal.\nctags &lt;name-of-your-file&gt;.md\nAlternatively, you can generate a tags file for your entire project recursively using the following command.\nctags -R .\nNow you can use vim’s built-in :tags command followed by the &lt;tab&gt; key to see a list of all the sections in your current markdown file.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/searching-in-vim/index.html",
    "href": "blogs/searching-in-vim/index.html",
    "title": "Searching in Vim",
    "section": "",
    "text": "In buffer search\nThere are several stratergies for searching in vim. For searching within the buffer the / and ? normal commands exist which take in a search pattern. The ignorecase, smartcase and infercase options affects the matching for the above normal commands. I additionally install the wincent/loupe plugin which enhances the buffer searchers.\n\n\nProject wide search\nFor searching in a project or across several files there are several options. First is the :vimgrep &lt;pattern&gt; &lt;files&gt; command which accepts vim regex for the search pattern and glob patterns for the files to search. The downside is that it is not recursive by default, although **/*.filetype does the trick (but needs to be specified every time).\nThe next option is the :grep command which invokes the grep command. Problem is that it does not ignore dotfiles or binaries by default. The following snippet uses the wildignore option to exclude files when calling grep (assuming wildignore is properly configured). Found on [[https://vi.stackexchange.com/a/8858][stackoverflow]].\n\n\n~/.vimrc\n\nlet &grepprg='grep -n -R --exclude=' . shellescape(&wildignore) . ' $*'\n\n\n\nCustomising the grep program\nThe grepprg and grepformat options allows us to specify a grep command and output format of out liking. I like to use ripgrep as an alternative to grep since it ignores dotfiles and binaries by default.\n\n\n~/.vimrc\n\nif executable('rg')\n    set grepprg=rg\\ --vimgrep\\ --no-heading\\ --smart-case\n    set grepformat=%f:%l:%c:%m,%f:%l:%m\nendif\n\nTiny downside which bugs me with the above two solutions is that every search takes away focus from the active splits and shows the search results in a temporary buffer before populating the quickfix list.\nFinally, the vim-fugitive plugin provides a :Ggrep &lt;pattern&gt; &lt;glob&gt; command which uses git-grep under the hood. The upside is that only files tracked by git are searched.\nPersonally, I like to keep external dependencies to a minimum. Since vim-fugitive is pretty much a must have, I prefer a combination of modifying the grepprg to ignore files and directories specified by wildignore and :Ggrep.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/navigating-in-vim/index.html",
    "href": "blogs/navigating-in-vim/index.html",
    "title": "Navigating in Vim",
    "section": "",
    "text": "The gf visits the file under the cursor if it exists in the path variable. I find this quite useful and use it constantly (for instance to navigate to imported files in python projects). There are several variants of this command:\n\n\n\ngF\nsame as gf but also navigate to the line\n\n\nC-w f\nsame as gf but open in a split window\n\n\nC-w F\nsame as C-w f with line number\n\n\nC-w gf\nsame as gf but open in new tab\n\n\nC-w gF\nsame as C-w gf with line number\n\n\n\nSince the line number variants fall back to their non line number counter parts, I remap them to the non line number variants.\n\n\n~/.vimrc\n\nnmap gf gF\nnmap &lt;C-w&gt;f &lt;C-w&gt;F\nnmap &lt;C-w&gt;&lt;C-f&gt; &lt;C-w&gt;F\nnmap &lt;C-w&gt;gf &lt;C-w&gt;gF\n\nVim provides the &lt;cname&gt; parameter which expands to the filename under the cursor. To make gf automatically create the file if it doesn’t exist, a mapping can be created: map fg :e &lt;cname&gt;. However, I prefer to keep this operation transparent and manual (so that I know what I am doing). Check :h gf for more info. The visual variant of gf uses the visual selection as the filename.\nVim also allows navigation by tags using C-[. This however requires the external ctags command and the tag generation is left to the user.\nFinally vim provides the :find command which searches for the given file in path and opens the first hit. The :sfind does the same but in a split window. Both commands accept a glob pattern which I extensively use to find what I need to edit without a fuzzy finder.\n\n\n\n Back to top"
  }
]