[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "All materials (excluding links to external websites and third party websites) on this website are open sourced under the Creative Commons Attribution 4.0 license. In brief, you are free to share and adapt all materials in any way you wish.\nThe above text was adapted from this website.\n\n\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Following is a list of my scientific publications.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Venue\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Authors\n        \n         \n          affiliation\n        \n         \n          doi\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nVenue\n\n\nDate\n\n\nAuthors\n\n\naffiliation\n\n\ndoi\n\n\n\n\n\n\nData vs. Model Machine Learning Fairness Testing: An Empirical Study (Extended Abstract)\n\n\nProceedings of 46th International Conference on Software Engineering (Companion Track)\n\n\nJan 22, 2024\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://doi.org/10.1145/3639478.3643121\n\n\n\n\nData vs. Model Machine Learning Fairness Testing: An Empirical Study\n\n\nProceedings of 5th International Workshop on Deep Learning for Testing and Testing for Deep Learning\n\n\nJan 12, 2024\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://doi.org/10.1145/3643786.3648022\n\n\n\n\nTowards Automatic Translation of Machine Learning Visual Insights to Analytical Assertions\n\n\nProceedings of 3nd International Workshop on NL-based Software Engineering\n\n\nJan 12, 2024\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://doi.org/10.1145/3643787.3648032\n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nProceedings of 2nd International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 15, 2023\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://doi.org/10.1109/CAIN58948.2023.00028\n\n\n\n\nData Smells in Public Datasets\n\n\nProceedings of 1st International Conference on AI Engineering: Software Engineering for AI\n\n\nMay 1, 2022\n\n\nArumoy Shome,Luis Cruz,Arie van Deursen\n\n\nDelft University of Technology\n\n\nhttps://doi.org/10.1145/3522664.3528621\n\n\n\n\nPrivacy Preserving Deep Learning for Medical Imaging\n\n\nMsc. Systematic Literature Review, unpublished\n\n\nDec 1, 2020\n\n\nArumoy Shome,Saba Amiri,Adam Belloum\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://arumoy.me/literature-study/ppdl.pdf\n\n\n\n\nKM3NeT Neutrino Detection using Deep Learning\n\n\nMsc. Thesis, unpublished\n\n\nOct 29, 2020\n\n\nArumoy Shome,Adam Belloum,Ben van Werkhoven,Ronald Bruijn\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://arumoy.me/km3net/km3net.pdf\n\n\n\n\nACE: Art, Color and Emotions\n\n\nProceedings of the 27th ACM Conference on Multimedia\n\n\nOct 1, 2019\n\n\nGjorgji Strezoski,Arumoy Shome,Riccardo Bianchi,Shruti Rao,Marcel Worring\n\n\nVrije Universiteit Amsterdam,Universiteit van Amsterdam\n\n\nhttps://dl.acm.org/doi/abs/10.1145/3343031.3350588\n\n\n\n\nImproving the Cognitive Assessment of Individuals with Down Syndrome\n\n\nBsc. Capstone Project, unpublished\n\n\nJan 1, 2018\n\n\nMaathusan Rajendram,Arumoy Shome,Mira Sleiman\n\n\nUniversity of Waterloo\n\n\nhttps://arumoy.me/elevate/report.pdf\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "talks/bridging-gap-between-visual-analytical-ml-testing/index.html",
    "href": "talks/bridging-gap-between-visual-analytical-ml-testing/index.html",
    "title": "Bridging the Gap Between Visual and Analytical ML Testing",
    "section": "",
    "text": "Introduction\nHello everyone and thank you for being here.\nI am Arumoy. I am a PhD Candidate at the Software Engineering Research Group at TU Delft. I have the privilege of working with excellent researchers such as Luis and Arie (who are somewhere in the audience). And after 2 years, I have found my calling.\nIn this talk, I am going to present the current vision I have for my PhD. I hope to generate some interesting discussions and get some feedback along the way.\n\n\nImplicit expectations to explicit tests\nHopefully, I don’t need to convience you that testing is important. We are software engineers, attending a software engineering conference afterall.\nThere is a wonderful paper by Zhang et al. (2020) that summarises the existing literature on ML testing. However, what has been ignored—until now—is the role of visualisations and how we use visual tests in the earlier, more “data-centric” stages of the ML lifecycle.\n\nZhang, Jie M, Mark Harman, Lei Ma, and Yang Liu. 2020. “Machine Learning Testing: Survey, Landscapes and Horizons.” IEEE Transactions on Software Engineering.\nVisualisations enable a rapid, exploratory form of analysis. Practitioners use “visual tests” to check for data properties. These visualisations tell a story. They are there for a reason. The expertise and domain knowledge is embedded within the visualisation.\nThis works really well when we are working on our laptop, on say an assignment. But visualisations do not scale well across organisation changes or when we want to move towards a large-scale production system. Visual tests tend to be left behind as latent expectations, rather than explicit failing tests. This research gap between moving from visual to analytical tests is where we wish to contribute.\nVisualisations become latent expectations rather than explicit tests. And this gap between going from latent visualisations to more analytical tests is exactly the research gap where we wish to contribute.\n\n\nThe hunt for data properties\nThe good news is that we have a rich source of data—jupyter notebooks. We are using a two-pronged approach. The first step—which I have been working on for the past month—is to collect these visualisations or data properties manually. We are exploring two sources of data, namely github and kaggle.\nOnce we have found a sufficient quantity of data properties, we will scale it to a larger subset. We are aware of two such datasets proposed by Quaranta, Calefato, and Lanubile (2021) and Pimentel et al. (2019) which contains notebooks from Kaggle and Github respectively.\n\nQuaranta, Luigi, Fabio Calefato, and Filippo Lanubile. 2021. “KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle.” In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), 550–54. IEEE.\n\nPimentel, João Felipe, Leonardo Murta, Vanessa Braganholo, and Juliana Freire. 2019. “A Large-Scale Study about Quality and Reproducibility of Jupyter Notebooks.” In 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR), 507–17. IEEE.\n\n\nNext steps and beyond\nOur immediate objective is to provide a format definition of “visual tests” along with examples of alternative analytical tests that can be used by the practitioners.\nOur ultimate research goal is to recommend such analytical tests automatically to the practitioner. Here it becomes a mining challenge which jupyter notebooks contain three sources of information: text, code and images.\nWe see several opportunities to collaborate with researchers working in other areas. Besides ML testing, I see implications in reproducibity and code quality of jupyter notebooks, explainable AI and HCI.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bib.html",
    "href": "bib.html",
    "title": "Annotated Bibliography",
    "section": "",
    "text": "Following is a list of papers that I frequently cite or found interesting to read during the course of my PhD."
  },
  {
    "objectID": "bib.html#vogelsang2019requirements",
    "href": "bib.html#vogelsang2019requirements",
    "title": "Annotated Bibliography",
    "section": "vogelsang2019requirements",
    "text": "vogelsang2019requirements\n@InProceedings{   vogelsang2019requirements,\n  doi           = {10.1109/rew.2019.00050},\n  url           = {https://doi.org/10.1109/rew.2019.00050},\n  year          = {2019},\n  month         = sep,\n  publisher     = {{IEEE}},\n  author        = {Andreas Vogelsang and Markus Borg},\n  title         = {Requirements Engineering for Machine Learning:\n                  Perspectives from Data Scientists},\n  booktitle     = {2019 {IEEE} 27th International Requirements Engineering\n                  Conference Workshops ({REW})}\n}"
  },
  {
    "objectID": "bib.html#breck2017ml",
    "href": "bib.html#breck2017ml",
    "title": "Annotated Bibliography",
    "section": "breck2017ml",
    "text": "breck2017ml\n@InProceedings{   breck2017ml,\n  title         = {The ML test score: A rubric for ML production readiness\n                  and technical debt reduction},\n  url           = {http://dx.doi.org/10.1109/BigData.2017.8258038},\n  doi           = {10.1109/bigdata.2017.8258038},\n  booktitle     = {2017 IEEE International Conference on Big Data (Big\n                  Data)},\n  publisher     = {IEEE},\n  author        = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib,\n                  Michael and Sculley, D.},\n  year          = {2017},\n  month         = dec\n}"
  },
  {
    "objectID": "bib.html#sculley2015hidden",
    "href": "bib.html#sculley2015hidden",
    "title": "Annotated Bibliography",
    "section": "sculley2015hidden",
    "text": "sculley2015hidden\n@InProceedings{   sculley2015hidden,\n  author        = {Sculley, D. and Holt, Gary and Golovin, Daniel and\n                  Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and\n                  Chaudhary, Vinay and Young, Michael and Crespo,\n                  Jean-Fran\\c{c}ois and Dennison, Dan},\n  booktitle     = {Advances in Neural Information Processing Systems},\n  editor        = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and\n                  R. Garnett},\n  pages         = {},\n  publisher     = {Curran Associates, Inc.},\n  title         = {Hidden Technical Debt in Machine Learning Systems},\n  url           = {https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf},\n  volume        = {28},\n  year          = {2015}\n}"
  },
  {
    "objectID": "bib.html#sambasivan2021everyone",
    "href": "bib.html#sambasivan2021everyone",
    "title": "Annotated Bibliography",
    "section": "sambasivan2021everyone",
    "text": "sambasivan2021everyone\n@InProceedings{   sambasivan2021everyone,\n  series        = {CHI ’21},\n  title         = {“Everyone wants to do the model work, not the data\n                  work”: Data Cascades in High-Stakes AI},\n  url           = {http://dx.doi.org/10.1145/3411764.3445518},\n  doi           = {10.1145/3411764.3445518},\n  booktitle     = {Proceedings of the 2021 CHI Conference on Human Factors in\n                  Computing Systems},\n  publisher     = {ACM},\n  author        = {Sambasivan, Nithya and Kapania, Shivani and Highfill,\n                  Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo,\n                  Lora M},\n  year          = {2021},\n  month         = may,\n  collection    = {CHI ’21}\n}"
  },
  {
    "objectID": "bib.html#martinez-plumed2021crisp-dm",
    "href": "bib.html#martinez-plumed2021crisp-dm",
    "title": "Annotated Bibliography",
    "section": "martinez-plumed2021crisp-dm",
    "text": "martinez-plumed2021crisp-dm\n@Article{         martinez-plumed2021crisp-dm,\n  title         = {CRISP-DM Twenty Years Later: From Data Mining Processes to\n                  Data Science Trajectories},\n  volume        = {33},\n  issn          = {2326-3865},\n  url           = {http://dx.doi.org/10.1109/TKDE.2019.2962680},\n  doi           = {10.1109/tkde.2019.2962680},\n  number        = {8},\n  journal       = {IEEE Transactions on Knowledge and Data Engineering},\n  publisher     = {Institute of Electrical and Electronics Engineers (IEEE)},\n  author        = {Martinez-Plumed, Fernando and Contreras-Ochando, Lidia and\n                  Ferri, Cesar and Hernandez-Orallo, Jose and Kull, Meelis\n                  and Lachiche, Nicolas and Ramirez-Quintana, Maria Jose and\n                  Flach, Peter},\n  year          = {2021},\n  month         = aug,\n  pages         = {3048–3061}\n}"
  },
  {
    "objectID": "bib.html#hutchinson2021towards",
    "href": "bib.html#hutchinson2021towards",
    "title": "Annotated Bibliography",
    "section": "hutchinson2021towards",
    "text": "hutchinson2021towards\n@InProceedings{   hutchinson2021towards,\n  series        = {FAccT ’21},\n  title         = {Towards Accountability for Machine Learning Datasets:\n                  Practices from Software Engineering and Infrastructure},\n  url           = {http://dx.doi.org/10.1145/3442188.3445918},\n  doi           = {10.1145/3442188.3445918},\n  booktitle     = {Proceedings of the 2021 ACM Conference on Fairness,\n                  Accountability, and Transparency},\n  publisher     = {ACM},\n  author        = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and\n                  Denton, Emily and Greer, Christina and Kjartansson, Oddur\n                  and Barnes, Parker and Mitchell, Margaret},\n  year          = {2021},\n  month         = mar,\n  collection    = {FAccT ’21}\n}"
  },
  {
    "objectID": "bib.html#haakman2021ai",
    "href": "bib.html#haakman2021ai",
    "title": "Annotated Bibliography",
    "section": "haakman2021ai",
    "text": "haakman2021ai\n@Article{         haakman2021ai,\n  title         = {AI lifecycle models need to be revised: An exploratory\n                  study in Fintech},\n  volume        = {26},\n  issn          = {1573-7616},\n  url           = {http://dx.doi.org/10.1007/s10664-021-09993-1},\n  doi           = {10.1007/s10664-021-09993-1},\n  number        = {5},\n  journal       = {Empirical Software Engineering},\n  publisher     = {Springer Science and Business Media LLC},\n  author        = {Haakman, Mark and Cruz, Luís and Huijgens, Hennie and van\n                  Deursen, Arie},\n  year          = {2021},\n  month         = jul\n}"
  },
  {
    "objectID": "bib.html#bosch2021engineering",
    "href": "bib.html#bosch2021engineering",
    "title": "Annotated Bibliography",
    "section": "bosch2021engineering",
    "text": "bosch2021engineering\n@InBook{          bosch2021engineering,\n  title         = {Engineering AI Systems: A Research Agenda},\n  issn          = {2327-3461},\n  url           = {http://dx.doi.org/10.4018/978-1-7998-5101-1.ch001},\n  doi           = {10.4018/978-1-7998-5101-1.ch001},\n  booktitle     = {Artificial Intelligence Paradigms for Smart Cyber-Physical\n                  Systems},\n  publisher     = {IGI Global},\n  author        = {Bosch, Jan and Olsson, Helena Holmström and Crnkovic,\n                  Ivica},\n  year          = {2021},\n  pages         = {1–19}\n}"
  },
  {
    "objectID": "bib.html#arpteg2018software",
    "href": "bib.html#arpteg2018software",
    "title": "Annotated Bibliography",
    "section": "arpteg2018software",
    "text": "arpteg2018software\n@InProceedings{   arpteg2018software,\n  title         = {Software Engineering Challenges of Deep Learning},\n  url           = {http://dx.doi.org/10.1109/SEAA.2018.00018},\n  doi           = {10.1109/seaa.2018.00018},\n  booktitle     = {2018 44th Euromicro Conference on Software Engineering and\n                  Advanced Applications (SEAA)},\n  publisher     = {IEEE},\n  author        = {Arpteg, Anders and Brinne, Bjorn and Crnkovic-Friis, Luka\n                  and Bosch, Jan},\n  year          = {2018},\n  month         = aug\n}"
  },
  {
    "objectID": "bib.html#amershi2019software",
    "href": "bib.html#amershi2019software",
    "title": "Annotated Bibliography",
    "section": "amershi2019software",
    "text": "amershi2019software\n@InProceedings{   amershi2019software,\n  doi           = {10.1109/icse-seip.2019.00042},\n  url           = {https://doi.org/10.1109/icse-seip.2019.00042},\n  year          = 2019,\n  month         = may,\n  publisher     = {{IEEE}},\n  author        = {Saleema Amershi and Andrew Begel and Christian Bird and\n                  Robert DeLine and Harald Gall and Ece Kamar and Nachiappan\n                  Nagappan and Besmira Nushi and Thomas Zimmermann},\n  title         = {Software Engineering for Machine Learning: A Case Study},\n  booktitle     = {2019 {IEEE}/{ACM} 41st International Conference on\n                  Software Engineering: Software Engineering in Practice\n                  ({ICSE}-{SEIP})}\n}"
  },
  {
    "objectID": "bib.html#liu2023pre-train",
    "href": "bib.html#liu2023pre-train",
    "title": "Annotated Bibliography",
    "section": "liu2023pre-train",
    "text": "liu2023pre-train\n@Article{         liu2023pre-train,\n  doi           = {10.1145/3560815},\n  url           = {https://doi.org/10.1145/3560815},\n  year          = {2023},\n  month         = jan,\n  publisher     = {Association for Computing Machinery ({ACM})},\n  volume        = {55},\n  number        = {9},\n  pages         = {1--35},\n  author        = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao\n                  Jiang and Hiroaki Hayashi and Graham Neubig},\n  title         = {Pre-train, Prompt, and Predict: A Systematic Survey of\n                  Prompting Methods in Natural Language Processing},\n  journal       = {{ACM} Computing Surveys}\n}"
  },
  {
    "objectID": "bib.html#quaranta2021kgtorrent",
    "href": "bib.html#quaranta2021kgtorrent",
    "title": "Annotated Bibliography",
    "section": "quaranta2021kgtorrent",
    "text": "quaranta2021kgtorrent\n@InProceedings{   quaranta2021kgtorrent,\n  title         = {KGTorrent: A Dataset of Python Jupyter Notebooks from\n                  Kaggle},\n  url           = {http://dx.doi.org/10.1109/MSR52588.2021.00072},\n  doi           = {10.1109/msr52588.2021.00072},\n  booktitle     = {2021 IEEE/ACM 18th International Conference on Mining\n                  Software Repositories (MSR)},\n  publisher     = {IEEE},\n  author        = {Quaranta, Luigi and Calefato, Fabio and Lanubile,\n                  Filippo},\n  year          = {2021},\n  month         = may\n}"
  },
  {
    "objectID": "bib.html#psallidas2019data",
    "href": "bib.html#psallidas2019data",
    "title": "Annotated Bibliography",
    "section": "psallidas2019data",
    "text": "psallidas2019data\n@Misc{            psallidas2019data,\n  title         = {Data Science through the looking glass and what we found\n                  there},\n  author        = {Fotis Psallidas and Yiwen Zhu and Bojan Karlas and Matteo\n                  Interlandi and Avrilia Floratou and Konstantinos Karanasos\n                  and Wentao Wu and Ce Zhang and Subru Krishnan and Carlo\n                  Curino and Markus Weimer},\n  year          = {2019},\n  eprint        = {1912.09536},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.LG}\n}"
  },
  {
    "objectID": "bib.html#pimentel2019large-scale",
    "href": "bib.html#pimentel2019large-scale",
    "title": "Annotated Bibliography",
    "section": "pimentel2019large-scale",
    "text": "pimentel2019large-scale\n@InProceedings{   pimentel2019large-scale,\n  title         = {A Large-Scale Study About Quality and Reproducibility of\n                  Jupyter Notebooks},\n  url           = {http://dx.doi.org/10.1109/MSR.2019.00077},\n  doi           = {10.1109/msr.2019.00077},\n  booktitle     = {2019 IEEE/ACM 16th International Conference on Mining\n                  Software Repositories (MSR)},\n  publisher     = {IEEE},\n  author        = {Pimentel, Joao Felipe and Murta, Leonardo and Braganholo,\n                  Vanessa and Freire, Juliana},\n  year          = {2019},\n  month         = may\n}"
  },
  {
    "objectID": "bib.html#bavishi2021vizsmith",
    "href": "bib.html#bavishi2021vizsmith",
    "title": "Annotated Bibliography",
    "section": "bavishi2021vizsmith",
    "text": "bavishi2021vizsmith\n@InProceedings{   bavishi2021vizsmith,\n  title         = {VizSmith: Automated Visualization Synthesis by Mining\n                  Data-Science Notebooks},\n  url           = {http://dx.doi.org/10.1109/ASE51524.2021.9678696},\n  doi           = {10.1109/ase51524.2021.9678696},\n  booktitle     = {2021 36th IEEE/ACM International Conference on Automated\n                  Software Engineering (ASE)},\n  publisher     = {IEEE},\n  author        = {Bavishi, Rohan and Laddad, Shadaj and Yoshida, Hiroaki and\n                  Prasad, Mukul R. and Sen, Koushik},\n  year          = {2021},\n  month         = nov\n}"
  },
  {
    "objectID": "bib.html#yang2021subtle",
    "href": "bib.html#yang2021subtle",
    "title": "Annotated Bibliography",
    "section": "yang2021subtle",
    "text": "yang2021subtle\n@InProceedings{   yang2021subtle,\n  title         = {Subtle Bugs Everywhere: Generating Documentation for Data\n                  Wrangling Code},\n  url           = {http://dx.doi.org/10.1109/ASE51524.2021.9678520},\n  doi           = {10.1109/ase51524.2021.9678520},\n  booktitle     = {2021 36th IEEE/ACM International Conference on Automated\n                  Software Engineering (ASE)},\n  publisher     = {IEEE},\n  author        = {Yang, Chenyang and Zhou, Shurui and Guo, Jin L.C. and\n                  Kastner, Christian},\n  year          = {2021},\n  month         = nov\n}"
  },
  {
    "objectID": "bib.html#wang2020assessing",
    "href": "bib.html#wang2020assessing",
    "title": "Annotated Bibliography",
    "section": "wang2020assessing",
    "text": "wang2020assessing\n@InProceedings{   wang2020assessing,\n  series        = {ASE ’20},\n  title         = {Assessing and restoring reproducibility of Jupyter\n                  notebooks},\n  url           = {http://dx.doi.org/10.1145/3324884.3416585},\n  doi           = {10.1145/3324884.3416585},\n  booktitle     = {Proceedings of the 35th IEEE/ACM International Conference\n                  on Automated Software Engineering},\n  publisher     = {ACM},\n  author        = {Wang, Jiawei and Kuo, Tzu-yang and Li, Li and Zeller,\n                  Andreas},\n  year          = {2020},\n  month         = dec,\n  collection    = {ASE ’20}\n}"
  },
  {
    "objectID": "bib.html#rule2018exploration",
    "href": "bib.html#rule2018exploration",
    "title": "Annotated Bibliography",
    "section": "rule2018exploration",
    "text": "rule2018exploration\n@InProceedings{   rule2018exploration,\n  series        = {CHI ’18},\n  title         = {Exploration and Explanation in Computational Notebooks},\n  url           = {http://dx.doi.org/10.1145/3173574.3173606},\n  doi           = {10.1145/3173574.3173606},\n  booktitle     = {Proceedings of the 2018 CHI Conference on Human Factors in\n                  Computing Systems},\n  publisher     = {ACM},\n  author        = {Rule, Adam and Tabard, Aurélien and Hollan, James D.},\n  year          = {2018},\n  month         = apr,\n  collection    = {CHI ’18}\n}"
  },
  {
    "objectID": "bib.html#kery2018story",
    "href": "bib.html#kery2018story",
    "title": "Annotated Bibliography",
    "section": "kery2018story",
    "text": "kery2018story\n@InProceedings{   kery2018story,\n  series        = {CHI ’18},\n  title         = {The Story in the Notebook: Exploratory Data Science using\n                  a Literate Programming Tool},\n  url           = {http://dx.doi.org/10.1145/3173574.3173748},\n  doi           = {10.1145/3173574.3173748},\n  booktitle     = {Proceedings of the 2018 CHI Conference on Human Factors in\n                  Computing Systems},\n  publisher     = {ACM},\n  author        = {Kery, Mary Beth and Radensky, Marissa and Arya, Mahima and\n                  John, Bonnie E. and Myers, Brad A.},\n  year          = {2018},\n  month         = apr,\n  collection    = {CHI ’18}\n}"
  },
  {
    "objectID": "bib.html#head2019managing",
    "href": "bib.html#head2019managing",
    "title": "Annotated Bibliography",
    "section": "head2019managing",
    "text": "head2019managing\n@InProceedings{   head2019managing,\n  series        = {CHI ’19},\n  title         = {Managing Messes in Computational Notebooks},\n  url           = {http://dx.doi.org/10.1145/3290605.3300500},\n  doi           = {10.1145/3290605.3300500},\n  booktitle     = {Proceedings of the 2019 CHI Conference on Human Factors in\n                  Computing Systems},\n  publisher     = {ACM},\n  author        = {Head, Andrew and Hohman, Fred and Barik, Titus and\n                  Drucker, Steven M. and DeLine, Robert},\n  year          = {2019},\n  month         = may,\n  collection    = {CHI ’19}\n}"
  },
  {
    "objectID": "bib.html#chattopadhyay2020what",
    "href": "bib.html#chattopadhyay2020what",
    "title": "Annotated Bibliography",
    "section": "chattopadhyay2020what",
    "text": "chattopadhyay2020what\n@InProceedings{   chattopadhyay2020what’s,\n  series        = {CHI ’20},\n  title         = {What’s Wrong with Computational Notebooks? Pain Points,\n                  Needs, and Design Opportunities},\n  url           = {http://dx.doi.org/10.1145/3313831.3376729},\n  doi           = {10.1145/3313831.3376729},\n  booktitle     = {Proceedings of the 2020 CHI Conference on Human Factors in\n                  Computing Systems},\n  publisher     = {ACM},\n  author        = {Chattopadhyay, Souti and Prasad, Ishita and Henley, Austin\n                  Z. and Sarma, Anita and Barik, Titus},\n  year          = {2020},\n  month         = apr,\n  collection    = {CHI ’20}\n}"
  },
  {
    "objectID": "bib.html#zeiler2014visualizing",
    "href": "bib.html#zeiler2014visualizing",
    "title": "Annotated Bibliography",
    "section": "zeiler2014visualizing",
    "text": "zeiler2014visualizing\nThis is a very nice paper, which I believe kicked-off the trend of visual analytics in Deep Learning? I have seen the visualisations shown in the paper before (probably during the DL course I took during Msc).\nThe visualisation techniques shown inspect the feature maps inside the model. I think this helps be align my work to the visualisations used before and after the model is trained. This is also in-line with our narrative of making decisions at a more holistic level, looking at the entire ML pipeline.\n@InBook{          zeiler2014visualizing,\n  title         = {Visualizing and Understanding Convolutional Networks},\n  isbn          = {9783319105901},\n  issn          = {1611-3349},\n  url           = {http://dx.doi.org/10.1007/978-3-319-10590-1_53},\n  doi           = {10.1007/978-3-319-10590-1_53},\n  booktitle     = {Lecture Notes in Computer Science},\n  publisher     = {Springer International Publishing},\n  author        = {Zeiler, Matthew D. and Fergus, Rob},\n  year          = {2014},\n  pages         = {818–833}\n}"
  },
  {
    "objectID": "bib.html#wexler2019what-if",
    "href": "bib.html#wexler2019what-if",
    "title": "Annotated Bibliography",
    "section": "wexler2019what-if",
    "text": "wexler2019what-if\n@Article{         wexler2019what-if,\n  title         = {The What-If Tool: Interactive Probing of Machine Learning\n                  Models},\n  issn          = {2160-9306},\n  url           = {http://dx.doi.org/10.1109/TVCG.2019.2934619},\n  doi           = {10.1109/tvcg.2019.2934619},\n  journal       = {IEEE Transactions on Visualization and Computer Graphics},\n  publisher     = {Institute of Electrical and Electronics Engineers (IEEE)},\n  author        = {Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga\n                  and Wattenberg, Martin and Viegas, Fernanda and Wilson,\n                  Jimbo},\n  year          = {2019},\n  pages         = {1–1}\n}"
  },
  {
    "objectID": "bib.html#kandel2012enterprise",
    "href": "bib.html#kandel2012enterprise",
    "title": "Annotated Bibliography",
    "section": "kandel2012enterprise",
    "text": "kandel2012enterprise\n@Article{         kandel2012enterprise,\n  title         = {Enterprise Data Analysis and Visualization: An Interview\n                  Study},\n  volume        = {18},\n  issn          = {1077-2626},\n  url           = {http://dx.doi.org/10.1109/TVCG.2012.219},\n  doi           = {10.1109/tvcg.2012.219},\n  number        = {12},\n  journal       = {IEEE Transactions on Visualization and Computer Graphics},\n  publisher     = {Institute of Electrical and Electronics Engineers (IEEE)},\n  author        = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph\n                  M. and Heer, Jeffrey},\n  year          = {2012},\n  month         = dec,\n  pages         = {2917–2926}\n}"
  },
  {
    "objectID": "bib.html#hohman2019visual",
    "href": "bib.html#hohman2019visual",
    "title": "Annotated Bibliography",
    "section": "hohman2019visual",
    "text": "hohman2019visual\n@Article{         hohman2019visual,\n  doi           = {10.1109/tvcg.2018.2843369},\n  url           = {https://doi.org/10.1109/tvcg.2018.2843369},\n  year          = {2019},\n  month         = aug,\n  publisher     = {Institute of Electrical and Electronics Engineers\n                  ({IEEE})},\n  volume        = {25},\n  number        = {8},\n  pages         = {2674--2693},\n  author        = {Fred Hohman and Minsuk Kahng and Robert Pienta and Duen\n                  Horng Chau },\n  title         = {Visual Analytics in Deep Learning: An Interrogative Survey\n                  for the Next Frontiers},\n  journal       = {{IEEE} Transactions on Visualization and Computer\n                  Graphics}\n}"
  },
  {
    "objectID": "bib.html#amershi2015modeltracker",
    "href": "bib.html#amershi2015modeltracker",
    "title": "Annotated Bibliography",
    "section": "amershi2015modeltracker",
    "text": "amershi2015modeltracker\n@InProceedings{   amershi2015modeltracker,\n  series        = {CHI ’15},\n  title         = {ModelTracker: Redesigning Performance Analysis Tools for\n                  Machine Learning},\n  url           = {http://dx.doi.org/10.1145/2702123.2702509},\n  doi           = {10.1145/2702123.2702509},\n  booktitle     = {Proceedings of the 33rd Annual ACM Conference on Human\n                  Factors in Computing Systems},\n  publisher     = {ACM},\n  author        = {Amershi, Saleema and Chickering, Max and Drucker, Steven\n                  M. and Lee, Bongshin and Simard, Patrice and Suh, Jina},\n  year          = {2015},\n  month         = apr,\n  collection    = {CHI ’15}\n}"
  },
  {
    "objectID": "bib.html#riccio2020testing",
    "href": "bib.html#riccio2020testing",
    "title": "Annotated Bibliography",
    "section": "riccio2020testing",
    "text": "riccio2020testing\n@Article{         riccio2020testing,\n  title         = {Testing machine learning based systems: a systematic\n                  mapping},\n  volume        = {25},\n  issn          = {1573-7616},\n  url           = {http://dx.doi.org/10.1007/s10664-020-09881-0},\n  doi           = {10.1007/s10664-020-09881-0},\n  number        = {6},\n  journal       = {Empirical Software Engineering},\n  publisher     = {Springer Science and Business Media LLC},\n  author        = {Riccio, Vincenzo and Jahangirova, Gunel and Stocco, Andrea\n                  and Humbatova, Nargiz and Weiss, Michael and Tonella,\n                  Paolo},\n  year          = {2020},\n  month         = sep,\n  pages         = {5193–5254}\n}"
  },
  {
    "objectID": "bib.html#xiao2021self-checking",
    "href": "bib.html#xiao2021self-checking",
    "title": "Annotated Bibliography",
    "section": "xiao2021self-checking",
    "text": "xiao2021self-checking\n@InProceedings{   xiao2021self-checking,\n  title         = {Self-Checking Deep Neural Networks in Deployment},\n  url           = {http://dx.doi.org/10.1109/ICSE43902.2021.00044},\n  doi           = {10.1109/icse43902.2021.00044},\n  booktitle     = {2021 IEEE/ACM 43rd International Conference on Software\n                  Engineering (ICSE)},\n  publisher     = {IEEE},\n  author        = {Xiao, Yan and Beschastnikh, Ivan and Rosenblum, David S.\n                  and Sun, Changsheng and Elbaum, Sebastian and Lin, Yun and\n                  Dong, Jin Song},\n  year          = {2021},\n  month         = may\n}"
  },
  {
    "objectID": "bib.html#pei2017deepxplore",
    "href": "bib.html#pei2017deepxplore",
    "title": "Annotated Bibliography",
    "section": "pei2017deepxplore",
    "text": "pei2017deepxplore\n@InProceedings{   pei2017deepxplore,\n  series        = {SOSP ’17},\n  title         = {DeepXplore: Automated Whitebox Testing of Deep Learning\n                  Systems},\n  url           = {http://dx.doi.org/10.1145/3132747.3132785},\n  doi           = {10.1145/3132747.3132785},\n  booktitle     = {Proceedings of the 26th Symposium on Operating Systems\n                  Principles},\n  publisher     = {ACM},\n  author        = {Pei, Kexin and Cao, Yinzhi and Yang, Junfeng and Jana,\n                  Suman},\n  year          = {2017},\n  month         = oct,\n  collection    = {SOSP ’17}\n}"
  },
  {
    "objectID": "bib.html#zhang2021ignorance",
    "href": "bib.html#zhang2021ignorance",
    "title": "Annotated Bibliography",
    "section": "zhang2021ignorance",
    "text": "zhang2021ignorance\n@InProceedings{   zhang2021ignorance,\n  title         = {“Ignorance and Prejudice” in Software Fairness},\n  url           = {http://dx.doi.org/10.1109/ICSE43902.2021.00129},\n  doi           = {10.1109/icse43902.2021.00129},\n  booktitle     = {2021 IEEE/ACM 43rd International Conference on Software\n                  Engineering (ICSE)},\n  publisher     = {IEEE},\n  author        = {Zhang, Jie M. and Harman, Mark},\n  year          = {2021},\n  month         = may\n}"
  },
  {
    "objectID": "bib.html#mehrabi2021survey",
    "href": "bib.html#mehrabi2021survey",
    "title": "Annotated Bibliography",
    "section": "mehrabi2021survey",
    "text": "mehrabi2021survey\n@Article{         mehrabi2021survey,\n  title         = {A Survey on Bias and Fairness in Machine Learning},\n  volume        = {54},\n  issn          = {1557-7341},\n  url           = {http://dx.doi.org/10.1145/3457607},\n  doi           = {10.1145/3457607},\n  number        = {6},\n  journal       = {ACM Computing Surveys},\n  publisher     = {Association for Computing Machinery (ACM)},\n  author        = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta\n                  and Lerman, Kristina and Galstyan, Aram},\n  year          = {2021},\n  month         = jul,\n  pages         = {1–35}\n}"
  },
  {
    "objectID": "bib.html#chen2023fairness",
    "href": "bib.html#chen2023fairness",
    "title": "Annotated Bibliography",
    "section": "chen2023fairness",
    "text": "chen2023fairness\n@Misc{            chen2023fairness,\n  title         = {Fairness Testing: A Comprehensive Survey and Analysis of\n                  Trends},\n  author        = {Zhenpeng Chen and Jie M. Zhang and Max Hort and Federica\n                  Sarro and Mark Harman},\n  year          = {2023},\n  eprint        = {2207.10223},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.SE}\n}"
  },
  {
    "objectID": "bib.html#biswas2021fair",
    "href": "bib.html#biswas2021fair",
    "title": "Annotated Bibliography",
    "section": "biswas2021fair",
    "text": "biswas2021fair\n@InProceedings{   biswas2021fair,\n  series        = {ESEC/FSE ’21},\n  title         = {Fair preprocessing: towards understanding compositional\n                  fairness of data transformers in machine learning\n                  pipeline},\n  url           = {http://dx.doi.org/10.1145/3468264.3468536},\n  doi           = {10.1145/3468264.3468536},\n  booktitle     = {Proceedings of the 29th ACM Joint Meeting on European\n                  Software Engineering Conference and Symposium on the\n                  Foundations of Software Engineering},\n  publisher     = {ACM},\n  author        = {Biswas, Sumon and Rajan, Hridesh},\n  year          = {2021},\n  month         = aug,\n  collection    = {ESEC/FSE ’21}\n}"
  },
  {
    "objectID": "bib.html#biswas2020do",
    "href": "bib.html#biswas2020do",
    "title": "Annotated Bibliography",
    "section": "biswas2020do",
    "text": "biswas2020do\n@InProceedings{   biswas2020do,\n  series        = {ESEC/FSE ’20},\n  title         = {Do the machine learning models on a crowd sourced platform\n                  exhibit bias? an empirical study on model fairness},\n  url           = {http://dx.doi.org/10.1145/3368089.3409704},\n  doi           = {10.1145/3368089.3409704},\n  booktitle     = {Proceedings of the 28th ACM Joint Meeting on European\n                  Software Engineering Conference and Symposium on the\n                  Foundations of Software Engineering},\n  publisher     = {ACM},\n  author        = {Biswas, Sumon and Rajan, Hridesh},\n  year          = {2020},\n  month         = nov,\n  collection    = {ESEC/FSE ’20}\n}"
  },
  {
    "objectID": "bib.html#zhang2022machine",
    "href": "bib.html#zhang2022machine",
    "title": "Annotated Bibliography",
    "section": "zhang2022machine",
    "text": "zhang2022machine\n@Article{         zhang2022machine,\n  title         = {Machine Learning Testing: Survey, Landscapes and\n                  Horizons},\n  volume        = {48},\n  issn          = {2326-3881},\n  url           = {http://dx.doi.org/10.1109/TSE.2019.2962027},\n  doi           = {10.1109/tse.2019.2962027},\n  number        = {1},\n  journal       = {IEEE Transactions on Software Engineering},\n  publisher     = {Institute of Electrical and Electronics Engineers (IEEE)},\n  author        = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},\n  year          = {2022},\n  month         = jan,\n  pages         = {1–36}\n}"
  },
  {
    "objectID": "bib.html#schelter2018automating",
    "href": "bib.html#schelter2018automating",
    "title": "Annotated Bibliography",
    "section": "schelter2018automating",
    "text": "schelter2018automating\n@Article{         schelter2018automating,\n  title         = {Automating large-scale data quality verification},\n  volume        = {11},\n  issn          = {2150-8097},\n  url           = {http://dx.doi.org/10.14778/3229863.3229867},\n  doi           = {10.14778/3229863.3229867},\n  number        = {12},\n  journal       = {Proceedings of the VLDB Endowment},\n  publisher     = {Association for Computing Machinery (ACM)},\n  author        = {Schelter, Sebastian and Lange, Dustin and Schmidt, Philipp\n                  and Celikel, Meltem and Biessmann, Felix and Grafberger,\n                  Andreas},\n  year          = {2018},\n  month         = aug,\n  pages         = {1781–1794}\n}"
  },
  {
    "objectID": "bib.html#lwakatare2021on",
    "href": "bib.html#lwakatare2021on",
    "title": "Annotated Bibliography",
    "section": "lwakatare2021on",
    "text": "lwakatare2021on\n@InProceedings{   lwakatare2021on,\n  title         = {On the Experiences of Adopting Automated Data Validation\n                  in an Industrial Machine Learning Project},\n  url           = {http://dx.doi.org/10.1109/ICSE-SEIP52600.2021.00034},\n  doi           = {10.1109/icse-seip52600.2021.00034},\n  booktitle     = {2021 IEEE/ACM 43rd International Conference on Software\n                  Engineering: Software Engineering in Practice (ICSE-SEIP)},\n  publisher     = {IEEE},\n  author        = {Lwakatare, Lucy Ellen and Rånge, Ellinor and Crnkovic,\n                  Ivica and Bosch, Jan},\n  year          = {2021},\n  month         = may\n}"
  },
  {
    "objectID": "bib.html#biessmann2021automated",
    "href": "bib.html#biessmann2021automated",
    "title": "Annotated Bibliography",
    "section": "biessmann2021automated",
    "text": "biessmann2021automated\n@Article{         biessmann2021automated,\n  author        = {Felix Biessmann and Jacek Golebiowski and Tammo Rukat and\n                  Dustin Lange and Philipp Schmidt},\n  title         = {Automated data validation in machine learning systems},\n  year          = {2021},\n  url           = {https://www.amazon.science/publications/automated-data-validation-in-machine-learning-systems},\n  journal       = {IEEE Data Engineering Bulletin}\n}"
  },
  {
    "objectID": "bib.html#shaw2003writing",
    "href": "bib.html#shaw2003writing",
    "title": "Annotated Bibliography",
    "section": "shaw2003writing",
    "text": "shaw2003writing\n@InProceedings{   shaw2003writing,\n  title         = {Writing good software engineering research papers},\n  url           = {http://dx.doi.org/10.1109/ICSE.2003.1201262},\n  doi           = {10.1109/icse.2003.1201262},\n  booktitle     = {25th International Conference on Software Engineering,\n                  2003. Proceedings.},\n  publisher     = {IEEE},\n  author        = {Shaw, M.},\n  year          = {2003}\n}"
  },
  {
    "objectID": "bib.html#shaw2002what",
    "href": "bib.html#shaw2002what",
    "title": "Annotated Bibliography",
    "section": "shaw2002what",
    "text": "shaw2002what\n@Article{         shaw2002what,\n  title         = {What makes good research in Software Engineering?},\n  volume        = {4},\n  issn          = {1433-2779},\n  url           = {http://dx.doi.org/10.1007/s10009-002-0083-4},\n  doi           = {10.1007/s10009-002-0083-4},\n  number        = {1},\n  journal       = {International Journal on Software Tools for Technology\n                  Transfer},\n  publisher     = {Springer Science and Business Media LLC},\n  author        = {Shaw, Mary},\n  year          = {2002},\n  month         = oct,\n  pages         = {1–7}\n}"
  },
  {
    "objectID": "bib.html#wohlin2012experimentation",
    "href": "bib.html#wohlin2012experimentation",
    "title": "Annotated Bibliography",
    "section": "wohlin2012experimentation",
    "text": "wohlin2012experimentation\n@Book{            wohlin2012experimentation,\n  title         = {Experimentation in Software Engineering},\n  isbn          = {9783642290442},\n  url           = {http://dx.doi.org/10.1007/978-3-642-29044-2},\n  doi           = {10.1007/978-3-642-29044-2},\n  publisher     = {Springer Berlin Heidelberg},\n  author        = {Wohlin, Claes and Runeson, Per and Höst, Martin and\n                  Ohlsson, Magnus C. and Regnell, Björn and Wesslén,\n                  Anders},\n  year          = {2012}\n}"
  },
  {
    "objectID": "bib.html#rugg2004unwritten",
    "href": "bib.html#rugg2004unwritten",
    "title": "Annotated Bibliography",
    "section": "rugg2004unwritten",
    "text": "rugg2004unwritten\n@Book{            rugg2004unwritten,\n  title         = {The Unwritten rules of phd research},\n  url           = {https://postgrado.bio.uc.cl/wp-content/uploads/2014/11/Unwritten-Rules-of-PhD-Research.pdf},\n  publisher     = {Open University Press},\n  author        = {Rugg, Gordon and Petre, Marian},\n  year          = {2004}\n}"
  },
  {
    "objectID": "bib.html#baylor2017tfx",
    "href": "bib.html#baylor2017tfx",
    "title": "Annotated Bibliography",
    "section": "baylor2017tfx",
    "text": "baylor2017tfx\n@InProceedings{   baylor2017tfx,\n  doi           = {10.1145/3097983.3098021},\n  url           = {https://doi.org/10.1145/3097983.3098021},\n  year          = {2017},\n  month         = aug,\n  publisher     = {{ACM}},\n  author        = {Denis Baylor and Eric Breck and Heng-Tze Cheng and Noah\n                  Fiedel and Chuan Yu Foo and Zakaria Haque and Salem Haykal\n                  and Mustafa Ispir and Vihan Jain and Levent Koc and Chiu\n                  Yuen Koo and Lukasz Lew and Clemens Mewald and Akshay\n                  Naresh Modi and Neoklis Polyzotis and Sukriti Ramesh and\n                  Sudip Roy and Steven Euijong Whang and Martin Wicke and\n                  Jarek Wilkiewicz and Xin Zhang and Martin Zinkevich},\n  title         = {{TFX}},\n  booktitle     = {Proceedings of the 23rd {ACM} {SIGKDD} International\n                  Conference on Knowledge Discovery and Data Mining}\n}"
  },
  {
    "objectID": "bib.html#virtanen2020scipy",
    "href": "bib.html#virtanen2020scipy",
    "title": "Annotated Bibliography",
    "section": "virtanen2020scipy",
    "text": "virtanen2020scipy\n@Article{         virtanen2020scipy,\n  author        = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E.\n                  and Haberland, Matt and Reddy, Tyler and Cournapeau, David\n                  and Burovski, Evgeni and Peterson, Pearu and Weckesser,\n                  Warren and Bright, Jonathan and {van der Walt}, St{\\'e}fan\n                  J. and Brett, Matthew and Wilson, Joshua and Millman, K.\n                  Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and\n                  Jones, Eric and Kern, Robert and Larson, Eric and Carey, C\n                  J and Polat, {\\.I}lhan and Feng, Yu and Moore, Eric W. and\n                  {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef\n                  and Cimrman, Robert and Henriksen, Ian and Quintero, E. A.\n                  and Harris, Charles R. and Archibald, Anne M. and Ribeiro,\n                  Ant{\\^o}nio H. and Pedregosa, Fabian and {van Mulbregt},\n                  Paul and {SciPy 1.0 Contributors}},\n  title         = {{{SciPy} 1.0: Fundamental Algorithms for Scientific\n                  Computing in Python}},\n  journal       = {Nature Methods},\n  year          = {2020},\n  volume        = {17},\n  pages         = {261--272},\n  adsurl        = {https://rdcu.be/b08Wh},\n  doi           = {10.1038/s41592-019-0686-2}\n}"
  },
  {
    "objectID": "bib.html#seabold2010statsmodels",
    "href": "bib.html#seabold2010statsmodels",
    "title": "Annotated Bibliography",
    "section": "seabold2010statsmodels",
    "text": "seabold2010statsmodels\n@InProceedings{   seabold2010statsmodels,\n  title         = {statsmodels: Econometric and statistical modeling with\n                  python},\n  author        = {Seabold, Skipper and Perktold, Josef},\n  booktitle     = {9th Python in Science Conference},\n  year          = {2010}\n}"
  },
  {
    "objectID": "bib.html#pedregosa2011scikit",
    "href": "bib.html#pedregosa2011scikit",
    "title": "Annotated Bibliography",
    "section": "pedregosa2011scikit",
    "text": "pedregosa2011scikit\n@Article{         pedregosa2011scikit,\n  title         = {Scikit-learn: Machine Learning in {P}ython},\n  author        = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and\n                  Michel, V. and Thirion, B. and Grisel, O. and Blondel, M.\n                  and Prettenhofer, P. and Weiss, R. and Dubourg, V. and\n                  Vanderplas, J. and Passos, A. and Cournapeau, D. and\n                  Brucher, M. and Perrot, M. and Duchesnay, E.},\n  journal       = {Journal of Machine Learning Research},\n  volume        = {12},\n  pages         = {2825--2830},\n  year          = {2011}\n}"
  },
  {
    "objectID": "blogs/docker-commands/index.html",
    "href": "blogs/docker-commands/index.html",
    "title": "Docker commands",
    "section": "",
    "text": "Building a docker image\ndocker build -t foobarbaz .\nBuild an image using the Dockerfile in the current working directory, and tag it as foobarbaz.\n\n\nRunning arbitrary commands inside a container\ndocker run --rm -it -v \"$(pwd):/app\" --cpus 8.0 --memory 320000000000 foobarbaz ./bin/my-script.bash\n\nRun bin/my-script.bash inside a container created from the foobarbaz image.\nRemove the container automatically after the script finishes (with the --rm flag).\nConnect the current tty with the container (with the -it flags).\nAnd connect the current working directory with the /app directory inside the container (with the -v flag). This assumes that the WORKDIR has been set to /app in the Dockerfile.\nRestrict the number of cpus to 8 cores and and 32GB memory used by the container (using the --cpus and --memory flags respectively).\n\n\n\nCleanup\ndocker system prune --all\nRemoves all: 1. stopped containers 2. networks not used by at least 1 container 3. dangling images 4. dangling build caches\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/bash-commands/index.html",
    "href": "blogs/bash-commands/index.html",
    "title": "Bash commands",
    "section": "",
    "text": "Best practices\nStart scripts with the following flags:\nset -euo pipefail\nset -x\n\nset -e: immediately exit if any command has a non-zero exit code\nset -u: immediately exit if any undefined variables are used\nset -o pipefail: immediately exit if any command in a pipeline fails\nset -x: print each command being run with its values\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/remla-data-validation/index.html",
    "href": "blogs/remla-data-validation/index.html",
    "title": "Data Validation with TFDV",
    "section": "",
    "text": "Note\n\n\n\nI gave this guest lecture once again in the 2023 iteration of the REMLA course.\n\n\nIn this lecture we will go over the basics of data validation. The first half of this lecture will be a talk on the fundamentals of data validation. We will answer what is data validation?, why should we validate our data? and how we can validate our data?. The second half of the lecture will be a hands-on tutorial on using Tensorflow Data Validation, instructions & code for which can be found on this github repo.\n\nWhat is data validation?\nI like to think of data validation in terms of expectations vs. reality. When working with data, we tend to have many implicit expectations from our data and data validation allows us to make such expectations explicit but defining validation rules.\nAnother (perhaps more technical) perspective would be that data validation is equivalent to data testing. This may include testing for presence of data, the data type of the columns (int, float or string) and statistical tests pertaining to the distribution of the feature.\n\n\nWhy should we validate our data?\nLets answer this question with an example. Lets assume we are working on a project which involves working with tabular data presented in Figure Figure 1. The dataset contains several numerical features such as the area & perimeter of the tumour and we want to train a model to predict whether the tumour is malignant or benign.\n\n\n\nFigure 1: Example dataset\n\n\nAnd lets say—being the ML experts that we are—we do some experimentation with various models and we manage to find one that fits the data well. We evaluate the model with a test set and achieve an acceptable value for the metric we are checking (accuracy, precision, recall or something else). Everybody is happy, you give yourself a pat on the back for a job well done, and call it a day.\nThis is a typical ML workflow which we tend to see in academia or in an educational setting. Turns out however, that the ML model related work is a single component of a much larger system as see in Figure Figure 2.\n\n\n\nFigure 2: ML Production Components\n\n\nContinuing along with the theme of data, lets dive deeper into the data collection stage. There may be several sources of data for the model. For instance, there may be a web service which is continually scrapping the internet for data, or we may have data stored in a database, a data warehouse or data lake.\nIn addition, we may have several systems with or without ML components which our system communicates with. For instance, in Figure Figure 3 our system may rely on data from another service. In return, other services may depend on the predictions from our system.\n\n\n\nFigure 3: ML Spaghetti\n\n\nMy point here is that ML pipelines are inherently complex and tangled. They consist of several stages and the ML model work tends to be a small part of a much larger system. A small change or bug in any of the stages ripples throughout the entire pipeline. Therefore, we cannot make implicit assumptions regarding the quality of the data.\nContinuing with the scenario of cancer detection, lets say that we have now managed to deploy our ML model in production. After a certain period of time (days, weeks or months) we may decide to re-train the model due to degrade in performance (perhaps the accuracy is lower than it used to be). The next batch of training data is typically generated by combining the unlabelled data in production with the predictions our live model is making as seen in Figure Figure 4.\n\n\n\nFigure 4: ML Production Training\n\n\nHowever, what happens if we no longer track the area_mean feature? Or what if we start tracking the numerical features in centimetres rather than millimetres? Or what if we use comma instead of periods to denote decimals?\nWith the exception of the last example, changes are that our pipeline continues to work albeit with a degraded performance. This is because we hold several implicit expectations from our data based on the training data which was used however the data in production may tell a completely different story. Thus it is important to make such expectations explicit by validating our data and catch data quality issues from feedback loops.\n\n\nHow should we validate our data?\nAlthough data validation has existing in the domain of database management systems for a very long time, its application in ML is new and still evolving. In this part of the talk I will present the theoretical principles on which tfdv operates.\nWe first generate a schema from the training data. A schema defines what we want our data to look like. For instance, for our cancer dataset, the schema may specify the columns we expect in the dataset, their data types and the distribution of each feature. Next, we gather statistics from the dataset we want to validate (this can be the local test set or the live data from production). Finally, we compare the statistics against the schema to make sure that they match. Figure Figure 5 puts a software engineering lens on how data validation works. The schema can be thought of as the test oracle and the live data is the current state. And we validate the two to ensure that there are no bugs in the live dataset.\nIt is important to realise that the schema generated by tfdv is best effort and the ML practitioner is still required to tweak the schema based on their understanding of the data & domain expertise.\n\n\n\nFigure 5: Data Validation How\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/cms-pandoc/index.html",
    "href": "blogs/cms-pandoc/index.html",
    "title": "CMS using Pandoc and Friends",
    "section": "",
    "text": "Migrated to Quarto\n\n\n\nSince 2023-07-01, I have been using Quarto to manage my website.\nThe CMS system presented here works well. However, I felt the need for features such as code annotations and custom document layouts (to name a few) while still authoring content in plaintext. Quarto provides all this functionality (and more) without me having to dig around in pandoc’s documentation or write custom javascript.\nMy original website which I managed using the CMS system presented here, is open-sourced and can be viewed on Github.\n\n\nIn a prior post, I shared my humble system for running a static website using pandoc. Since that post, I have replaced several manual steps in the process with automated bash scripts.\n\nCreating and naming new posts\nI use the following human and machine readable naming convention for all my posts.\nYYYY-MM-DD--&lt;category&gt;--&lt;title&gt;\nWithin the post, I use yaml metadata to record additional information related to the post such as its title, date, author and a short abstract.\n\n\nmy-new-blog.md\n\n---\ntitle: foo bar baz\nauthor: John Doe\ndate: 2023-09-09\nabstract: |\n    This is the abstract for this post. This abstract shows up on the\n    index page automatically! Read on to learn how I do this.\n---\n\nAlthough the naming convention is clear, writing it is a bit cumbersome. Note that I also need to write the same information twice—once within the file in the yaml metadata, and again when naming the file. To reduce chances of human error, and make my life a bit easier, I automate the process of creating a new post using the following python script.\n\n\nbin/new\n\n#!/usr/bin/env python3\n\nimport os\nimport subprocess\nimport sys\nimport argparse\nfrom datetime import datetime\n\nEXT = \".md\"\nTIMESTAMP = datetime.now()\nTIMESTAMP = TIMESTAMP.__format__(\"%Y-%m-%d %a %H:%M\")\nTODAY = datetime.now()\nTODAY = TODAY.__format__(\"%Y-%m-%d\")\n\nparser = argparse.ArgumentParser()\n1parser.add_argument(\n    \"title\",\n    help=\"Title of new content\",\n)\n2parser.add_argument(\n    \"-t\",\n    \"--type\",\n    help=\"Type of content\",\n    choices=[\n        \"blog\",\n        \"talk\",\n    ],\n)\n3parser.add_argument(\n    \"-x\",\n    \"--noedit\",\n    help=\"Do not open new file in EDITOR\",\n    action=\"store_true\",\n)\n4parser.add_argument(\n    \"-f\",\n    \"--force\",\n    help=\"Do not ask for confirmation\",\n    action=\"store_true\",\n)\nargs = parser.parse_args()\n\nif args.type:\n    TYPE = args.type\nelse:\n    TYPE = \"blog\"\n\nTITLE = args.title.strip().lower().replace(\" \", \"-\")\nNAME = \"--\".join([TODAY, TYPE, TITLE])\nFILE = f\"_{TYPE}s/{NAME}{EXT}\"\n\nFRONTMATTER = [\n    \"---\",\n    \"\\n\",\n    f\"title: {TITLE}\",\n    \"\\n\",\n    f\"date: {TIMESTAMP}\",\n    \"\\n\",\n    f\"filename: {NAME}\",\n    \"\\n\",\n    \"author: Arumoy Shome\",\n    \"\\n\",\n    \"abstract: |\",\n    \"\\n\",\n    \"---\",\n]\n\nif not args.force:\n    confirm = input(f\"Create {FILE}? [y]es/[n]o: \")\n\n    if confirm.lower()[0] == \"n\":\n        sys.exit(\"Terminated by user\")\n\ntry:\n    with open(f\"{FILE}\", \"x\") as f:\n        f.writelines(FRONTMATTER)\nexcept FileExistsError:\n    sys.exit(f\"{FILE} already exists\")\n\nif not args.noedit:\n    subprocess.run([os.getenv(\"EDITOR\"), f\"{FILE}\"])\n\nsys.exit(f\"{FILE} created\")\n\n\n1\n\nAccept the title of the new post as the first positional argument. This argument is mandatory.\n\n2\n\nOptionally specify a type of post.\n\n3\n\nIf this flag is passed, don’t open the new file in $EDITOR.\n\n4\n\nIf this flag is passed, don’t ask for confirmation.\n\n\n\n\n\n\n\n\nPython argparse\n\n\n\nThe Python argparse module provides a convenient API to create commandline tools. This code is much more legible and understandable compared to how we parse arguments in say bash or zsh.\nFor instance, compare this to the argument parsing code I wrote in AIMS, my information management script.\n\n\nThe script has a title positional argument which is mandatory. Additionally, the script can also accept a type of the post using the --type or -t flag. With the --force or -f flag, the script does not ask for any confirmation when creating files. By default, the script will open the newly created post using the default editor. However, this can be bypassed by passing the --noedit or -x flag. The script automatically creates the yaml frontmatter for the post and names it in the specified format.\n\n\nAutomatically generating index pages\nI have two index pages on my website—the blogs page which list all the blogposts I have written and the talks page which lists all the talks I have given in the past. Previously, I was creating these pages manually. However, with a bit of unix shell scripting, I have now managed to do this automatically!\nI use the following script to generate the blogs and the talks index pages.\n\n\nbin/create-indices\n\n#!/usr/bin/env bash\n\n1# generate blogs.md\nTMP=$(mktemp)\n[[ -e blogs.md ]] && rm blogs.md\nfind _blogs -name '*.md' |\n  sort --reverse |\n  while read -r file; do\n    pandoc --template=_templates/index.md \"$file\" --to=markdown &gt;&gt;\"$TMP\"\n  done\n\ncat _templates/blogs-intro.md \"$TMP\" &gt;&gt;blogs.md\nrm \"$TMP\"\n\n2# generate talks.md\nTMP=$(mktemp)\n[[ -e talks.md ]] && rm talks.md\nfind _talks -name '*.md' |\n  sort --reverse |\n  while read -r file; do\n    pandoc --template=_templates/index.md \"$file\" --to=markdown &gt;&gt;\"$TMP\"\n  done\n\ncat _templates/talks-intro.md \"$TMP\" &gt;&gt;talks.md\nrm \"$TMP\"\n\n\n1\n\nSteps to generate blogs.md file. First clean slate by removing the file if it already exists. Find all markdown files in the _blogs directory, and run them through pandoc with a custom markdown template (explained in more details below). Append the entires in blogs.md in chronological order. Note as extra precaution, we use a temporary file to prevent accidental data loss.\n\n2\n\nSame as above, but create talks.md now.\n\n\nFirst we find all relevant markdown pages that we want to export to html using find. Next, we sort the results in chronological order such that the latest posts show up at the top of the page. The final part is the most interesting bit. We use pandoc’s templating system to extract the date, title and abstract of each file and generate an intermediate markdown file in the format that I want each post to show on the index page. Here is the template file that I use.\n\n\n_templates/index.md\n\n# ${date} ${title}\n$if(abstract)$\n\n${abstract}\n\n$endif$\n$if(filename)$\n[[html](${filename})]\n\n$endif$\n\nAll that is left to do is stitch everything together using cat to generate the final file.\n\n\nPutting everything together using make\nOnce the index pages are created, I use the following script to export all markdown files to html.\n\n\nbin/publish\n\n#!/usr/bin/env bash\n\nfind . -name \"*.md\" -not -path \"*_templates*\" |\n  while read -r file; do\n    pandoc --template=public -o docs/\"$(basename \"${file/%.md/.html}\")\" \"$file\"\n  done\n\nThe script finds all markdown files in the relevant directories, and converts them to html using pandoc. I use a custom template once again which includes some custom css and fonts of my choice.\nFinally, to automate the entire build process I use GNU make. I have a single all target which simply runs the create-indices and publish scripts in the right order.\n\n\nMakefile\n\nall:\n    bin/create-indices\n    bin/publish\n\n\n\nFurther optimisations\nThe create-indices script is currently sequential. You can imagine that this will keep getting slower as the number of posts increases. This step can be further optimised making the template extraction step parallel using xargs and then sorting the results.\nIn the publish script, we are converting all markdown files to html. Here, we can make the markdown file selection process smarter by using git ls-files. This will allow us to only select modified and untracked markdown files.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/website-management-pandoc/index.html",
    "href": "blogs/website-management-pandoc/index.html",
    "title": "There and Back Again A Tale of Website Management",
    "section": "",
    "text": "After years of using orgmode along with the org-publish package to run my website, I came back to markdown, shell & vim.\nIn my humble shell dwelling days—before I began my journey into Emacs land—I was using Jekyll. Rather, I was fighting with it. Github requires a CNAME file in the directory from which the website should be served. Now, the github-pages gem can be used to instruct Github Pages (GHP) to automatically build and serve the website. But I faced several challenges getting the compatible versions of the github-pages, jekyll and ruby to match.\nI decided to forgo this madness and just use html & css to build my website. I used org-publish to accomplish this using the following setup in my init.el.\nSee the documentation for org-publish-project-alist on how to setup org-publish.\n\n\n~/.emacs.d/init.el\n\n(setq org-publish-project-alist\n '((\"org\" :components (\"org-posts\" \"org-static\"))\n   (\"website-posts\"\n    :base-directory \"~/code/arumoy\"\n    :base-extension \"org\"\n    :publishing-directory \"~/code/arumoy/docs/\"\n    :section-numbers nil\n    :auto-preamble t\n    :auto-sitemap t\n    :html-head \"&lt;link rel=\\\"stylesheet\\\" href=\\\"assets/css/main.css\\\" type=\\\"text/css\\\"/&gt;\"\n    :publishing-function org-html-publish-to-html)\n   (\"website-static\"\n    :base-directory \"~/code/arumoy/assets\"\n    :base-extension \"css\\\\|js\\\\|png\\\\|jpg\\\\|gif\\\\|pdf\\\\|mp3\\\\|ogg\\\\|swf\"\n    :publishing-directory \"~/code/arumoy/docs/assets/\"\n    :recursive t\n    :publishing-function org-publish-attachment)\n   (\"website-cname\"\n    :base-directory \"~/code/arumoy/\"\n    :base-extension \"\"\n    :publishing-directory \"~/code/arumoy/docs/\"\n    :include (\"CNAME\")\n    :publishing-function org-publish-attachment)\n   (\"website\" :components (\"website-posts\" \"website-static\" \"website-cname\"))))\n\nSince org-publish wipes the :publishing-directory clean prior to each build, I copy the CNAME file back in there.\nI was very pleased with its simplicity and its text-centric nature. The fact that it just worked out of the box was a pleasant surprise. However this intricate setup only worked in Emacs and this did not sit well with me. So I decided to find a more universal solution and landed on Pandoc.\nPandoc has the --standalone flag which produces a document which is valid on its own (think HTML documents with header and footer). One can write custom templates to produce documents styled to their liking. The default template can be viewed using pandoc -R FORMAT. A custom template can be specified using the --template flag. See section on templates in the pandoc manual for more info.\nFollowing the advice laid out by https://jgthms.com/web-design-in-4-minutes/, I designed a minimal pandoc custom template which you can find my in dotfiles repo.\nMy current workflow comprises of authoring content in markdown which I edit in vim. I use GNU make to automate the html generation using pandoc. The contents of my Makefile are as follows.\n\n\nMakefile\n\n# Taken from &lt;https://gist.github.com/kristopherjohnson/7466917&gt;\n\nSRCFILES:= $(wildcard *.md)\nPUBFILES=$(SRCFILES:.md=.html)\n\n%.html: %.md\n    pandoc --template=public -o docs/$@ $&lt;\n\n# Targets and dependencies\n\n.PHONY: all clean\n\nall : $(PUBFILES)\n\nclean:\n    rm $(PUBFILES)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/dired-commands/index.html",
    "href": "blogs/dired-commands/index.html",
    "title": "Dired Commands",
    "section": "",
    "text": "% m\nmark the files matching the provided regex (also useful % d which marks the files for deletion)\n\n\n% g\nmark the files whose contents match the provided regex (essentially dired interface to grep)\n\n\nt\ntoggle the mark (I usually follow this with k to kill the lines, and g to restore)\n\n\n! or &\nrun shell command in current dir, if marked files are present pass them to the command as arguments\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/searching-vim/index.html",
    "href": "blogs/searching-vim/index.html",
    "title": "Searching in Vim",
    "section": "",
    "text": "In buffer search\nThere are several stratergies for searching in vim. For searching within the buffer the / and ? normal commands exist which take in a search pattern. The ignorecase, smartcase and infercase options affects the matching for the above normal commands. I additionally install the wincent/loupe plugin which enhances the buffer searchers.\n\n\nProject wide search\nFor searching in a project or across several files there are several options. First is the :vimgrep &lt;pattern&gt; &lt;files&gt; command which accepts vim regex for the search pattern and glob patterns for the files to search. The downside is that it is not recursive by default, although **/*.filetype does the trick (but needs to be specified every time).\nThe next option is the :grep command which invokes the grep command. Problem is that it does not ignore dotfiles or binaries by default. The following snippet uses the wildignore option to exclude files when calling grep (assuming wildignore is properly configured). Found on [[https://vi.stackexchange.com/a/8858][stackoverflow]].\n\n\n~/.vimrc\n\nlet &grepprg='grep -n -R --exclude=' . shellescape(&wildignore) . ' $*'\n\n\n\nCustomising the grep program\nThe grepprg and grepformat options allows us to specify a grep command and output format of out liking. I like to use ripgrep as an alternative to grep since it ignores dotfiles and binaries by default.\n\n\n~/.vimrc\n\nif executable('rg')\n    set grepprg=rg\\ --vimgrep\\ --no-heading\\ --smart-case\n    set grepformat=%f:%l:%c:%m,%f:%l:%m\nendif\n\nTiny downside which bugs me with the above two solutions is that every search takes away focus from the active splits and shows the search results in a temporary buffer before populating the quickfix list.\nFinally, the vim-fugitive plugin provides a :Ggrep &lt;pattern&gt; &lt;glob&gt; command which uses git-grep under the hood. The upside is that only files tracked by git are searched.\nPersonally, I like to keep external dependencies to a minimum. Since vim-fugitive is pretty much a must have, I prefer a combination of modifying the grepprg to ignore files and directories specified by wildignore and :Ggrep.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/python-context-file/index.html",
    "href": "blogs/python-context-file/index.html",
    "title": "Python context file",
    "section": "",
    "text": "Stolen from The Hitchhiker’s Guide to Python1, I use a context.py file to import python source code located in another directory. This is especially useful for importing source code into test files (which are typically located under the test/ directory for me) and for ipython notebooks (which are typically located under the notebooks directory for me).1 https://docs.python-guide.org/writing/structure/\nStick the following snippet in a context.py file in the directory where the source code is required to be imported.\n\n\ncontext.py\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport sample\n\nAnd in the file where the module need to be imported, stick the following.\n\n\nyour-python-file.py\n\nfrom .context import sample\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/scientific-paper-discovery/index.html",
    "href": "blogs/scientific-paper-discovery/index.html",
    "title": "Scientific Paper Discovery",
    "section": "",
    "text": "In this article I describe my process for discovering relevant and important papers in a new scientific field. Since I am working towards a doctoral degree, I need to read a lot to develop a deep understanding of my field of research and stay current on the latest developments.\n\nWhere to Start\nThe starting point is always the most difficult. If possible, the best place to start is through recommendation from your peers/supervisor/advisor. If this is not possible, then coming up with a few good keywords from your research question is the next best approach. Reading a few non-scientific work (such as wikipedia) might be necessary to obtain the “buzz words” of technical jargon which are required to formulate the search queries.\nThe right way to familiarize oneself with a new field is through a systematic literature review. However, I find that to be too big a commitment for a new field of research (I may not stick with it if I don’t find it that interesting). So the smarter (and quicker way) to familiarize oneself with the literature is to read the related papers.\n\n\nFinding More Papers\nI categorise this search process into two parts: 1. Intrinsic and 2. Extrinsic.\nWe can find relevant papers from within the current paper we are reading. This can be done by first reading the related work section of the paper and second by reviewing the reference section of the paper. The related work section often leads to papers that are at a similar level of technical depth as the current paper. The papers in the reference section tend to be more general and provide a good high level overview of the field.\nWe can also find relevant papers from the search engine (Google Scholar in my case). The cited by feature of GS is a good place to find forward references or the papers which are citing the current paper. These tend to be more specific and bleeding edge as they came after the current paper. The related articles are also a good place to find similar bodies of work. Google Scholar and Connected Papers are good resources to explore this.\nUsually, 5-10 good, well reputed papers is sufficient to start with. And then, reading their related papers (and in turn their related papers, so on and so forth as necessary), should result in a collection of bibliography which paints a good picture of the field.\n\n\nStaying Current\nOne also needs to stay current on the field. For this, Google Scholar alerts are a good start. By now the “position papers” and important authors in the field should be known. Following these authors on Google Scholar and creating an alert for when a position paper gets cited is a good way to stay up to speed. Another approach is to keep an eye on the relevant conferences and find papers directly from there.\n\n\nConcluding Remarks and Acknowledgements\nI was surprised to find that not a lot of information on this matter is easily available. It is similar to how students are not taught how to learn effectively, before they start attending university classes. The emphasis on the “meta”, the methodology of doing something is lacking. I cannot take credit for the wisdom above, a lot of it came from conversations with my peers and Google Scholar help website.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/navigating-large-markdown-files-vim/index.html",
    "href": "blogs/navigating-large-markdown-files-vim/index.html",
    "title": "Navigating large markdown files in vim",
    "section": "",
    "text": "Here are three strategies that I use to navigate large markdown files in vim.\n\nFolding\nRelatively new versions of vim already include the markdown runtime files package by the infamous tpope. Looking at the source code, we can see that we can enable folding in markdown files by adding the following configuration to our vimrc file.\n\n\n~/.vimrc\"\n\nlet g:markdown_folding = 1\n\nNow we can use the vim’s standard folding keybindings to collapse or open sections of a large markdown file. See :help folding in vim for more information.\n\n\nMovement by text-object\nThe plugin also includes a pair of handy normal mode keybindings to navigate between the sections of a markdown document.\n\n[[: go to previous section\n]]: go to next section\n\n\n\nNavigating using ctags\nUse the universal ctags program to generate a tags files for your markdown document.\nNote that most unix like operating systems already include a ctags executable but this does not support markdown files. There is also exuberent ctags which provides the same binary however also does not support markdown.\nYou can generate a tags files for your markdown document by running the following command in your terminal.\nctags &lt;name-of-your-file&gt;.md\nAlternatively, you can generate a tags file for your entire project recursively using the following command.\nctags -R .\nNow you can use vim’s built-in :tags command followed by the &lt;tab&gt; key to see a list of all the sections in your current markdown file.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/packing-unpacking-kwargs/index.html",
    "href": "blogs/packing-unpacking-kwargs/index.html",
    "title": "Packing and Unpacking **kwargs",
    "section": "",
    "text": "In python, we can unpack keyword arguments using the **kwargs in the function definition like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\n\n\nTurns out, that we can convert the unpacked dict into back into keyword arguments and pass it along to another function as well! This is done like so.\n\ndef greet(**kwargs):\n    for _, v in kwargs.items():\n        print(\"Hello {}!\".format(v))\n\n    meet(**kwargs)\n\ndef meet(a=None, b=None):\n    print(\"Nice to meet you again, {} & {}\".format(a, b))\n\ngreet(a=\"Aru\", b=\"Ura\")\n\nHello Aru!\nHello Ura!\nNice to meet you again, Aru & Ura\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/ripgrep-commands/index.html",
    "href": "blogs/ripgrep-commands/index.html",
    "title": "Ripgrep commands",
    "section": "",
    "text": "--no-ignore\ndon’t ignore patterns in .gitignore file\n\n\n--hidden\ndon’t ignore hidden (dot) files\n\n\n--text/-a\ndon’t ignore binary files\n\n\n--follow\ndon’t ignore symlinks\n\n\n--fixed-string/-F\ntreat pattern as literal string (not regex)\n\n\n--type/-t\nlimit search scope to specific filetype\n\n\n--type-not/-T\ninverse of --type\n\n\n--type-list\nprint builtin types\n\n\n--glob/-g\ninclude manually specified glob patterns in search scope, use ! in glob pattern to inverse\n\n\n\nRipgrep follows the Rust regex syntax, more details can be found here but the usual stuff mostly apply as well.\nBy default, rg performs a recursive search in the current directory while respecting the glob patterns in the .gitignore file. It also ignores hidden (dot) files, binary files and symbolic links. The --no-ignore and --hidden flags can be used to change that. Binary files can be searched using the --text/-a flag and symlinks can be followed with --follow.\nTo ignore certain glob patterns in rg (but not in git) a =.ignore= file can be placed within the directory.\nAn example of an inverse glob pattern is shown below.\n  rg hello -g '!*.md' # don't search in md files\nA note on rg’s types: These are customisable and a hand full of them are built in. rg --type-list lists them. New types can be added using the --type-add flag. To persist the new type, create a shell alias or define a rg config file.\nAs previously mentioned, rg can be configured using a config file. The name does not matter since rg looks for the file using the RIPGREP_CONFIG_PATH environment variable.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/shell-path-finder/index.html",
    "href": "blogs/shell-path-finder/index.html",
    "title": "shell path_finder",
    "section": "",
    "text": "The /etc/profile file is executed each time a login shell is started. If we investigate this file, we see the following:\n\n\n/etc/profile\n\n# System-wide .profile for sh(1)\n\nif [ -x /usr/libexec/path_helper ]; then\n    eval `/usr/libexec/path_helper -s`\nfi\n\nif [ \"${BASH-no}\" != \"no\" ]; then\n    [ -r /etc/bashrc ] && . /etc/bashrc\nfi\n\nSo each time a login shell is started, path_helper is run. In short, it takes the paths listed in /etc/paths & etc/paths.d/, appends the existing PATH and clears the duplicates1.1 This Stack overflow post explains how path_helper operates in mode details.\nTmux2 always runs as a login shell. Thus a common problem within tmux is duplicate entries =PATH= (in a different order than what we specify in our shell config files).2 https://github.com/tmux/tmux/wiki\nThus, it’s okay to append to =PATH= in the shell config files, but prepending causes unwanted side-effects. We could edit =etc/paths= manually, but this requires sudo and is generally not advised.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/organise-research-project-git/index.html",
    "href": "blogs/organise-research-project-git/index.html",
    "title": "Organising research projects with git",
    "section": "",
    "text": "In this post I list some of the standards and conventions I have developed over the course of my PhD to organise research project data. I use git to version control the relevant files and host them on Github."
  },
  {
    "objectID": "blogs/organise-research-project-git/index.html#additionally-for-the-subject-line",
    "href": "blogs/organise-research-project-git/index.html#additionally-for-the-subject-line",
    "title": "Organising research projects with git",
    "section": "Additionally for the subject line",
    "text": "Additionally for the subject line\nFor instance, say you want to commit a first draft of your paper. Use a subject line as follows:\nfeat(report): init icse24 paper draft # &lt;1&gt;\nWhere feat is an abbreviation for “feature”. The braces specify the folder within which the changes were made, and the title provides a quick description of the change.\nIn contrast, say you refactor the data processing pipeline script. The subject line could be as follows:\nrefac(bin): remove magic numbers in data-process.sh\nThis makes it very easy to track down prior commits that introduced changes in the paper versus in the code. For instance, you can target all refactoring commits within the report/ directory using the following git grep command:\ngit log --grep 'refac.*report.'"
  },
  {
    "objectID": "blogs/research-workflow/index.html",
    "href": "blogs/research-workflow/index.html",
    "title": "Research Workflow",
    "section": "",
    "text": "Here I outline a workflow which I have developed to handle the non-linear nature of scientific research. There are days when one reads several papers without making much sense of them. However, a single paper the next day can result in a clear picture of the domain. The workflow acknowledges the fact that there are many more failures rather than successes in research. However, a failure is still an outcome—and this is what helps us decide what to do next—so it should still be recorded.\nResearch workflows are highly personal. If you decide to adopt any or all ideas I present here, it may require some tweaking so that it works for you. Years of performing an extrinsic search for existing solutions was unfruitful. Instead, I simply started reading papers and let the process develop organically. Soon I noticed some common patterns and standardisation which allowed me to automate some aspects of the process.\nThere are 3 interdependent phases to this process described below.\n\nFinding papers\nI save the bibtex entry in a bib file (generally in the project root along with the other latex files). This is because the bibtex-mode in Emacs provides powerful search options, templates for adding entries, sorting entries, cleaning up, and much more. Besides, the bib files ultimately gets used in the report so it makes sense to group the two together and keep them in the same location.\n\n\n\n\n\n\nUPDATE 2021-06-15 Tue\n\n\n\nI no longer save bibtex entry in a bib file. Instead, I store them with my notes in the same org file in source code blocks. I use org-tangle to generate the bib file when needed.\n\n\nFirst, I use good ol’ Google Scholar (along with a plugin to enable my institution’s proxy) to find papers. Once, I find a paper I open it and read the abstract. If the abstract appeals to my current research focus or is interesting to me in general, I download it and save it in a folder which syncs across my devices. Next, I grab the bibtex entry and create a task using org-capture where I save the bibtex entry, one liner on why I found the paper interesting. I add the location to the pdf on disk, I set the title of the task as the bibtex entry and I save it to my inbox (or relevant project org file).\n\n\n\n\n\n\nUPDATE 2021-06-15 Tue\n\n\n\nI have written an Elisp package that automates all of this. With the bibtex entry in my clipboard, I now issue an org-capture template which creates a new task with the bibkey as the header, adds relevant properties such as first author last author and source of publication, and sticks the bibtex entry in a source block (see aocp.el).\n\n\nIn scientific paper discovery I describe my process of finding relevant papers in more detail.\n\n\nReading papers\nI read the papers on my laptop. My screen is usually split 50-50 between Emacs (where I take notes) and a pdf viewer. I do not make any highlights because its easy to abuse and it ties the information to the pdf viewer application that I am using.\nInstead, I take notes in plain text (as of 2020, I have been using org-mode, an Emacs major mode to do so). There are several, rationales for doing this digitally. First, I require these notes to be searchable and link-able to other notes. Second, research projects often span for a long duration. Having digital notes ensures that I do not have notes distributed amongst several analog papers/journals. Lastly, I take notes on papers in a specific manner. These notes have a specific purpose - and thus a carefully constructed structure as described in the next section - and thus come under the category of hard information (I talk more about productivity and how I categorise information in productivity).\nI use the 3 pass technique as presented by S. Keshav in his paper titled “How to read a paper” (2007). I employ the first pass as a screening method to identify if a given paper is worth reading in full. During the first pass, I read the introduction and conclusion and briefly glance over the sections of the paper and the diagrams. The goal here is to determine the problem that the paper is trying to solve and get a preliminary idea of the proposed solution and results. Generally, I can determine all the above characteristics just by the reading the introduction and conclusion if the paper is well written.\nDuring the second pass, I read the results, discussion and limitation sections in full. During this phase I {edit, elaborate} my earlier notes made during first pass and also add my personal remarks pertaining to the paper. When reading scientific work in detail, it is often helpful to read at a macro and micro level. For instance, before reading a section from top to bottom - in a linear sense - I often find it helpful to first scan the sub-sections and draw a tree diagram of the topics. This allows me to have a visual representation of the section which often times is enough to quickly conduct a mental cost-benefit analysis of reading the section in detail.\nI rarely read a paper in it’s entirety. I think this is essential when you peer-review a paper (which I do not do yet as of [2021-06-30 Wed]) but unnecessary for day-to-day scientific work.\n\n\nWriting notes\nNotes on paper being read will always be in relation to the current research focus/goal. A paper may be read multiple times but will have different notes if they were made with reference to different research questions.\nFollowing this rationale, I store the notes in an org file pertaining to the project I am working on. I store the pdfs in a central location, synced with a cloud service provider so as to have offline access.\nAnd following are the aspects that I look out for when reading papers.\n+ problem statement :: What is the problem the paper is trying to\n  solve? Why is this problem important? Why should I care? This\n  generally leads to a good material for the introduction of my\n  paper as well.\n+ solution :: What is the solution the paper proposes? How is it\n  better compared to existing ones (do the authors try to show its\n  worth by comparing to existing solutions)? Why should I care about\n  it?\n+ results :: What were the analytical experiments conducted and what\n  were their results? What datasets were used? What models were used\n  and how did they perform? How was their performance evaluated?\n\n  Generally, in my field these questions can be answered by reading\n  the results section, very rarely are results disclosed in the\n  conclusion section. Note that falls out of the 'first pass' content\n  so I may come back to this only if I need this information for the\n  task at hand.\n+ limitations :: Does the paper identify its limitations? what\n  future work can be done?\n+ remarks :: My personal remarks on the paper. Was it well written? Is\n  it scientifically sound? Does it present a good overview of the\n  problem it is trying to solve? This critical examination and\n  cross-checking is essential when conducting a literature review.\nAfter writing down notes on the topics above, I try to write a one line summary of the paper along with my remarks. These are always guided by the research question or work at hand.\nThe “why should I care?” flavor questions will generally be guided by the research question or personal interest that I may have at the time of reading.\nIt’s worth elaborating on the writing process itself because this in itself is a highly non-linear and dynamic activity. With regards to my personal writing, I have noticed that the quantity and quality of notes are inversely proportional. In the early stages of a project, or when I am trying to familiarise myself with a new research topic, I tend to write more notes which are primitive (meaning they emphasise on the first principles) and are similar to the papers I am reading at that point in time. However, with exposure to more papers, the notes become more concise and opinionated (they reflect my thoughts and ideas on the matter).\nI also employ writing on paper and typing in an iterative, cyclic manner to formulate concise and clear text which express my discoveries, ideas and opinions.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/private-link-sharing/index.html",
    "href": "blogs/private-link-sharing/index.html",
    "title": "Private Link Sharing",
    "section": "",
    "text": "Poor man’s document sharing with private links, sort of like what you get with Google Docs. The solution is very simple and I stumbled upon it when I was trying to device a solution for publishing my non-scientific work. The following snippet, when put in the html source, prevents web crawlers from indexing the page. Thus, the page can be hosted on a web server (I use Github Pages) but won’t show up on a web search engine (such as Google, Bing, Yahoo, etc.).\n\n\nindex.html\n\n&lt;meta name=\"robots\" content=\"noindex\" /&gt;\n\nThe page is thus only accessible to those who possess the page url. There are several strategies to generate unique urls. I tend to prepend a timestamp to all my file names. You could also generate a uuid for your file names using the uuidgen command on *nix like operating systems.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html",
    "href": "blogs/privacy-preserving-deep-learning/index.html",
    "title": "Privacy Preserving Deep Learning",
    "section": "",
    "text": "Good morning all, thank you for being here. Today I wanted to share my research on Privacy Preserving Deep Learning (PPDL) which I conducted during my Msc. As an example, we will be looking specifically at PPDL for Medical Image Analysis (MIA). However, I think that this field of research is highly relevant these days and the techniques can be applied to any domain working with sensitive data (such as governance, finance and others)."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-mia",
    "href": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-mia",
    "title": "Privacy Preserving Deep Learning",
    "section": "Challenges of MIA",
    "text": "Challenges of MIA\nMIA concerns itself with the study of medical images in order to determine Regions of Interest(ROI) and image segmentation. MIA is usually performed by radiologists and requires intensive training and practise. For each patient, a radiologist may have to inspect more than 100 images. This is a repetitive task and may pose a cognitive strain, making the process prone to human errors.\nMIA is limited by the skills and cognitive fitness of the human performing the analysis. While humans may not be so good in performing repetitive tasks, machine will do them tirelessly and consistently.\nThis is where Deep Learning (DL) comes into the picture. DL gained popularity and momentum both in the industry and academia due to it’s success with image classification through the ImageNet challenge. It was soon adopted not only within MIA but also other fields of medical science. Although this is an active field of research, several challenges remain. One of the challenges is to train DL models without violating the privacy of the patients."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-dl-in-mia",
    "href": "blogs/privacy-preserving-deep-learning/index.html#challenges-of-dl-in-mia",
    "title": "Privacy Preserving Deep Learning",
    "section": "Challenges of DL in MIA",
    "text": "Challenges of DL in MIA\nDL models require a lot of data for training, generally the more data used, better the prediction results. However, this quantity of data may not always be available at a single medical institute (for instance at small hospitals located in remote locations). Larger institutes who have sufficient data are able to train models however these models are subjected to bias. This is because the data does not accurately represent the entire population. Moreover, if the institute is specialized in a specific disease or observes a disease more frequently due to it’s unique geographic location, the data is biased.\nThus DL models need to be trained using data from several institutes. The traditional server-client architecture posses several privacy and security concerns. Once the institutes upload their data to a server, they may lose their data governance and ownership rights as the server may be located in a different geographical location with a different set of jurisdictions. The data upload and storage procedure may also not be transparent thus raising concerns regarding it’s safety and security.\nAlthough efforts have been made to create centralized medical data repositories (eICU Collaborative Research Database and The Cancer Genome Atlas), data privacy laws such as GDPR in Europe and HIPAA in the State pose a large overhead. The data in such repositories are anonymised, but this does not guarantee security against privacy leaks. Datasets have a unique statistical fingerprint making them vulnerable to linkage attacks.\nDistributed Learning addresses the data ownership and governance problems of centralised learning. However, it still needs to utilise data privacy and security techniques to ensure privacy of patient records. In the following sections of this talk we will take a closer look at Distributed Learning and data privacy techniques for PPDL."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#data-privacy-techniques",
    "href": "blogs/privacy-preserving-deep-learning/index.html#data-privacy-techniques",
    "title": "Privacy Preserving Deep Learning",
    "section": "Data Privacy Techniques",
    "text": "Data Privacy Techniques\nTo date, anonymisation and de-identification remain the predominant privacy-preserving technique for sharing sensitive information. Although there are not standardised methods of doing so, we commonly see 3 techniques in the literature:\n\nDe-identification or removal of sensitive information from datasets. This is not the best solution since it results in loss of valuable information which may be useful for analysis/training.\nPseudo-anonymisation which replaces sensitive information with unique pseudonyms. This is a bit better since we retain the unique linkages and correlations between data points which may be of interest to us.\nAnonymisation which simply put means de-identification with a cherry on top. A few additional things are done on top of de-identification to significantly reduce the probability of re-identification.\n\nDifferential Privacy (DP) preserves the privacy of patients by injecting a certain quantity of noise into the data. This allows for statistical analysis to be conducted without compromising sensitive information. A trade-off between privacy and performance occurs here. More perturbations in the dataset give higher privacy however may negatively affect the model’s performance.\nThe alternative is to encrypt the data or the model parameters. The state-of-the-art encryption schemes cannot be cracked using brute force techniques making them the most secure means of sharing sensitive information. The Homomorphic Encryption (HE) scheme allows certain operations (such as addition, subtraction and multiplication) to be carried out directly over the cyphertext. This is beneficial for neural networks as models can be trained directly using encrypted, unperturbed data with additional computational overhead."
  },
  {
    "objectID": "blogs/privacy-preserving-deep-learning/index.html#security-threats-attacks",
    "href": "blogs/privacy-preserving-deep-learning/index.html#security-threats-attacks",
    "title": "Privacy Preserving Deep Learning",
    "section": "Security Threats & Attacks",
    "text": "Security Threats & Attacks\nAnonymisation techniques are built into MIA software which may explains their popularity. However, anonymised data contain unique statistical fingerprints which make them vulnerable to linkage attacks. A famous example is the Netflix Prize Dataset where researchers were able to combine Netflix’s anonymised user subscription data with the imdb public dataset to identify specific people along with their political preferences and other sensitive information. Another well known example is linkage attack on anonymised hospital records from the State of California along with public voter records to identify the complete medical history of the Governor of California.\nIn traditional software, computers strictly follow a specific set of programmed instructions. In contrast, ML algorithms derive their own set of rules based on a substantial amount of data provided to them. This behaviour often leads to neural networks being interpreted as a black box, preventing users from understanding it’s inner workings. This black box behavior makes neural networks a potential target for exploitation. This identification of such threats and vulnerabilities must be prioritised.\nThese threats can be classified as ones targeting the data and others targeting the model. Adversarial examples are inputs that are often indistinguishable from typical inputs, yet contain intentional feature changes that lead to incorrect classification. Adversarial attacks are of significance since they question the robustness of DL models. They can have devastating consequences for DL applications in high-stakes applications such as medical imaging, facial recognition and autonomous vehicles.\nData poisoning refers to changing the training data such that the model can learn with malicious intent and manifest that as it’s predictions. Open access and public datasets are especially vulnerable to data poisoning. Such datasets are often used to validate proof-of-concept models by companies or worse, to validate bleeding edge innovations in research.\nIn addition to attacks which target the data, the DL model itself can be exploited. By observing the gradients and parameters of a trained network, parts of the dataset can be obtained. Model inversion, membership inference and reconstruction attacks often utilise this technique to obtain the training data or infer if a public dataset was used for training. Combined with linkage attacks, the presence of an individual in a dataset and their sensitive information can be obtained."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html",
    "href": "blogs/data-smells-public-datasets/index.html",
    "title": "Data Smells in Public Datasets",
    "section": "",
    "text": "In this talk I will present our recent paper titled Data Smells in Public Datasets which was published at the 1st International Conference on AI Engineering (CAIN) 2022. I will first present the problem we are trying to solve along with the contributions that we made. I will present the methodology which was followed along with the results obtained. I will present a select few smells which I personally find interesting & hope will generate some discussion. Finally, we will conclude the talk with some high level takeaways from our study along with the limitations & future directions of work."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#correlated-features",
    "href": "blogs/data-smells-public-datasets/index.html#correlated-features",
    "title": "Data Smells in Public Datasets",
    "section": "Correlated Features",
    "text": "Correlated Features\n\n\n\nFigure 4: Correlation of features in all datasets\n\n\nI will start with the presence of correlated features smell which many of us are perhaps already familiar with. I want to start with this smell as it was most frequently observed in our sample of datasets.\nWhen two features A & B are correlated, inducing a positive or negative change in A does the same in B. This presents an opportunity to perform feature engineering and construct a more efficient dataset. ML is highly experimental and any optimisation—no matter how smalls—counts. Small datasets ultimately are easier to understand, faster to train a model on & take up less storage."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#presence-of-sensitive-features",
    "href": "blogs/data-smells-public-datasets/index.html#presence-of-sensitive-features",
    "title": "Data Smells in Public Datasets",
    "section": "Presence of Sensitive Features",
    "text": "Presence of Sensitive Features\n\n\n\nFigure 5: Sensitive features\n\n\nI want to motivate this smell with this figure. This is a probability density plot from the adult census dataset. This dataset consists information regarding individuals including their race & sex. The supervised learning task is to predict the income class they belong to.\nLets focus primarily on the top left plot which shows that for this dataset, a male individual of fairer skin is likely to earn more. Such a bias also exist between male & female individuals of the same race. A model trained & tested on this dataset will perform well however putting such a model in production will result in devastating consequences since the model was trained using biased historical data which does not reflect the real world.\nAnd we see examples of this in the real world, when financial institutes use ML to predict if an individual is entitled to a loan, or when the criminal justice system want to predict the severity of sentencing and when police try to identify if an individual is a threat using video surveillance."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#hierarchy-from-label-encoding",
    "href": "blogs/data-smells-public-datasets/index.html#hierarchy-from-label-encoding",
    "title": "Data Smells in Public Datasets",
    "section": "Hierarchy from Label Encoding",
    "text": "Hierarchy from Label Encoding\n\n\n\nFigure 6: Hierarchy from label encoding\n\n\nFigure Figure 6 presents a probability density plot from the adult census dataset. Here we compare the income class of individuals to their level of education. We can see that for this dataset, an individual with a higher level of education is likely to earn more.\nSuch a hierarchy in categorical features is useful information which can be utilised by the model. A common practise is to encode categorical features using numbers. As an example we can encode the education levels using numbers between 0 and 4, where higher education gets a larger number. Such an encoding scheme can be beneficial to the model as it exposes the hierarchy amongst the values of a categorical feature. However applying the same encoding scheme for the race or sex feature can introduce an unwanted hierarchy amongst the values where non should exist."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#binary-missing-values",
    "href": "blogs/data-smells-public-datasets/index.html#binary-missing-values",
    "title": "Data Smells in Public Datasets",
    "section": "Binary Missing Values",
    "text": "Binary Missing Values\n\n\n\nFigure 7: Binary missing values\n\n\nTwo features from the permit dataset contain a lot of missing values (over 90% of the data in these features are missing). A common technique in such cases is to drop such features since they do not impart any knowledge to the model. However, taking a closer look at the value of the non-missing data, we find that the missing values in these features carry an implicit meaning of ‘no’ or a negative response.\nAttention must be paid to the distribution of the missing values. If the missing values are concentrated within a specific feature (along columns) as opposed to being evenly distributed across the dataset (along columns & rows), it may indicate that they carry an implicit meaning. A novice data scientist may hastily drop such features however in doing so they alter the original information portrait by dataset."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#strings-in-human-friendly-formats",
    "href": "blogs/data-smells-public-datasets/index.html#strings-in-human-friendly-formats",
    "title": "Data Smells in Public Datasets",
    "section": "Strings in Human-friendly Formats",
    "text": "Strings in Human-friendly Formats\n\n\n\nFigure 8: Strings in human-friendly formats\n\n\nThe netfix dataset contains information regarding content on the popular entertainment streaming service. The dataset contains information regarding movies & TV shows along with their duration.\nAlthough the duration for movies can be easily converted to a numerical representation, doing the same for TV shows poses several challenges and requires further effort and domain expertise."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#lack-of-proper-documentation",
    "href": "blogs/data-smells-public-datasets/index.html#lack-of-proper-documentation",
    "title": "Data Smells in Public Datasets",
    "section": "Lack of Proper Documentation",
    "text": "Lack of Proper Documentation\nWe saw several instances where a lack of proper documentation was felt. The heart dataset contains very cryptic column names and understanding the information contained within these column require domain expertise. The sex feature within the same dataset is label encoded however we do not know which number represents which gender. The cancer dataset contains several numerical features but we do not know the unit in which the measurements were recorded.\nEvery dataset is unique & contains its own idiosyncrasies and we require proper documentation to understand them. Documentation provides useful metadata & context to data scientists who are getting started with a dataset & also help re-familiarise them to the dataset quickly when they come back at a later time."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#lack-of-best-practices",
    "href": "blogs/data-smells-public-datasets/index.html#lack-of-best-practices",
    "title": "Data Smells in Public Datasets",
    "section": "Lack of Best Practices",
    "text": "Lack of Best Practices\nWe also found several instances of technical debt from lack of best practices in upstream processes. Going back to the netflix dataset where extracting numerical duration for TV shows was found to be challenging, or the heart dataset with its cryptic column names and the cancer dataset where the unit was not recorded. In all these instances, technical debt could have been avoided by simply using better column names or providing documentation. By following standardised procedures in the upstream data collection/creation stages, technical debt in the downstream stages can be avoided.\nWe feel that data smells can help identify such sources of technical debt in the early stages of a ML pipeline where the complexity is relatively lower, and fixes are cheaper and easier to implement. This becomes especially important when working with external stakeholders within financial constraints."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#limitations",
    "href": "blogs/data-smells-public-datasets/index.html#limitations",
    "title": "Data Smells in Public Datasets",
    "section": "Limitations",
    "text": "Limitations\nWe opted for a shallow as opposed to a deep analysis of this datasets. This means that we did not fit a model to each dataset and carry out a supervised learning task. While such a workflow may reveal more smells, we believe that the smells also become specific to the dataset, model or problem we are trying to solve. Our intention was to pick the smallest subset of analysis tasks that can be scaled across several datasets.\nThe smells are linked to the version of the dataset analysed. Unfortunately this is true for all data-centric work. However we do our best to make our results reproducible by providing the version of the data that was analysed in our paper.\nWe do not know the impact of the smells. For instance, if we consider the missing units smell, we do not know if and to what extent this smell affects the performance of a model. This was considered beyond the scope of this project however remains to be a viable extension to our current work.\nFinally, smells are subjective to the human. But this is true for code smells as well. Not all long methods are bad and god classes still exist in public software projects."
  },
  {
    "objectID": "blogs/data-smells-public-datasets/index.html#future-work",
    "href": "blogs/data-smells-public-datasets/index.html#future-work",
    "title": "Data Smells in Public Datasets",
    "section": "Future Work",
    "text": "Future Work\nGrowing the catalogue using more datasets is a low-hanging fruit. It would also be interesting to explore the notion of smells for semi-structured and unstructured datasets. Finally, it would be interesting to understand the co-occurance and evolution of smells throughout the ML lifecycle."
  },
  {
    "objectID": "blogs/doi2bib/index.html",
    "href": "blogs/doi2bib/index.html",
    "title": "Automatically retrieving Bibtex information from DOI",
    "section": "",
    "text": "doi2bib is a simple Python script I wrote to automatically retrieve bibtex information for a given DOI. The script queries Crossref to obtain the bibtex. The script can also handle pre-prints published on Arxiv. Here is the content of doi2bib along with some explaination of what the script does.\n\n\ndoi2bib\n\n#!/usr/bin/env python3\n\nimport sys\nimport argparse\nfrom urllib import request, error\n\n3def get_bibtex(doi,ispreprint):\n    if ispreprint:\n        url = f\"https://arxiv.org/bibtex/{doi}\"\n    else:\n        url = f\"https://api.crossref.org/works/{doi}/transform/application/x-bibtex\"\n    req = request.Request(url)\n\n    try:\n        with request.urlopen(req) as response:\n            return response.read().decode()\n    except error.HTTPError as e:\n        return f\"HTTP Error: {e.code}\"\n    except error.URLError as e:\n        return f\"URL Error: {e.reason}\"\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n            description=\"Convert DOI or Arxiv ID to Bibtex\"\n            )\n1    parser.add_argument(\n            \"doi\",\n            help=\"DOI or Arxiv ID of paper\"\n            )\n2    parser.add_argument(\n            \"-p\",\n            \"--preprint\",\n            help=\"Treat provided DOI as Arxiv ID\",\n            action=\"store_true\",\n            default=False,\n            )\n    args = parser.parse_args()\n\n    bibtex = get_bibtex(args.doi, args.preprint)\n    print(bibtex)\n\n\n1\n\nThe script must be provided with a DOI. This can also be an Arxiv ID.\n\n2\n\nThe -p or --preprint flag can be specified to indicate that the provided DOI is an Arxiv ID.\n\n3\n\nThe Crossref API is queried with the provided DOI to retrieve the bibtex information. With the --preprint flag, the Arxiv API is used.\n\n\nI pipe the results through the bib-tool CLI, to format the text and generate a unique key. I specify the following key format in my .bibtoolrsc file.\n\n\n.bibtoolrsc\n\nprint.use.tab=off\nfmt.et.al=\"\"\nkey.format=\"%-1n(author)%4d(year)%-T(title)\"\n\nBy default, bibtool uses tabs for indentation. I turn this off. Bibtool adds “.ea” to the author name to indicate “and others”. I prefer to just have the last name of the first author in the key, so I set it to an empty string. I set the format of the key to the last name of the first author, followed by the year of publication and the first meaningful word from the title.\nHere is the script in action, I use one of my own publications as an example.\n$ doi2bib 10.1145/3522664.3528621 |bibtool -k\n\n@InProceedings{   shome2022data,\n  series        = {CAIN ’22},\n  title         = {Data smells in public datasets},\n  url           = {http://dx.doi.org/10.1145/3522664.3528621},\n  doi           = {10.1145/3522664.3528621},\n  booktitle     = {Proceedings of the 1st International Conference on AI\n                  Engineering: Software Engineering for AI},\n  publisher     = {ACM},\n  author        = {Shome, Arumoy and Cruz, Luís and van Deursen, Arie},\n  year          = {2022},\n  month         = may,\n  collection    = {CAIN ’22}\n}\nAnd here is another example using an Arxiv ID (again, one of my own).\n$ doi2bib --preprint 2305.04988 |bibtool -k\n\n@Misc{            shome2023towards,\n  title         = {Towards Understanding Machine Learning Testing in\n                  Practise},\n  author        = {Arumoy Shome and Luis Cruz and Arie van Deursen},\n  year          = {2023},\n  eprint        = {2305.04988},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.SE}\n}\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/today/index.html",
    "href": "blogs/today/index.html",
    "title": "Timestamps in the Shell (today)",
    "section": "",
    "text": "I often work with text files containing pros (such as blog posts and git commit messages) and require adding a timestamp containing the current date, day & time.\nI wrote today, a shellscript which returns the current date in various formats. Here is the script as of 2022-03-09, the latest version can be found in my dotfiles.\n\n\ntoday\n\n#!/usr/bin/env bash\n\n# today: return today's date in various formats\n\n# Usage: today [OPTS]\n# Without any options, today will print today's date in %Y-%m-%d\n# format. Following are the supported options:\n#    --with-day: %Y-%m-%d %a\n#    --with-time: %Y-%m-%d %H:%M\n#    -l | --long: %Y-%m-%d %a %H:%M\n#    -h | --human: %a %b %d, %Y\n#    -s | --stamp: enclose the date in square braces\n\n# NOTE: when using both --with-day & --with-time, the order in which\n# the options are passed matters. For example:\n#\n# today --with-day --with-time will produce\n#    2022-03-03 Thu 02:20\n#\n# But today --with-time --with-day will produce\n# 2022-03-03 02:21 Thu\n\n# NOTE: when using --human option, all other options are ignored.\n\nmain() {\n  local FMT='%Y-%m-%d'\n  local STAMP_FLAG=1 # false\n\n  while [[ \"$1\" =~ ^- && ! \"$1\" == \"--\" ]]; do\n    case \"$1\" in\n      --with-day)\n        FMT=\"$FMT %a\"\n        ;;\n      --with-time)\n        FMT=\"$FMT %H:%M\"\n        ;;\n      -s | --stamp)\n        STAMP_FLAG=0 # true\n        ;;\n      -l | --long) # short for --with-day --with-time\n        FMT=\"$FMT %a %H:%M\"\n        ;;\n      -h | --human) # alternate format, ignore other flags\n        FMT=\"%a %b %d, %Y\"\n        ;;\n      *)\n        echo \"Error: unknown option $1.\"\n        return 1\n    esac; shift # only shift here since we only pass flags\n  done\n\n  local OUT=$(date +\"$FMT\")\n\n  [[ \"$STAMP_FLAG\" -eq 0 ]] && OUT=\"[$OUT]\"\n\n  echo \"$OUT\"\n}\n\nmain \"$@\"\n\nWithout any arguments, today prints the date in ‘%Y-%m-%d’ format. Using the following optional flags the output can be manipulated.\n\n\n\nflag\noutput\n\n\n\n\n–with-day\n‘%Y-%m-%d %a’\n\n\n–with-time\n‘%Y-%m-%d %H:%M’\n\n\n–long\n‘%Y-%m-%d %a %H:%M’\n\n\n–human\n‘%a %b %d, %Y’\n\n\n\nThe --stamp flag can be used to optionally wrap the output in square braces.\nI frequently use this to add a timestamp to my git commit messages.\ngit commit -m \"feat: timestamps from the shell $(today -l -s)\"\nOr insert a timestamp into the current buffer I am editing in vim.\n:r! today -l -s\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/project-management/index.html",
    "href": "blogs/project-management/index.html",
    "title": "Project Management",
    "section": "",
    "text": "I think every project has 3 distinct components.\n\nKnowledge. Which may include notes, wikis, task lists, meeting notes, reviews, planning, etc.\nProduct. Which for me tends to be software, data, visualizations, etc.\nPublish. Which for research can be a paper or for product driven projects can be blog posts, company announcements, etc.\n\nThere are several tools for project management. The one that worked best for me was Basecamp and I have stolen many of their philosophies to craft my own.\nThe principles of managing a project is fairly simple. First, identify who are involved in the project, what you are going to do, how you are going to do it and by when you are going to do it.\n\nThe Who\nThe who become the stakeholders or clients of the project and need to be kept up to date on the progress, potential problems, approached for input, etc. Effective and clear communication is of utmost importance here.\n\n\nThe What\nThe what is the goal of the project. Some of the questions that we may ask are: What are we trying to solve? What is our research question? What are we building? etc.\n\n\nThe How\nThe how is the task list. Try to visualise the entire lifecycle of the project (to the best of your abilities given your current knowledge and technical capabilities) and write down the distinct phases of the project. Next, for each phase write down a list of tasks (and sub tasks). Keep a fluid mindset, projects change, clients want different things, research is dynamic. The idea is to start with a set of tasks and revise as and when necessary.\n\n\nThe When\nThe when is the date of completion. Deadlines are important for proper goal and expectation setting. It’s important to have a final deadline for the entire project, but also to have intermediate deadlines for smaller batches/mini-projects. Divide and conquer is the name of the game here.\nSome recommend a few weeks (4-6) for these batches/mini-projects. For research, I have found that taking things at a week-by-week basis works best and is the most flexible method of working given how dynamic research can be.\nFor each of these batches, I like to have a single focus of work. I then extract the relevant tasks that I should get done in order to accomplish the focus.\nDetermining focus is subjective and external factors such as other engagements play a role. The goal is to always plan sufficient work so as to not under deliver. I actively try not to plan too many things so as to feel overwhelmed or anxious.\n\n\n\n\n\n\nUPDATE 2021-09-05\n\n\n\nSome notes on attaining focus can be found here.\n\n\nAt the end of each batch/cycle/week I conduct a review. I think about what went well (so things I was able to finish), what did not go well (problems, challenges, etc.) and what can be improved (mostly for personal reflection). During the review I triage the task list, revise project scope, deliverables, etc.\n\n\n\n\n\n\nNote\n\n\n\nYou can find more about my reviewing process here.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/yob/index.html",
    "href": "blogs/yob/index.html",
    "title": "Toggling background color in kitty and vim (yob)",
    "section": "",
    "text": "yob is a shell script that I wrote to toggle between a light and dark colorscheme in Kitty, my terminal of choice. Some additional configuration also allow me to sync the vim colorscheme to that of kitty. This is a much simpler version of this script by Greg Hurrell.\nThe name is inspired by the vim-unimpaired package for vim. The package adds keybindings prefixed with the yo* that toggle various vim settings. The b in yob stands for “background”.\nHere is the script in its entirety as of 2024-01-19. You can also find the latest version in my dotfiles repository.\n\n\nyob\n\n#!/usr/bin/env bash\n\n1LIGHT_THEME=\"colors/gruvbox-light.conf\"\nDARK_THEME=\"colors/gruvbox-dark.conf\"\nLOCATION=\"$HOME/.local/share/yob\"\nVIM_BG_FILE=\"$LOCATION/background\"\n\n__init() {\n  [[ ! -d \"$LOCATION\" ]] && mkdir -p \"$LOCATION\"\n  [[ ! -e \"$VIM_BG_FILE\" ]] && touch \"$VIM_BG_FILE\"\n}\n\n2__update() {\n  (\n  cd \"$HOME/.config/kitty\"\n  if [[ \"$1\" == \"light\" ]]; then\n    ln -sf \"$LIGHT_THEME\" current-theme.conf\n    echo \"light\" &gt;\"$VIM_BG_FILE\"\n  else\n    ln -sf \"$DARK_THEME\" current-theme.conf\n    echo \"dark\" &gt;\"$VIM_BG_FILE\"\n  fi\n  )\n\n  kitten @ set-colors --all --configured \"$HOME/.config/kitty/current-theme.conf\"\n}\n\n3__toggle() {\n  local CURRENT_BG\n  CURRENT_BG=\"$(head -n 1 \"$VIM_BG_FILE\")\"\n  if [[ \"$CURRENT_BG\" =~ \"light\" ]]; then\n    __update \"dark\"\n  else\n    __update \"light\"\n  fi\n}\n\nmain() {\n  if [[ ! \"$TERM\" =~ \"kitty\" ]]; then\n    echo \"yob: not running kitty, doing nothing.\"\n    exit 1\n  fi\n\n  if [[ ! -e \"$VIM_BG_FILE\" ]]; then\n    __init\n    __update \"dark\"\n    exit 0\n  fi\n\n  __toggle\n}\n\nmain \"$@\"\n\n\n1\n\nSome initial setup. Here we define the location of the light and dark colorscheme files for kitty (I choose to save them in the kitty config directory and check it into git so that they are available wherever I clone my dotfiles repo). We also store additional data for yob in ~/.local/share/yob/background as per XDG best practices for unix systems.\n\n2\n\nHere we create a symbolic link between the colorscheme file and current-theme.conf. The current-theme.conf file is picked up by kitty next time it starts. Finally, we set the theme for the existing kitty sessions using kitty @ set-colors.\n\n3\n\nThis function toggles between the light and dark themes based on information in ~/.local/share/yob/background.\n\n\n\nChoosing a colorscheme with light and dark variant\nAfter many experiments with various colorschemes in different lighting conditions, I picked Gruvbox as my theme of choice. There are several reasons for this decision:\n\nThe colorscheme has been around for a little over 10 years and is very stable (very few changes since 2018 as per the commits chart on github).\nBoth light and dark variants of the theme are legible in various lighting conditions.\nDue to its popularity, the theme is available in Kitty and Vim 9 out-of-the-box.\n\n\n\nPreserving kitty colors across sessions\nTo preserve the colorscheme across kitty sessions, yob symlinks the colorsheme files to current-theme.conf in kitty’s config directory. The following line ensures that kitty sources the right colorscheme next time kitty starts.\n\n\n~/.config/kitty/kitty.conf\n\ninclude current-theme.conf\n\n\n\nSyncing vim colors with kitty\nThe aru#set_background() function reads the first line of ~/.local/share/yob/background using the built-in readfile function in vim (see :help readfile) and updates the background.\n\n\n.vim/autoload/aru.vim\n\nfunction! aru#set_background() abort\n  let config_file = expand('~/.local/share/yob/background')\n  if filereadable(config_file)\n    let bg = readfile(config_file, '', 1)[0]\n  endif\n\n  execute 'set background=' .. bg\nendfunction\n\nIf we run yob from another shell, the colors in existing vim sessions does not update. To account for this I introduce an autocommand that is fired every time vim is started and when it gets focus.\n\n\n.vimrc\n\nset termguicolors\ncolor retrobox\n\nfunction! AruAutoBackground() abort\n  augroup AruAutoBackground\n    autocmd!\n    autocmd FocusGained,VimEnter * call aru#set_background()\n  augroup END\nendfunction\ncall AruAutoBackground()\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/focus/index.html",
    "href": "blogs/focus/index.html",
    "title": "Focus",
    "section": "",
    "text": "Usually around late July - early August, I experience what I call “Productivity Anxiety”. It starts by me siting down in front of my laptop to work but not being able to identify what to work on. After an hour or so of contemplation, I start feeling guilty of not working. I know that if I can’t get myself to work on something I should get up and do something else. But then I would be missing out on work wouldn’t I? And so this vicious cycle continues over days, weeks, sometimes entire months.\nThe quickest way to get over this mental block is to work on something of importance, something with stakes where you have something to lose. Work projects are a good place to start. However that’s not enough. The other part is knowing what to work on, or what to focus our time and energy on.\nThe assumption here is that we have a functional task/information management system in place. This system allows us to collect all tasks in a centralised location which makes finding things to do next easier. Task management is not enough though, we must establish a notion of priority next. I find goals and project deadlines are a good place to start to derive some prioritisation of tasks/projects at a macro level (think year, quarter or month). For long running projects that stretch over several months or years (which is fairly common in research projects) I find setting pseudo-deadlines a great way to establish what to work on next.\n\n\n\n\n\n\nTip\n\n\n\nSee my prior post on productivity and research workflow for how I manage information.\n\n\nAlthough it’s easier to see progress over a longer period of time such as months or a year, we must approach work in smaller chunks such as over a few days or weeks. At this micro scale, I find assigning a theme per day helps minimise context switch, gain focus and stop wasting time trying to identify what to work on next. This theme can simply be a specific project or a group of similar tasks. For instance, I dedicate Mondays to writing and Fridays to reading papers. The rest of the days I break based on the current projects I am working on.\n\n\n\n\n\n\nTip\n\n\n\nSee project management for how I manage projects.\n\n\nTime blocking sounds like a wonderful productivity technique on paper, but in practise it does not work. First, it takes a lot of time to plan out work blocks every week. Second, I simply do not want to live my life based on what a piece of software tells me to do. Life is dynamic and time blocking does not accommodate emergencies or impromptu decisions. I don’t pre-plan work blocks. Instead, when I do find a block of uninterrupted time, I dedicate 100% of my energy to the task(s) at hand. I turn off all distractions and minimise breaks. Usually a 15 min stretch/coffee/bathroom break in the middle of a 3 hour block works well.\nThe overarching secret to the success of this system is the act of periodic review. The reviews help me adjust the priorities and tasks for the upcoming week/month/year. I also find tracking the time spent on the work blocks valuable. This paints a nice overview of how I spent my time during the last day/week/month and helps identify any “leaks” which need to be addressed.\n\n\n\n\n\n\nTip\n\n\n\nNotes on my reviewing process can be found here.\n\n\nPerhaps the most important realisation I have had is that enjoying the free time, the mundane things such as cleaning and also doing absolutely nothing is equally important as working. Being in the moment and mindful when spending time away from the screen has made the single biggest difference in my work-life balance. Even though I have a constant hum of work to do, I am able to remain sane and produce a consistent, high quality of work by simply being present in the moment.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/reflections-scientific-research/index.html",
    "href": "blogs/reflections-scientific-research/index.html",
    "title": "Reflections on Scientific Research",
    "section": "",
    "text": "I have been working as a researcher for several months now and have been working towards my first publication for the past few weeks. I have a few points of reflection on the scientific process and what constitutes as a good researcher.\n\nReading\nThis has been a challenge for me so far. I started my PhD with a literature review and during those initial 3 months I read a lot of papers. However once the analysis started, the reading dropped down to 0 papers per week. I am not sure if it is possible to keep a consistent reading practise, however reading at least 1 or 2 papers per week is ideal. Reading not only helps to be inspired and spark new ideas, but also makes for good content for the related work & discussion section of your paper. Moving forward, I want to review recent publications from the top journals of my field, and try to reflect upon what they did well, what I will do differently and identify interesting intersections between their topic and my research interests.\n\n\nWriting\nI started to use org-mode at the start of my PhD, capture and organize my thoughts and ideas. This worked out well because when the time came to start writing, I already had a pretty good outline for the paper ready. One thing that was still a challenge (at least for the first week of writing) is that the process itself was very slow. I don’t think this can be helped/improved. This is simply my process and all I can really do it work through it. Moving forward, I will try to plan out content for the report throughout the project.\n\n\nPaper Discovery\nThis is relevant for reading & writing. I think my current system works really well (see scientific paper discovery).\n\n\nExperimentation & Methodology\nThis could have been planned better. While writing the report I realised several loops & flaws in my data collection and methodology. My supervisor has recommended a book (see reference below) which I will read prior to our next project. Moving forward, I will also try to think about the dataset and its design earlier.\n\n\nGeneral Thoughts & Remarks\nI am curious to understand how my supervisor was able to see the potential in the idea we ended up pursuing versus the ones I proposed. Is this something that comes with experience? How can I spot a good (scientific research) idea?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aspell-commands/index.html",
    "href": "blogs/aspell-commands/index.html",
    "title": "Aspell Commands",
    "section": "",
    "text": "--master\nset the language, I always use “en_GB” for British English\n\n\n--mode\nset the filetype, typically I use “markdown” or “latex”\n\n\n-c\ncheck for spelling mistakes in the provided file\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/navigating-vim/index.html",
    "href": "blogs/navigating-vim/index.html",
    "title": "Navigating in Vim",
    "section": "",
    "text": "The gf visits the file under the cursor if it exists in the path variable. I find this quite useful and use it constantly (for instance to navigate to imported files in python projects). There are several variants of this command:\n\n\n\ngF\nsame as gf but also navigate to the line\n\n\nC-w f\nsame as gf but open in a split window\n\n\nC-w F\nsame as C-w f with line number\n\n\nC-w gf\nsame as gf but open in new tab\n\n\nC-w gF\nsame as C-w gf with line number\n\n\n\nSince the line number variants fall back to their non line number counter parts, I remap them to the non line number variants.\n\n\n~/.vimrc\n\nnmap gf gF\nnmap &lt;C-w&gt;f &lt;C-w&gt;F\nnmap &lt;C-w&gt;&lt;C-f&gt; &lt;C-w&gt;F\nnmap &lt;C-w&gt;gf &lt;C-w&gt;gF\n\nVim provides the &lt;cname&gt; parameter which expands to the filename under the cursor. To make gf automatically create the file if it doesn’t exist, a mapping can be created: map fg :e &lt;cname&gt;. However, I prefer to keep this operation transparent and manual (so that I know what I am doing). Check :h gf for more info. The visual variant of gf uses the visual selection as the filename.\nVim also allows navigation by tags using C-[. This however requires the external ctags command and the tag generation is left to the user.\nFinally vim provides the :find command which searches for the given file in path and opens the first hit. The :sfind does the same but in a split window. Both commands accept a glob pattern which I extensively use to find what I need to edit without a fuzzy finder.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/gc-matplotlib-orgmode/index.html",
    "href": "blogs/gc-matplotlib-orgmode/index.html",
    "title": "Garbage Collection for Matplotlib in org-mode",
    "section": "",
    "text": "I was experiencing random crashes when using emacs and orgmode as a replacement for jupyter notebooks. Turns out that matplotlib does not clear the memory automatically after every plot. So after a few plots, the python interpreter running inside emacs was using a lot of memory and causing emacs to crash. The solution seems to be manual memory management. The solution proposed by this stack overflow comment seems to be manual memory release followed by garbage collection.\nThe process is a bit more involved when using seaborn as it interacts with matplotlib internally. For axes level figures, we can pass an axes object to seaborn. In essense, we have control over the matplotlib.pyplot object so we can use the solution proposed above. For figure level objects however, seaborn returns a PairGrid or FacetGrid object. We can however access the underlying axes and figure from this object and call their corresponding clear() function.\n\n\n\n\n\n\n2021-10-08\n\n\n\nI was assigning the same variable to the various plots so I don’t understand why it was accumulating so much memory. Another alternative is to execute each plot in a separate python process. This can be done by writing individual methods to perform a certain transformation or visualisation. If we execute the entire file as a script, we can invoke all methods, or import the module and pick-and-choose what to do.\nThe problem may be specific to Emacs because it’s built to edit text after all. With the python process running inside Emacs, it may be panicking when it sees a 1GB ‘file’ and crashing cause it thinks it is using too much memory. So perhaps even Jupyter was using so much memory but since there is no limit to how much memory it can use (and my laptop has plenty of memory to give) I didn’t observe this problem. Regardless, I think the solution proposed above will help.\n\n\n\n\n\n\n\n\n2021-10-15\n\n\n\nWhen using seaborn axes level methods, it’s best to pass the matplotlib Axes object ourselves (using matplotlib.pylot.subplots) so that we can manually clear the memory using the solution provided in the stack overflow post. When using figure level methods, seaborn returns a FacetGrid object which has the axes & fig attributes which give us to the underlying matplotlib axes and figure objects respectively. We can call the clear method on them to clear the memory.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/effortless-parallel-execution-xargs/index.html",
    "href": "blogs/effortless-parallel-execution-xargs/index.html",
    "title": "Effortless Parallel Execution with xargs & Friends",
    "section": "",
    "text": "Recently, I had to run Tensorflow Data Validation on over 500 public datasets from Kaggle to generate a baseline schema file for further analysis. I chose to do this using the xargs unix command.\nFollowing is a python script which generates the schema file and saves it to disk for a single csv dataset.\n\n\ncsv2schema.py\n\n#!/usr/bin/env python\n\nimport os\nimport sys\nimport tensorflow_data_validation as tfdv\nimport pandas as pd\n\n_CWD = os.path.dirname(__file__)\nDATADIR = os.path.abspath(os.path.join(_CWD, '..', 'data'))\nSTATSDIR = os.path.join(DATADIR, 'stats', 'train')\nSCHEMADIR = os.path.join(DATADIR, 'schema')\n\nname, _ = os.path.basename(sys.argv[1]).split('.')\n\nif os.path.isfile(os.path.join(SCHEMADIR, name+'.proto')):\n    print(name+'.proto', 'already exists, skipping...')\nelse:\n    frame = pd.read_csv(os.path.join(DATADIR, 'train', name+'.csv'))\n    stats = tfdv.generate_statistics_from_dataframe(frame)\n    schema = tfdv.infer_schema(stats)\n    tfdv.write_stats_text(stats, os.path.join(STATSDIR, name+'.proto'))\n    tfdv.write_schema_text(schema, os.path.join(SCHEMADIR, name+'.proto'))\n\nThe script accepts as argument a valid csv file (we assume that the file names are pruned and do not contain a period character within the name, but only to denote the extension). We read the file as a pandas dataframe, generate the statistics using tfdv.generate_statistics_from_dataframe function and infer a schema which is stored on disk for later analysis.\nFollowing is the bash shellscript wrapper which executes the python script presented above across several datasets using the find command. You may have to experiment with the -P flag which specifies the number of cores to distribute the execution across.\n\n\n\"csv2schema.bash\n\n#!/usr/bin/env bash\n\nmkdir -p data/{schema,stats/train}\n\nfind data/train -type f |\n    xargs -n 1 -P 4 ./bin/write-schema.py\n\nThat’s all there is to it! Write your main script with one file in mind, and distribute across several files using a combination of find and xargs.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aims/index.html",
    "href": "blogs/aims/index.html",
    "title": "Aru’s Information Management System (AIMS)",
    "section": "",
    "text": "AIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\nAlthough org-mode is great, the primary motivation for writing AIMs is because I was feeling a lot of resistance when trying to tune it to my workflow, primarily because of Elisp. Org-mode also requires that you use Emacs as your text editor. I did not appreciate the “vendor lock-in” enforced by org-mode.\nYou can find the latest version of the script on my dotfiles repo, below is the script as it stands on 2022-02-28.\n\n\naims\n\n#!/usr/bin/env bash\n\n1NOTESDIR=\"$HOME/org\"\nINBOX=\"$NOTESDIR/inbox.md\"\nTEMPLATESDIR=\"$XDG_DATA_HOME/aims\"\n[[ ! -e \"$INBOX\" ]] && touch \"$INBOX\"\n\n__capture() {\n  # Capture incoming info quickly. All items are appended to INBOX\n  # which defaults to `inbox.md' in NOTESDIR. Optionally a template\n  # can be specified using the --template| -t flag.\n2  local TEMPLATE=\"$TEMPLATESDIR/default\"\n\n  while [[ \"$1\" =~ ^-+.* && ! \"$1\" == \"--\" ]]; do\n    case \"$1\" in\n      --template | -t)\n        shift\n        TEMPLATE=\"$TEMPLATESDIR/$1\"\n        ;;\n      *)\n        echo \"Error: unknown option $1.\"\n        return 1\n        ;;\n    esac; shift\n  done\n\n3  local ITEM=$(mktemp)\n  if [[ -e \"$TEMPLATE\" && -x \"$TEMPLATE\" ]]; then\n    eval \"$TEMPLATE $ITEM\"\n  fi\n\n4  if eval \"$EDITOR -c 'set ft=markdown' $ITEM\"; then\n    [[ \"$1\" && -e \"$NOTESDIR/$1\" ]] && INBOX=\"$NOTESDIR/$1\"\n    cat \"$ITEM\" &gt;&gt; \"$INBOX\"\n    echo \"Info: captured in $INBOX.\"\n  fi\n\n5  echo \"Info: cleaning up $(rm -v \"$ITEM\")\"\n}\n\n__capture \"$@\"\n\n\n1\n\nStore all notes in $HOME/org/inbox.md, creating it if necessary. Also look for template scripts in ~/.local/share/aims.\n\n2\n\nParse the flags passed to AIMS. Currently it only supports the --template/-t flag which accepts the name of the template to use. Use the default template if none is provided. More on this later.\n\n3\n\nCreate a temporary file and insert the contents of the template.\n\n4\n\nEdit the temporary file using $EDITOR (here I assume its vim or neovim), setting the filetype to markdown. If the first positional argument passed to AIMS is a valid file inside $NOTESDIR then set that to the $INBOX file. Finally, prepend the contents of the temporary file to $INBOX file, if vim does not report an error.\n\n5\n\nCleanup, remove the temporary file.\n\n\nFor the time being, it only provides the capture functionality. A temporary file is used to compose the text first. Upon successful completion, the contents of the temporary file are appended to the default $INBOX file if no other files are specified.\nWhat I find really neat is the templating system. An arbitrary name for a template can be passed to aims using the --template (or -t for short) flag. aims looks for a shellscript with the same name in the ~/.local/share/aims directory and executes it if it exists. The beauty of this design is in its simplicity. Since templates are shellscripts, it gives us the full expressiveness of the shell. This is best demonstrated with some examples. Here is my default template as of 2022-02-28 which is used when no template is specified.\n\n\n~/.local/share/aims/default\n\n#!/usr/bin/env bash\n\n1[[ -z \"$1\" ]] && return 1\n\n2echo &gt;&gt; \"$1\"\necho \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\n\n\n1\n\nSanity check, ensure that a positional argument was passed (that is, the temporary file path).\n\n2\n\nInsert an empty line and a level 1 markdown header with a time stamp.\n\n\nIt simply adds a level 1 markdown header followed by a timestamp. Here is another for capturing bibtex information for research papers.\n\n\n\n\n\n\nTip\n\n\n\nI also wrote aocp.el, an emacs package to capture bibtex information of research papers using org-mode.\n\n\n#!/usr/bin/env bash\n\n[[ -z \"$1\" ]] && return 1\n\necho &gt;&gt; \"$1\"\n\n1BIBKEY=$(pbpaste | grep '^@.*' | sed 's/^@.*{\\(.*\\),/\\1/')\nif [[ -n \"$BIBKEY\" ]]; then\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')] $BIBKEY\" &gt;&gt; $1\nelse\n  echo \"# [$(date +'%Y-%m-%d %a %H:%M')]\" &gt;&gt; $1\nfi\n\n2echo &gt;&gt; \"$1\"\necho '+ **Problem Statement:**' &gt;&gt; \"$1\"\necho '+ **Solution**' &gt;&gt; \"$1\"\necho '+ **Results**' &gt;&gt; \"$1\"\necho '+ **Limitations**' &gt;&gt; \"$1\"\necho '+ **Remarks**' &gt;&gt; \"$1\"\necho &gt;&gt; \"$1\"\n3echo '```bibtex' &gt;&gt; \"$1\"\n\nif [[ -n \"$BIBKEY\" ]]; then\n  pbpaste | sed '/^$/d' &gt;&gt; \"$1\"\n  pbcopy &lt;(echo \"$BIBKEY\")\nfi\n\necho '```' &gt;&gt; \"$1\"\n\n1\n\nCheck that the bibtex information is currently in the system clipboard by attempting to extract the key using grep and sed. If a key was successfully extracted, then create a level 1 markdown header with a time stamp and the key. Otherwise, fall back to just a time stamp.\n\n2\n\nAdd my prompts for note-taking when reading scientific papers.\n\n3\n\nRemove empty lines and put the bibtex information in a markdown source block.\n\n\nThis one is a bit more involved but highlights the power of using shellscripts for templating. Given that a bibentry is copied in the clipboard, this template adds a level 1 markdown header with a timestamp and the bibkey. It adds my note-taking prompts and sticks the bibentry at the bottom.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html",
    "href": "blogs/research-workflow-plaintext/index.html",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "In this talk I will go over how we can use Emacs and org-mode to craft a research workflow. We will look at how we can leverage the power of Emacs and org-mode to capture, store, search and retrieve research data, all in plain text! The talk will touch upon how org-mode can be used as an environment for literate programming and reproducible research. I do not assume any prior knowledge of emacs or org-mode and I want this to be more of a discussion rather than a talk. Please ask me questions as I go along and share your thoughts, tips and techniques with others!\n\n\nOutline of this talk is as follows:\n\nI will start with a brief history of Emacs and org-mode.\nA brief background on plaintext.\nWe will follow with some motivation for why we would want to use plaintext to manage information and some of my supporting philosophies.\nNext, we will jump into the building blocks of org-mode.\nWhich we will leverage to craft research workflows and introduce some automation into menial tasks.\nI will highlight some additional features of org-mode that we can use for literate programming and reproducible research.\nI will end this talk by pointing out additional resources that you may use to get started with Emacs.\n\n\n\n\nWhen I was preparing material for this talk, I thought: what better way to talk about emacs and org-mode than to do it in emacs itself. There is a lot of text compared to a traditional presentation, and there is justification for doing so:\n\nThis talk is highly interactive and there are demos throughout. It would have been a bit annoying having to constantly switch between emacs and my slides.\nThere is another reason however I will reveal this towards the end of the talk\n\n\n\n\nThe original talk was giving using an org document, from within Emacs. This version that you are reading however, is the markdown version, exported from the original org document. Since the original talk was designed to be interactive and dynamic, certain parts of this post may not make sense in this static HTML format."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#outline",
    "href": "blogs/research-workflow-plaintext/index.html#outline",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "Outline of this talk is as follows:\n\nI will start with a brief history of Emacs and org-mode.\nA brief background on plaintext.\nWe will follow with some motivation for why we would want to use plaintext to manage information and some of my supporting philosophies.\nNext, we will jump into the building blocks of org-mode.\nWhich we will leverage to craft research workflows and introduce some automation into menial tasks.\nI will highlight some additional features of org-mode that we can use for literate programming and reproducible research.\nI will end this talk by pointing out additional resources that you may use to get started with Emacs."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#a-note-for-the-audience",
    "href": "blogs/research-workflow-plaintext/index.html#a-note-for-the-audience",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "When I was preparing material for this talk, I thought: what better way to talk about emacs and org-mode than to do it in emacs itself. There is a lot of text compared to a traditional presentation, and there is justification for doing so:\n\nThis talk is highly interactive and there are demos throughout. It would have been a bit annoying having to constantly switch between emacs and my slides.\nThere is another reason however I will reveal this towards the end of the talk"
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#a-note-for-the-readers",
    "href": "blogs/research-workflow-plaintext/index.html#a-note-for-the-readers",
    "title": "Research Workflow in Plaintext",
    "section": "",
    "text": "The original talk was giving using an org document, from within Emacs. This version that you are reading however, is the markdown version, exported from the original org document. Since the original talk was designed to be interactive and dynamic, certain parts of this post may not make sense in this static HTML format."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#emacs",
    "href": "blogs/research-workflow-plaintext/index.html#emacs",
    "title": "Research Workflow in Plaintext",
    "section": "Emacs",
    "text": "Emacs\nEmacs is the other text editor that you may have heard of aside from Vim. There are several flavours of Emacs, the most popular being GNU Emacs which was created by Richard Stallman. The core, performance critical components of Emacs is written in C, however majority of it’s codebase is written in emacs-lisp (elisp) which is a variant of Lisp.\nThe first public release of Emacs was in 1970 (it has 20 years on Vim which was released in 1991), it is fully open-source, has had 27 stable releases, has a vibrant and large community and continues to be in active development to this date."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#org-mode",
    "href": "blogs/research-workflow-plaintext/index.html#org-mode",
    "title": "Research Workflow in Plaintext",
    "section": "org-mode",
    "text": "org-mode\norg-mode is an emacs package. created by Carsten Dominik (professor of Astronomy at UvA, faculty of natural sciences) in 2003. org-mode provides two core functionalities:\n\nIt provides a major-mode (Emacs lingo for syntax highlighting based on filetype) for plaintext files.\nAnd a slew of tools to organize and edit information in the form of lists (I like to think of them as trees)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#plaintext",
    "href": "blogs/research-workflow-plaintext/index.html#plaintext",
    "title": "Research Workflow in Plaintext",
    "section": "Plaintext",
    "text": "Plaintext\nWe have been talking about plaintext and I think it’s important to clarify what I mean. Plaintext in this context means text files which are unencrypted and written in a format (.{txt,md,org}) that is readable through a terminal emulator."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#why-use-plaintext",
    "href": "blogs/research-workflow-plaintext/index.html#why-use-plaintext",
    "title": "Research Workflow in Plaintext",
    "section": "Why use Plaintext?",
    "text": "Why use Plaintext?\nWhy would we want to use plaintext to store information? There are several benefits, and the word “Freedom” encapsulates everything well. By freedom, I mean:\n\nFreedom to do what we see fit with our data. Its stored as plaintext files on our system. We can choose to keep it that way, upload to our trusted cloud provider or even introduce encryption for additional security and privacy.\nFreedom from yet another proprietary software. There are solutions that market themselves as systems for note-taking, knowledge management or a “second brain” such as Notion and Evernote. But the problem is that they are in control of your data (stored on their servers) and they often store this information in a proprietary format resulting in a vendor lock-in.\nFreedom to choose (or craft) our own tools which are optimised for us and can scale with our ever-growing body of knowledge (remember, learning is a lifelong journey)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#plaintext-productivity",
    "href": "blogs/research-workflow-plaintext/index.html#plaintext-productivity",
    "title": "Research Workflow in Plaintext",
    "section": "Plaintext & Productivity",
    "text": "Plaintext & Productivity\nLooking at the current landscape of productivity tools, I do not see anything that aids in exercising the one true productivity system that actually matters: our brain. Ultimately, it all comes down to effort and time taken to read papers, understanding concepts, learning, and forming connections which give rise to ideas. By takes away the “fluff”, plaintext allows us to focus on the essential. The text. The ideas. The connections.\nThere is no “one size fits all” or “cookie cutter” solution when it comes to productivity. The problem with existing productivity software is that they enforce their own structure and constraints which does more harm than good. I have given this a fair amount of thought and experimentation. In my experience, a two step process of 1. Capturing information quickly and 2. The act of reviewing captured information and deliberate summarisation works best. To this end, we will see how org-mode can help us with this."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#importance-of-structure-standardisation",
    "href": "blogs/research-workflow-plaintext/index.html#importance-of-structure-standardisation",
    "title": "Research Workflow in Plaintext",
    "section": "Importance of Structure & Standardisation",
    "text": "Importance of Structure & Standardisation\nI made a profound observation while reflection on my current research workflow. I spent a lot of my time during my masters on finding tools and systems for managing knowledge. Extrinsic search for existing solutions/methods was not fruitful. Instead, I used whatever solution came naturally to me. Through several iterations and minor changes, I developed a set of rules to store the data in a consistent structure. From this structure, developing tools to automate the process developed organically. I think this is such a simple thing, which makes it so powerful. We see similar phenomenon in software as well. We have decades of research which boils down to doing things in a consistent and standardised manner."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#non-linear-nature-of-research",
    "href": "blogs/research-workflow-plaintext/index.html#non-linear-nature-of-research",
    "title": "Research Workflow in Plaintext",
    "section": "Non-linear Nature of Research",
    "text": "Non-linear Nature of Research\nThe main take-away for me was realising that research is dynamic, non-linear and personal. Finding the right tools is a journey which we must make ourselves. Using plaintext grants us the freedom to explore and experiment different options and techniques. It allows us to craft our own set of tools, standards and techniques which are curated towards how we think and operate."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#document-structure",
    "href": "blogs/research-workflow-plaintext/index.html#document-structure",
    "title": "Research Workflow in Plaintext",
    "section": "Document Structure",
    "text": "Document Structure\nWe can create a new header (or “tree” in org lingo) by prepending the header title with a *. Trees can be nested, a tree may have several other sub-trees within itself. Sub-trees are created by adding more *, the number indicates the depth. We can promote or demote headers quickly using the M-left and M-right keybindings.\nWe can hide or show specific sections of a document by pressing TAB while positioning the cursor (or “point” in emacs lingo) on the header. We can also cycle the visibility of the entire document using S-TAB.\nOrg provides a few handy keybindings to move between headers. C-c n (mnemonic: “next”) takes us to the next header while C-c p (mnemonic: “previous”) takes us to the previous header. C-c u (mnemonic: “up”) can be used to navigate to the parent of the current sub-tree. C-c f (mnemonic: “forward”) and C-c b (mnemonic: “back”) navigates between headers of the same level.\nWe can edit the structure of a org document quickly by using M-down and M-up to move headers."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#todo-states",
    "href": "blogs/research-workflow-plaintext/index.html#todo-states",
    "title": "Research Workflow in Plaintext",
    "section": "Todo States",
    "text": "Todo States\nOrg was originally built to manage tasks in plaintext. A header can be converted to a task by prepending it with a “TODO” keyword. Org also understands the notion of various stages that a task may go through during it’s lifecycle. We can change the todo state interactively using C-c C-t.\nThe todo states are customizable and we will talk more about the ones I have created and use regularly to organize scientific research."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#capturing-refiling",
    "href": "blogs/research-workflow-plaintext/index.html#capturing-refiling",
    "title": "Research Workflow in Plaintext",
    "section": "Capturing & Refiling",
    "text": "Capturing & Refiling\nNOTE this section requires some setup and plumbing before it can be replicated. Refer to the org manual on capture to get started.\nEarlier I mentioned the notion of capturing information and reviewing them at a later stage as a system for productivity (see the section on Motivation & Philosophy). Let’s dive deeper into the capturing segment and explore how org can help introduce some automation into this process.\nWe can invoke org-capture using C-c c which prompts us for a capture template. This is customizable and the most commonly used one is to capture tasks. Once we are satisfied with our content, we can use C-c C-c (this is also the universal keystroke you would use to tell emacs that you are “done” with something or to “confirm” something) to confirm, save and close the capture buffer. Later, I will also talk about the custom capture template that I have written for my research workflow.\nBy default, the captured text is saved in a predefined file, however we can also choose to save it somewhere else using org-refile (C-c C-w in the org-capture buffer)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#planning-structuring-papers",
    "href": "blogs/research-workflow-plaintext/index.html#planning-structuring-papers",
    "title": "Research Workflow in Plaintext",
    "section": "Planning & Structuring Papers",
    "text": "Planning & Structuring Papers\nI find org to be particularly useful for planning and structuring scientific papers. I also use it to capture my thoughts, ideas and notes for various projects that I am working on. Once I have accumulated a large body of text, I find the the “narrow” functionality provided by emacs to help me focus on a particular section or region of text."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#bibliography-management",
    "href": "blogs/research-workflow-plaintext/index.html#bibliography-management",
    "title": "Research Workflow in Plaintext",
    "section": "Bibliography Management",
    "text": "Bibliography Management\nBesides keeping track of tasks, I use org to manage bibliographic information of scientific publications. Let’s look at an example in a project that I worked on earlier.\nOrg allows us to assign metadata to org headers known as org-properties. For instance, I like to store the first author, last author, source of publication, year of publication and a link to the pdf file on my disk as properties. Using the org-sparse-tree command (bound to C-c / in org-mode buffers), we can quickly find what we are looking for."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#aocp.el",
    "href": "blogs/research-workflow-plaintext/index.html#aocp.el",
    "title": "Research Workflow in Plaintext",
    "section": "aocp.el",
    "text": "aocp.el\n\n\n\n\n\n\naocp.el\n\n\n\nMore details regarding this package can be found in my prior post.\n\n\nAs you may expect, this task is repetitive and entering these properties manually becomes cumbersome. To automate this process, I wrote an emacs package: aocp.el. The package provides a few helper functions which are meant to be used in an org-capture template (refer to the project readme for more information)."
  },
  {
    "objectID": "blogs/research-workflow-plaintext/index.html#additional-resources",
    "href": "blogs/research-workflow-plaintext/index.html#additional-resources",
    "title": "Research Workflow in Plaintext",
    "section": "Additional Resources",
    "text": "Additional Resources\nA few resources and pointers to help you get started:\n\nStart with the tutorial: C-h t.\nRead the manual: C-h r or alternatively C-h i to open the documentation viewer (info) and select “Emacs”.\nUse the excellent and extensive emacs help system, some of the most frequent ones that I use are: C-h f, C-h v, C-h o and C-h m.\nConsult the emacs wiki. I usually use it as reference once I have read about a particular topic in the manual.\nWatch Harry Schwartz’s excellent talk on getting started with org-mode.\nWatch Howard Abrams’ excellent talk on emacs introduction and demonstration.\nConsult my emacs config files if you prefer code."
  },
  {
    "objectID": "blogs/bibliography-management-orgmode/index.html",
    "href": "blogs/bibliography-management-orgmode/index.html",
    "title": "Managing Scientific Bibliography using Emacs Org-mode",
    "section": "",
    "text": "Front Matter and setup\nI store all my bibliographic information in a single bib.org file. I show a hypothetical version of this file below, with two entries. I store each paper, as a level 1 header. I use the bibtex key as the title for the header.\n\n\nbib.org\n\n#+title: Bibliography\n#+tags: test viz data self notebook survey\n#+tags: [ test : fair ]\n\n* TODO [#A] amershi2015modeltracker :test:viz:\n\n#+begin_src bibtex\n@InProceedings{   amershi2015modeltracker,\n  series        = {CHI ’15},\n  title         = {ModelTracker: Redesigning Performance Analysis Tools for\n                  Machine Learning},\n  url           = {http://dx.doi.org/10.1145/2702123.2702509},\n  doi           = {10.1145/2702123.2702509},\n  booktitle     = {Proceedings of the 33rd Annual ACM Conference on Human\n                  Factors in Computing Systems},\n  publisher     = {ACM},\n  author        = {Amershi, Saleema and Chickering, Max and Drucker, Steven\n                  M. and Lee, Bongshin and Simard, Patrice and Suh, Jina},\n  year          = {2015},\n  month         = apr,\n  collection    = {CHI ’15}\n}\n#+end_src\n\n* DONE [#A] chen2022fairness       :test:fair:survey:\n:LOGBOOK:\n- State \"DONE\"       from \"TODO\"       [2022-09-19 Mon 14:14]\n:END:\n\nSome notes that I may have regarding this paper.\n\n#+begin_src bibtex\n@Misc{            chen2023fairness,\n  title         = {Fairness Testing: A Comprehensive Survey and Analysis of\n                  Trends},\n  author        = {Zhenpeng Chen and Jie M. Zhang and Max Hort and Federica\n                  Sarro and Mark Harman},\n  year          = {2023},\n  eprint        = {2207.10223},\n  archiveprefix = {arXiv},\n  primaryclass  = {cs.SE}\n}\n#+end_src\n\n\n\nRetrieving bibtex information from Crossref\nMy search for papers always begins on Google Scholar. For papers that I find interesting, I retreive the bibtex information using the doi2bib script. The script accepts the DOI as an argument, and prints the bibtex information obtained from Crossref.\nThe script also accepts a --preprint flag, in which case, it accepts an Arxiv ID and obtains the bibtex information from Arxiv directly.\n\n\n\n\n\n\nScientific Paper Discovery\n\n\n\nYou can find more information on how I discovery scientific papers in this blogpost.\n\n\n\n\n\n\n\n\ndoi2bib\n\n\n\nYou can find more details regarding the doi2bib script in this blogpost.\n\n\n\n\nCapturing bibtex information using org-capture\nEmacs org-mode has a nifty capture feature that allows the user to quickly capture information. I have the following capture template to save bibtex information into the bib.org file above.\n\n\nbib.txt\n\n* %?\n\n#+begin_src bibtex\n#+end_src\n\nI have the following org-capture configuration in my init.el file.\n\n\ninit.el\n\n(org-capture-templates\n`((\"p\" \"Paper\" entry (file aru/org-bib-file)\n   \"%[~/.emacs.d/org-templates/bib.txt]\" :prepend t)))\n\nIn Emacs, I hit the keystrokes C-c c followed by the p key to initiate the capture sequence. Org-mode automatically inserts the capture template shown above. It creates a new level 1 header and inserts an empty bibtex source block.\nTo populate the source block, I hit C-c ' (see org-special-edit for more information on this keybinding). With C-u M-|, I run the doi2bib command along with the DOI to add the output of the command into the source block. C-c ' closes the special edit buffer and returns back to bib.org.\n\n\n\n\n\n\nEvil Mode\n\n\n\nI now use evil-mode which provides vim keybindings within Emacs. I populate the source block using the :read! command.\n\n\n\n\nMapping of orgmode features and my usage\nIn the following table I summarise how I use the built-in orgmode features for organising the bibliographic information.\n\n\n\n\n\n\n\nFeature\nPurpose\n\n\n\n\nTODO keywords\nI mark papers that I want to read with the TODO state. Papers that I have already read are marked with the DONE state.\n\n\nPriority\nPapers that I find interesting, and cite frequently are marked with the A priority.\n\n\nTags\nI use tags to broadly classify the papers based on topics relevant for my Phd. You can see the tags I use in the example bib.org file provided above.\n\n\n\n\n\nSearching and Retrieving\nI use org-agenda to search and retrieve papers of interest. For instance, I can filter papers that I need to read by asking org-agenda for papers that are marked with the TODO state (see org-todo-list). I can produce a list of all papers that have the testing and data tag (see org-tags-view). More complex search queries can be constructed using the org advanced search commands: for instance, give me all papers on testing that were written by author X in the year of 2001 (see org advanced search syntax).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/nutrition/index.html",
    "href": "blogs/nutrition/index.html",
    "title": "Nutrition",
    "section": "",
    "text": "This document provides information regarding diet and nutrition. The premise is to identify underlying principles that determine optimal dietary requirements for weight loss and gain. For the prior goal, I am interested in nutrition which will provide optimal recovery and minimal muscle loss. For the later, I am interested in maximising muscle gain whilst minimising fat gain.\nNutrition is simply thermodynamics: Energy in must equal energy out for a system in equilibrium. If we consider that our body is the system, then the food we consume (energy in) must equal the energy required for our body to function (energy out). Logic thus dictates that when we eat less food than required, we lose weight and conversely, gain weight when we eat more.\nThe quantity of food/caloric intake required to maintain our bodyweight can be estimated from a TDEE calculator (easily available online, I have been using this one). From there, research has shown that a 10-20% increase/decrease in caloric intake will result in optimal weight gain/loss.\nEmphasis must also be put on the macro and micro nutrients. Specifically, research shows that 120gm of protein minimum results in optimal muscular recovery. Up to 160gm or 0.8/lb of bodyweight (whichever is larger) has been proven to result in optimal muscle gain when trying to gain weight. For the micro nutrients, it goes without saying, eat vegetables as much as possible! I personally also take vitamin D, omega-3 and multi-vitamin supplements. Vegetables also help me feel full longer.\nSome lessons I have learned from experience. Having a good diet is not a temporary setting and is precisely the reason why diets do not work. Having a good diet requires time, patience and a lot of experimentation to find out what works for the individual. Having a good diet is only the first part, having a good diet which is also sustainable is the key challenge (remember, you need to eat until you die). Finally, I think this requires a fair share of psychological strengthening. I noticed that for me, it took years until I found a good balance of food that works for me and learned to control my hunger/cravings.\nFollowing are some no-nonsense, community driven, public information that I keep coming back to time-and-time again. r/bodyweightfitness wiki contains links to valuable bodyweight fitness routines. The r/fitness wiki has several important pages. Specifically, the entries on weight/fat loss, muscle building and the FAQ page.\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/aocp/index.html",
    "href": "blogs/aocp/index.html",
    "title": "Aru’s Org Capture Template (aocp.el)",
    "section": "",
    "text": "After observing my workflow of managing bibliographic information in Emacs, I extracted the repeated actions into an Emacs package.\nTo gain some perspective on my workflow, see my prior article on my research workflow.\nThe package is available on github with two alternative installation methods: 1. By manually downloading the aocp.el file and sticking it in your Emacs load-path, or 2. Using straight.el which is what I recommend.\nThe package works under the assumption that you manage your bibliographic information in org-mode (a major-mode for Emacs). The functions made available through this package are intended to be used in an org-capture template, they are not meant to be called interactively (ie. by using M-x).\nAssuming that you have a bibtex entry in your kill-ring (either by killing text within Emacs or by coping text from an external application into your clipboard), this package will do the following:\n\nExtract the bibkey\nExtract the first author\nExtract the last author\nExtract the source of publication\n\n* TODO %(aocp--get-bibkey nil)\n  :PROPERTIES:\n  :PDF: file:~/Documents/papers/%(aocp--get-bibkey t).pdf\n  :FIRST_AUTHOR: %(aocp--get-first-author)\n  :LAST_AUTHOR: %(aocp--get-last-author)\n  :SOURCE: %(aocp--get-source)\n  :END:\n%?\n+ problem statement ::\n+ solution ::\n+ results ::\n+ limitations ::\n+ remarks ::\n\n  #+begin_src bibtex :tangle yes\n  %c\n  #+end_src\nAssuming you have the above template in paper.txt, you can configure org as follows (replace your-org-inbox-file appropriately):\n(setq org-capture-templates\n    '((\"p\" \"Paper\" entry (file+headline your-org-inbox-file \"Inbox\")\n    \"%[~/.emacs.d/org-templates/paper.txt]\")))\nWith this in place, you can quickly collect all bibliographic information within an org file. Leveraging the powerful functionality provided by org-properties, one can quickly find relevant papers. For instance, I can look up all papers by author X or all papers by author X published at Y.\nA nice little tip is to download a local copy of the pdf and save them all in a folder. To make this easier, aocp.el also pushes the bibkey to the kill-ring. So all that is left to do is click the download button and paste the bibkey as the file name. This ensure 1. That you have all pdfs names consistently and 2. You have a link to the pdf from your org file (see the :PDF: property in the template above) which you can open by hitting C-c C-o over the link. You do not need to poke around in the directory containing the pdfs, all the context is available in the org file and should be the point of entry for all your bibliographic needs!\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/productivity/index.html",
    "href": "blogs/productivity/index.html",
    "title": "Productivity",
    "section": "",
    "text": "Productivity to me comes from proper information management. Information can come in many forms: email, thoughts/ideas, deadlines, events, project planning, online links, and many more."
  },
  {
    "objectID": "blogs/productivity/index.html#capturing",
    "href": "blogs/productivity/index.html#capturing",
    "title": "Productivity",
    "section": "Capturing",
    "text": "Capturing\nI find that absolutely nothing can beat a pen and a paper for this job (see the section on the benefits of writing on paper below).\nFor the rare occasions when pen and paper cannot do the job (such as saving a link to an online article to be read later or when we are on the move), a digital solution is required. Any app will do, as long as it has a “quick capture” mechanism (most todo apps do these days). You can also simply email yourself if you prefer not to have another dependency, another moving part, another “cog” in your system."
  },
  {
    "objectID": "blogs/productivity/index.html#reviewing",
    "href": "blogs/productivity/index.html#reviewing",
    "title": "Productivity",
    "section": "Reviewing",
    "text": "Reviewing\nNow that I have captured information, what do we do with it? 90% of it is just noise and there is a fine line between expanding knowledge and hoarding knowledge. Here, I find it important to maintain a reference of sort to determine what is important and what is not.\nI find that writing on paper acts as a natural buffer to ‘noise’.\nI review my notes on a weekly, monthly, quarterly and yearly basis. The goal of each review is to progressively summarise the captured ideas. So, weekly reviews summarize daily logs, monthly reviews summarize the weekly reviews, and so on and so forth. This progressive summarization is a deliberate practise of writing that allows me to develop my ideas."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Following is a list of public talks I have given in the past.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\nabstract\n\n\n\n\n\n\nBridging the Gap Between Visual and Analytical Machine Learning Testing\n\n\nJun 2, 2023\n\n\nLightening talk at SEN Symposium 2023. Presented my research direction, work done so far, challenges and next steps. Content was mostly similar to the talk below. \n\n\n\n\nTowards Understanding Machine Learning Testing in Practice\n\n\nMay 15, 2023\n\n\nPresentation for our poster titled Towards Understanding Machine Learning Testing in Practice which was published in CAIN 2023. \n\n\n\n\nData Validation with TFDV\n\n\nMay 16, 2022\n\n\nIn this lecture we will go over the basics of data validation. The first half of this lecture will be a talk on the fundamentals of data validation. We will answer what is data validation?, why should we validate our data? and how we can validate our data?. The second half of the lecture will be a hands-on tutorial on using Tensorflow Data Validation, instructions & code for which can be found on this github repo. \n\n\n\n\nData Smells in Public Datasets\n\n\nMay 4, 2022\n\n\nIn this talk I will present our recent paper titled Data Smells in Public Datasets which was published at the 1st International Conference on AI Engineering (CAIN) 2022. I will first present the problem we are trying to solve along with the contributions that we made. I will present the methodology which was followed along with the results obtained. I will present a select few smells which I personally find interesting & hope will generate some discussion. Finally, we will conclude the talk with some high level takeaways from our study along with the limitations & future directions of work. \n\n\n\n\nPrivacy Preserving Deep Learning\n\n\nSep 7, 2021\n\n\nA talk on Privacy Preserving Deep Learning (PPDL) I gave to my research group. It was largly based on a literature review I did during my Msc. \n\n\n\n\nResearch Workflow in Plaintext\n\n\nJul 12, 2021\n\n\nIn this talk I will go over how we can use Emacs and org-mode to craft a research workflow. We will look at how we can leverage the power of Emacs and org-mode to capture, store, search and retrieve research data, all in plain text! The talk will touch upon how org-mode can be used as an environment for literate programming and reproducible research. I do not assume any prior knowledge of emacs or org-mode and I want this to be more of a discussion rather than a talk. Please ask me questions as I go along and share your thoughts, tips and techniques with others! \n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arumoy Shome",
    "section": "",
    "text": "I am a researcher at TU Delft, working towards my doctorate as part of the Software Engineering Research Group. My research interests lie at the intersection of Data Science, Software Engineering and Design. Here I am developing tools, techniques and best practises which allow us to build, test, deploy, scale and maintain modern software systems with Machine Learning (ML) components. While we must look to the micro to identify and understand a problem, I try to keep an eye towards the current era of big-data and large-scale ML as a long-term area of research. You can find a list of my publications here.\nI completed my Msc. in Computer Science from the Vrije Universiteit Amsterdam and Universiteit van Amsterdam in the Netherlands, specializing in Big Data Engineering. As part of my thesis, I conducted interdisciplinary research with Nikhef and The Netherlands eScience Center to improve the data processing pipeline of the KM3NeT Neutrino Telescope using Graph Convolutional Neural Networks.\nI obtained my Bachelor’s in The Applied Sciences from the University of Waterloo in Canada, specializing in System Design Engineering with an option in Entrepreneurship. Here, I learned to bring creative solutions to complex problems with multiple facets such as society, economics, environment and politics. I also learned how to nurture an idea at it’s inception, develop it using systems theory and successfully bring it to the market as a finished product.\nMy professional experience is two pronged. I have over 5 years of software development experience through professional web development positions at and small companies. I am also versed with driven, research oriented projects rooted in diverse topics such as Large Scale Data Engineering, Data Visualization and Artificial Intelligence.\nI enjoy sharing knowledge and having discussions and am always happy to engage with other publicly. Sometimes, I write about my day-to-day challenges as a software engineering, data scientist & researcher.\nAll materials (excluding links to external websites or third party products) on this website are open sourced under the Creative Commons BY 4.0 license.\n\n\n Back to top"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "This page contains links to some of my writings on topics that interest me. Usually, they are inspired by problems that I experience in my day-to-day life.\nI like pondering over the act or the process of doing something.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nBash commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nBash (bash) commands I frequently use.\n\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\nDocker commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nDocker (docker) commands I frequently use.\n\n\n\n\n\n\nJan 31, 2024\n\n\n\n\n\n\n\n\nOrganising research projects with git\n\n\n\n\n\n\n\nresearch\n\n\nproductivity\n\n\n\n\nSome standards and conventions I follow when organising research project data using git.\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nToggling background color in kitty and vim (yob)\n\n\n\n\n\n\n\nshell\n\n\nvim\n\n\n\n\nyob is a tiny shell script which toggles between a light and dark colorscheme in Kitty, my terminal of choice.\n\n\n\n\n\n\nJan 19, 2024\n\n\n\n\n\n\n\n\nManaging Scientific Bibliography using Emacs Org-mode\n\n\n\n\n\n\n\nemacs\n\n\nproductivity\n\n\n\n\nHow I organise, search and retrieve my scientific papers using Emacs org-mode.\n\n\n\n\n\n\nNov 29, 2023\n\n\n\n\n\n\n\n\nAutomatically retrieving Bibtex information from DOI\n\n\n\n\n\n\n\nshell\n\n\nproductivity\n\n\n\n\ndoi2bib is a simple Python script I wrote that fetches bibtex information from the Crossref API using the provided DOI. It can also handle pre-prints published on Arxiv.\n\n\n\n\n\n\nNov 26, 2023\n\n\n\n\n\n\n\n\nAspell Commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nAspell (aspell) commands I frequently use.\n\n\n\n\n\n\nOct 9, 2023\n\n\n\n\n\n\n\n\nNavigating large markdown files in vim\n\n\n\n\n\n\n\nshell\n\n\nvim\n\n\n\n\nStrategies to navigate large plaintext (specifically markdown) files in vim.\n\n\n\n\n\n\nApr 29, 2023\n\n\n\n\n\n\n\n\nCMS using Pandoc and Friends\n\n\n\n\n\n\n\nshell\n\n\nweb\n\n\n\n\nSome tools & techniques I use to run a no non-sense blog using static html pages. All powered by a sane file naming convension, plaintext documents writing in markdown and exported to html using pandoc and other unix cli tools.\n\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n\n\nPacking and Unpacking **kwargs\n\n\n\n\n\n\n\npython\n\n\n\n\nPacking and unpacking keyword arguments in Python.\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n\n\nRipgrep commands\n\n\n\n\n\n\n\nshell\n\n\n\n\nRipgrep (rg) commands I frequently use.\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n\n\nData Validation with TFDV\n\n\n\n\n\n\n\ntalk\n\n\nresearch\n\n\n\n\nIn this lecture we will go over the basics of data validation. The first half of this lecture will be a talk on the fundamentals of data validation. We will answer what is data validation?, why should we validate our data? and how we can validate our data?. The second half of the lecture will be a hands-on tutorial on using Tensorflow Data Validation, instructions & code for which can be found on this github repo.\n\n\n\n\n\n\nMay 16, 2022\n\n\n\n\n\n\n\n\nEffortless Parallel Execution with xargs & Friends\n\n\n\n\n\n\n\nshell\n\n\n\n\nRecently, I had to run Tensorflow Data Validation on over 500 public datasets from Kaggle to generate a baseline schema file for further analysis. I chose to do this using the xargs unix command.\n\n\n\n\n\n\nMay 8, 2022\n\n\n\n\n\n\n\n\nData Smells in Public Datasets\n\n\n\n\n\n\n\ntalk\n\n\nresearch\n\n\n\n\nIn this talk I will present our recent paper titled Data Smells in Public Datasets which was published at the 1st International Conference on AI Engineering (CAIN) 2022. I will first present the problem we are trying to solve along with the contributions that we made. I will present the methodology which was followed along with the results obtained. I will present a select few smells which I personally find interesting & hope will generate some discussion. Finally, we will conclude the talk with some high level takeaways from our study along with the limitations & future directions of work.\n\n\n\n\n\n\nMay 4, 2022\n\n\n\n\n\n\n\n\nThere and Back Again A Tale of Website Management\n\n\n\n\n\n\n\nshell\n\n\nvim\n\n\nweb\n\n\n\n\nManaging websites using markdown, shell and vim.\n\n\n\n\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\nTimestamps in the Shell (today)\n\n\n\n\n\n\n\nshell\n\n\nproductivity\n\n\n\n\nCreating timestamps in the terminal.\n\n\n\n\n\n\nMar 3, 2022\n\n\n\n\n\n\n\n\nAru’s Information Management System (AIMS)\n\n\n\n\n\n\n\nshell\n\n\nproductivity\n\n\n\n\nAIMS or Aru’s Information Management System is a collection of shellscripts to manage information in plaintext. It is inspired by org-mode, and tries to replicate a subset of its functionalities which I frequently use. AIMS is completely tuned towards my workflow as a researcher and how I manage my digital notes.\n\n\n\n\n\n\nFeb 28, 2022\n\n\n\n\n\n\n\n\nReflections on Scientific Research\n\n\n\n\n\n\n\nresearch\n\n\n\n\nReflections on the scientific process and what constitutes being a good researcher.\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\nPrivacy Preserving Deep Learning\n\n\n\n\n\n\n\ntalk\n\n\nresearch\n\n\n\n\nA talk on Privacy Preserving Deep Learning (PPDL) I gave to my research group. It was largly based on a literature review I did during my Msc.\n\n\n\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\nFocus\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nNotes on attaining focus.\n\n\n\n\n\n\nSep 5, 2021\n\n\n\n\n\n\n\n\nProject Management\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nNotes on project management.\n\n\n\n\n\n\nAug 27, 2021\n\n\n\n\n\n\n\n\nResearch Workflow in Plaintext\n\n\n\n\n\n\n\ntalk\n\n\nemacs\n\n\n\n\nIn this talk I will go over how we can use Emacs and org-mode to craft a research workflow. We will look at how we can leverage the power of Emacs and org-mode to capture, store, search and retrieve research data, all in plain text! The talk will touch upon how org-mode can be used as an environment for literate programming and reproducible research. I do not assume any prior knowledge of emacs or org-mode and I want this to be more of a discussion rather than a talk. Please ask me questions as I go along and share your thoughts, tips and techniques with others!\n\n\n\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\n\nNutrition\n\n\n\n\n\n\n\nmisc\n\n\n\n\nNotes on diet and nutrition.\n\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\nAru’s Org Capture Template (aocp.el)\n\n\n\n\n\n\n\nemacs\n\n\nproductivity\n\n\n\n\nAn Emacs package I wrote for managing bibliographic information.\n\n\n\n\n\n\nJun 16, 2021\n\n\n\n\n\n\n\n\nScientific Paper Discovery\n\n\n\n\n\n\n\nresearch\n\n\n\n\nMy process of discovering relevant and important papers in a new scientific field.\n\n\n\n\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\nResearch Workflow\n\n\n\n\n\n\n\nproductivity\n\n\nresearch\n\n\n\n\nAn outline of my research workflow which I have developed to handle the non-linear nature of scientific work.\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\nproductivity\n\n\n\n\nProductivity to me comes from proper information management. Information can come in many forms: email, thoughts/ideas, deadlines, events, project planning, online links, and many more.\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\nDired Commands\n\n\n\n\n\n\n\nemacs\n\n\n\n\nDired commands I frequently use.\n\n\n\n\n\n\nApr 23, 2021\n\n\n\n\n\n\n\n\nGarbage Collection for Matplotlib in org-mode\n\n\n\n\n\n\n\npython\n\n\nemacs\n\n\n\n\nNotes on managing memory when using org-mode and python to conduct data science analysis.\n\n\n\n\n\n\nJan 5, 2021\n\n\n\n\n\n\n\n\nPython context file\n\n\n\n\n\n\n\npython\n\n\n\n\nPython context file for managing imports.\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n\n\nshell path_finder\n\n\n\n\n\n\n\nshell\n\n\n\n\nHow $PATH is constructed on OSX.\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n\n\nNavigating in Vim\n\n\n\n\n\n\n\nvim\n\n\n\n\nStrategies to navigate files in vim.\n\n\n\n\n\n\nAug 6, 2019\n\n\n\n\n\n\n\n\nSearching in Vim\n\n\n\n\n\n\n\nvim\n\n\n\n\nVarious strategies for searching text within vim.\n\n\n\n\n\n\nMay 13, 2019\n\n\n\n\n\n\n\n\nPrivate Link Sharing\n\n\n\n\n\n\n\nweb\n\n\n\n\nPoor man’s document sharing with private links, sort of what you get with Google Docs.\n\n\n\n\n\n\nJan 1, 2018\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  }
]